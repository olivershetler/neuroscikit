{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "\n",
    "PROJECT_PATH = os.getcwd()\n",
    "sys.path.append(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\LEC_full_merged_scores.xlsx')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" QUALITY CHECK DATA \"\"\" \n",
    "nan_idx = np.where(df['obj_q_0'].isna())[0]\n",
    "not_nan_idx = np.where(~df['obj_q_0'].isna())[0]\n",
    "nan_dates = (df['date'][nan_idx].unique())\n",
    "nan_names = (df['name'][nan_idx].unique())\n",
    "\n",
    "print('Number of NaN rows: ' + str(len(nan_idx)))\n",
    "print('Animals with NaN rows: ' + str(nan_names))\n",
    "print('Dates with NaN rows: ' + str(nan_dates))\n",
    "\n",
    "# remove rows with NaN values\n",
    "print('Removing nan rows')\n",
    "df = df.iloc[not_nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FILTERING \"\"\"\n",
    "\n",
    "\"\"\" REMOVE FIELD WITH LOW COVERAGE % \"\"\"\n",
    "# remove rows with field_coverage < 0.1\n",
    "# df = df[df['field_coverage'] >= 0.1]\n",
    "\n",
    "\"\"\" ONLY KEEPING MAIN FIELD \"\"\"\n",
    "# remove rows where field_id is not 1 and score is not 'whole' or 'spike_density'\n",
    "# df = df[(df['field_id'] == 1) | (df['score'] == 'whole') | (df['score'] == 'spike_density')]\n",
    "\n",
    "\"\"\" CHOOSING ANGLE FOR EACH ROW \"\"\"\n",
    "# for each row, choose lowest quantile from ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "df['obj_q'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].min(axis=1)\n",
    "df['obj_a'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].idxmin(axis=1)\n",
    "# convert obj_a to degrees\n",
    "df['obj_a'] = df['obj_a'].apply(lambda x: int(x.split('_')[2]))\n",
    "# use obj_wass with angle of min quantile\n",
    "df['obj_w'] = df.apply(lambda x: x['obj_wass_' + str(x['obj_a'])], axis=1)\n",
    "\n",
    "\"\"\" ASSESSING SIG FOR EACH ROW AT EACH ANGLE \"\"\"\n",
    "# obj_s_rows = ['obj_s_0', 'obj_s_90', 'obj_s_180', 'obj_s_270']\n",
    "# obj_q_rows = ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "# for i in range(len(obj_s_rows)):\n",
    "#     obj_q_x = obj_q_rows[i]\n",
    "#     df[obj_s_rows[i]] = df[obj_q_x].apply(lambda x: 1 if x < quantile_threshold else 0)\n",
    "\n",
    "\n",
    "# df2 = df[df['score'] == 'whole'].copy()\n",
    "df2 = df.copy()\n",
    "# group_by_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "# df2 = df2.groupby(group_by_cell).mean().reset_index()\n",
    "cts = df2[df2['spike_count'] > 30000]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count upper of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(30000 , str(len(df2[df2['spike_count'] > 30000])), \n",
    "                       cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_count'] < 100]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count lower of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(100 , str(len(df2[df2['spike_count'] < 100])),\n",
    "                          cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['information'] < 0.25]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spatial info of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.25 , str(len(df2[df2['information'] < 0.25])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['selectivity'] < 5]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Selectivity of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['selectivity'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['iso_dist'] < 7.5]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Isolation distance of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['iso_dist'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['firing_rate'] > 80]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Firing rate of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(20 , str(len(df2[df2['firing_rate'] > 20])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_width'] < 0.00005]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike width of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.00005 , str(len(df2[df2['spike_width'] < 0.00005])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "\n",
    "# drop spike count column \n",
    "df2 = df2.drop(columns=['spike_count'])\n",
    "# rename spike_count.1 to spike_count\n",
    "df2 = df2.rename(columns={'spike_count.1': 'spike_count'})\n",
    "\n",
    "# df2 = df2[df2['spike_count'] < 30000]\n",
    "# df2 = df2[df2['spike_count'] > 100]\n",
    "# df2 = df2[df2['information'] > 0.25]\n",
    "# df2 = df2[df2['selectivity'] > 5]\n",
    "df2 = df2[df2['iso_dist'] > 7.5]\n",
    "# df2 = df2[df2['firing_rate'] < 80]\n",
    "# df2 = df2[df2['spike_width'] > 0.00005]\n",
    "\n",
    "print('Remaining cells: ' + str(len(df2)) + ' of which ' + str(len(df2[df2['group'] == 'ANT'])) + ' ANT, ' + str(len(df2[df2['group'] == 'B6'])) + ' B6 and ' + str(len(df2[df2['group'] == 'NON'])) + ' NON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['obj_q'] = df2['obj_q_NO']\n",
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "# df = df[df['object_location'] != 'NO']\n",
    "df.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\filtered_df4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consecutive_sessions_threshold = 2\n",
    "# quantile_threshold = 0.25\n",
    "# consecutive = False\n",
    "# score = 'field'\n",
    "# main_field_only = False\n",
    "\n",
    "# df['obj_q'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].min(axis=1)\n",
    "# df['obj_a'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].idxmin(axis=1)\n",
    "# # convert obj_a to degrees\n",
    "# df['obj_a'] = df['obj_a'].apply(lambda x: int(x.split('_')[2]))\n",
    "# # use obj_wass with angle of min quantile\n",
    "# df['obj_w'] = df.apply(lambda x: x['obj_wass_' + str(x['obj_a'])], axis=1)\n",
    "\n",
    "# # drop rows where spike count > 30 000\n",
    "# # df2 = df[df['spike_count'] < 30000]\n",
    "# # df2 = df2[df2['spike_count'] > 100]\n",
    "# # drop rows where spatial info < 0.25 \n",
    "# # df = df[df['information'] > 0.25]\n",
    "# # df = df[df['selectivity'] > 5]\n",
    "# df2 = df[df['iso_dist'] > 5]\n",
    "# df2 = df2[df2['firing_rate'] < 80]\n",
    "# # df2 = df2[df2['spike_width'] > 0.00005]\n",
    "# # drop field count > 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i in ['ANT','B6','NON']:\n",
    "    ax = fig.add_subplot(1, 3, ['ANT','B6','NON'].index(i)+1)\n",
    "\n",
    "    df_to_use = df2[df2['group'] == i]\n",
    "    # plt.hist(df_to_use[df_to_use['score'] == 'whole']['obj_w'], bins=100)\n",
    "    sns.histplot(data=df_to_use[df_to_use['score'] == 'whole'], x='obj_q', bins=50, hue='name', kde=False, ax=ax)\n",
    "    ax.set_title(i)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corr'] = np.nan\n",
    "for grp in df.groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']):\n",
    "    corr =  grp[1]['obj_w'].corr(grp[1]['obj_q'])\n",
    "\n",
    "    df.loc[grp[1].index, 'corr'] = corr\n",
    "\n",
    "assert df['corr'].isna().sum() == 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))    \n",
    "\n",
    "for i in ['ANT','B6','NON']:\n",
    "    ax = fig.add_subplot(1, 3, ['ANT','B6','NON'].index(i)+1)\n",
    "\n",
    "    df_to_use = df[df['group'] == i]\n",
    "    # plt.hist(df_to_use[df_to_use['score'] == 'whole']['obj_w'], bins=100)\n",
    "    sns.histplot(data=df_to_use[df_to_use['score'] == 'spike_density'], x='corr', bins=np.arange(0,1,0.01), kde=False, ax=ax)\n",
    "    # sns.scatterplot(data=df_to_use[df_to_use['score'] == 'whole'], x='obj_q', y='obj_w', hue='name', ax=ax)\n",
    "\n",
    "    # plot corr\n",
    "    # sns.scatterplot(data=df_to_use[df_to_use['score'] == 'whole'], x='obj_q', y='obj_w', hue='name', ax=ax)\n",
    "    ax.set_title(i + ' - ' + str(df_to_use[df_to_use['score'] == 'spike_density']['corr'].mean()))\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# df.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\filtered_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot of iso_dist for each group\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "dft = df[df['score'] == 'whole']\n",
    "sns.kdeplot(dft[dft['group'] == 'ANT']['iso_dist'], label='ANT', ax=ax)\n",
    "sns.kdeplot(dft[dft['group'] == 'NON']['iso_dist'], label='NON', ax=ax)\n",
    "sns.kdeplot(dft[dft['group'] == 'B6']['iso_dist'], label='B6', ax=ax)\n",
    "ax.axvline(np.median(dft['iso_dist']), color='black', linestyle='--', label='median: ' + str(np.round(np.median(dft['iso_dist']), 2)))\n",
    "ax.set_xlabel('Isolation Distance')\n",
    "ax.legend()\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Isolation Distance Distribution')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['red', 'blue', 'green']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "# metric = 'obj_w'\n",
    "metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "\n",
    "    # # every row for that score\n",
    "    to_plot_single = df[df['score'] == score]\n",
    "    to_plot =  df[df['score'] == score]\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # # scores averaged for each session\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    # get group means + CI\n",
    "    means = to_plot.groupby('group')[metric].mean().round(2)\n",
    "    stds = to_plot.groupby('group')[metric].std()\n",
    "    n = to_plot.groupby('group')[metric].count()\n",
    "    sems = stds / np.sqrt(n)\n",
    "    sems = sems.round(2)\n",
    "    \n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gps[k]][metric], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "    # do t test \n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],usevar='unequal')\n",
    "    # ks test   \n",
    "    # res = ks_2samp(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric])\n",
    "    # mann whitney u test\n",
    "    res = mannwhitneyu(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric], alternative='greater')\n",
    "    # w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    # w2 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    # w1 = w1 / np.mean(w1)\n",
    "    # w2 = w2 / np.mean(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    antvsb6 = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot[metric] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.01\n",
    "        ax.plot([0, 0, 1, 1], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.315,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.315,0.85, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs B6: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal')\n",
    "    # ks test\n",
    "    # res = ks_2samp(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric])\n",
    "    # mann whitney u test\n",
    "    res = mannwhitneyu(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric])\n",
    "    # w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    # w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    # w1 = w1 / np.mean(w1)\n",
    "    # w2 = w2 / np.mean(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    antvsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.02\n",
    "        ax.plot([0, 0, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5,0.925, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal', alternative='two-sided')\n",
    "    # ks test\n",
    "    # res = ks_2samp(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric])\n",
    "    # mann whitney u test\n",
    "    res = mannwhitneyu(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric], alternative='two-sided')\n",
    "    # w1 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    # w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    # w1 = w1 / np.mean(w1)\n",
    "    # w2 = w2 / np.mean(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    b6vsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += .03\n",
    "        ax.plot([1, 1, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('B6 vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "\n",
    "    # BH correction\n",
    "    pvals = [antvsb6, antvsnon, b6vsnon]\n",
    "    pval_names = ['antvsb6', 'antvsnon', 'b6vsnon']\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "    print(pval_names)\n",
    "    print('Corrected p-values: ' + str(reject))\n",
    "\n",
    "\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_title(titles_to_use[i])\n",
    "    ax.set_ylabel('Wasserstein distances (cm)')\n",
    "\n",
    "    group_order = ['B6', 'ANT', 'NON']  # 'B6' becomes the reference group\n",
    "    to_plot['group'] = pd.Categorical(to_plot['group'], categories=group_order, ordered=True)\n",
    "    # to_plot_model = to_plot[to_plot['group'] != 'B6']\n",
    "\n",
    "    model = sm.MixedLM.from_formula(metric + ' ~ group', data=to_plot, groups=to_plot['name'])\n",
    "    # result = model.fit()\n",
    "\n",
    "    robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "    result = robust_model.fit()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(score)\n",
    "    print(result.summary())\n",
    "\n",
    "fig.suptitle('Averaged by Session')\n",
    "# fig.suptitle('All indiv. cell-session appearances')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40044375/how-to-calculate-the-kolmogorov-smirnov-statistic-between-two-weighted-samples\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def ks_w2(data1, data2, wei1, wei2):\n",
    "    ix1 = np.argsort(data1)\n",
    "    ix2 = np.argsort(data2)\n",
    "    data1 = data1[ix1]\n",
    "    data2 = data2[ix2]\n",
    "    wei1 = wei1[ix1]\n",
    "    wei2 = wei2[ix2]\n",
    "    data = np.concatenate([data1, data2])\n",
    "    cwei1 = np.hstack([0, np.cumsum(wei1)/sum(wei1)])\n",
    "    cwei2 = np.hstack([0, np.cumsum(wei2)/sum(wei2)])\n",
    "    cdf1we = cwei1[[np.searchsorted(data1, data, side='right')]]\n",
    "    cdf2we = cwei2[[np.searchsorted(data2, data, side='right')]]\n",
    "    return np.max(np.abs(cdf1we - cdf2we))\n",
    "\n",
    "from scipy.stats import distributions\n",
    "\n",
    "def ks_weighted(data1, data2, wei1, wei2, alternative='two-sided'):\n",
    "    ix1 = np.argsort(data1)\n",
    "    ix2 = np.argsort(data2)\n",
    "    data1 = data1[ix1]\n",
    "    data2 = data2[ix2]\n",
    "    wei1 = wei1[ix1]\n",
    "    wei2 = wei2[ix2]\n",
    "    data = np.concatenate([data1, data2])\n",
    "    cwei1 = np.hstack([0, np.cumsum(wei1)/sum(wei1)])\n",
    "    cwei2 = np.hstack([0, np.cumsum(wei2)/sum(wei2)])\n",
    "    cdf1we = cwei1[np.searchsorted(data1, data, side='right')]\n",
    "    cdf2we = cwei2[np.searchsorted(data2, data, side='right')]\n",
    "    d = np.max(np.abs(cdf1we - cdf2we))\n",
    "    # calculate p-value\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    m, n = sorted([float(n1), float(n2)], reverse=True)\n",
    "    en = m * n / (m + n)\n",
    "    if alternative == 'two-sided':\n",
    "        prob = distributions.kstwo.sf(d, np.round(en))\n",
    "    else:\n",
    "        z = np.sqrt(en) * d\n",
    "        # Use Hodges' suggested approximation Eqn 5.3\n",
    "        # Requires m to be the larger of (n1, n2)\n",
    "        expt = -2 * z**2 - 2 * z * (m + 2*n)/np.sqrt(m*n*(m+n))/3.0\n",
    "        prob = np.exp(expt)\n",
    "    return d, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['red', 'blue', 'green']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "\n",
    "    # # every row for that score\n",
    "    to_plot_single = df[df['score'] == score]\n",
    "    to_plot = df[df['score'] == score]\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # # scores averaged for each session\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    # get group means + CI\n",
    "    means = to_plot.groupby('group')[metric].mean().round(2)\n",
    "    stds = to_plot.groupby('group')[metric].std()\n",
    "    n = to_plot.groupby('group')[metric].count()\n",
    "    sems = stds / np.sqrt(n)\n",
    "    sems = sems.round(2)\n",
    "    \n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gps[k]][metric], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "    # do t test \n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],usevar='unequal')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    # w1 = w1 / np.sum(w1)\n",
    "    # w2 = w2 / np.sum(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    # weighted mann whitney u test from scipy\n",
    "\n",
    "    g1 = to_plot[to_plot['group'] == gps[0]][metric]\n",
    "    g2 = to_plot[to_plot['group'] == gps[1]][metric]\n",
    "    # weighted_ranksum_g1 = np.sum(stats.rankdata(g1) * w1)\n",
    "    # weighted_ranksum_g2 = np.sum(stats.rankdata(g2) * w2)\n",
    "    # observed_statistic = weighted_ranksum_g1 - weighted_ranksum_g2\n",
    "    # # observed_statistic = np.sum(stats.rankdata(g1) * w1) - (np.sum(w1) * (np.sum(w1) + 1) / 2)\n",
    "    # npermute = 10000\n",
    "    # permuted_stats = []\n",
    "    # for _ in range(npermute):\n",
    "    #     combined_data = np.concatenate((g1, g2))\n",
    "    #     combined_weights = np.concatenate((w1, w2))\n",
    "    #     np.random.shuffle(combined_data)\n",
    "    #     permuted_statistic = np.sum(np.multiply(stats.rankdata(combined_data), combined_weights))\n",
    "    #     permuted_stats.append(permuted_statistic)\n",
    "\n",
    "    # p = (np.sum(np.abs(permuted_stats) >= np.abs(observed_statistic)) + 1) / (npermute + 1)\n",
    "    # t = observed_statistic\n",
    "\n",
    "    # n_permutations = 10000\n",
    "    # permuted_ks_stats = []\n",
    "\n",
    "    # for _ in range(n_permutations):\n",
    "    #     shuffled_g1 = np.random.permutation(g1.values)\n",
    "    #     shuffled_g2 = np.random.permutation(g2.values)\n",
    "    #     ks_statistic = ks_w2(shuffled_g1, shuffled_g2, w1.values, w2.values)\n",
    "    #     permuted_ks_stats.append(ks_statistic)\n",
    "\n",
    "    # observed_ks_statistic = ks_w2(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # p_value = (np.sum(np.abs(permuted_ks_stats) >= np.abs(observed_ks_statistic)) + 1) / (n_permutations + 1)\n",
    "    # t = observed_ks_statistic\n",
    "\n",
    "    # t, p = ks_weighted(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # g1 = np.repeat(g1, w1)\n",
    "    # g2 = np.repeat(g2, w2)\n",
    "    t, p = mannwhitneyu(g1, g2, alternative='greater')\n",
    "    # t = res[0]\n",
    "    # p = res[1]\n",
    "    antvsb6 = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot[metric] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.01\n",
    "        ax.plot([0, 0, 1, 1], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.315,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.315,0.85, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs B6: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    # w1 = w1 / np.sum(w1)\n",
    "    # w2 = w2 / np.sum(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    # t = res[0]\n",
    "    # p = res[1]\n",
    "    g1 = to_plot[to_plot['group'] == gps[0]][metric]\n",
    "    g2 = to_plot[to_plot['group'] == gps[2]][metric]\n",
    "    # observed_statistic = np.sum(stats.rankdata(g1) * w1) - (np.sum(w1) * (np.sum(w1) + 1) / 2)\n",
    "    # weighted_ranksum_g1 = np.sum(stats.rankdata(g1) * w1)\n",
    "    # weighted_ranksum_g2 = np.sum(stats.rankdata(g2) * w2)\n",
    "    # observed_statistic = weighted_ranksum_g1 - weighted_ranksum_g2\n",
    "    # npermute = 10000\n",
    "    # permuted_stats = []\n",
    "    # for _ in range(npermute):\n",
    "    #     combined_data = np.concatenate((g1, g2))\n",
    "    #     combined_weights = np.concatenate((w1, w2))\n",
    "    #     np.random.shuffle(combined_data)\n",
    "    #     permuted_statistic = np.sum(np.multiply(stats.rankdata(combined_data), combined_weights))\n",
    "    #     permuted_stats.append(permuted_statistic)\n",
    "\n",
    "    # p = (np.sum(np.abs(permuted_stats) >= np.abs(observed_statistic)) + 1) / (npermute + 1)\n",
    "    # t = observed_statistic\n",
    "\n",
    "    # n_permutations = 10000\n",
    "    # permuted_ks_stats = []\n",
    "\n",
    "    # for _ in range(n_permutations):\n",
    "    #     shuffled_g1 = np.random.permutation(g1.values)\n",
    "    #     shuffled_g2 = np.random.permutation(g2.values)\n",
    "    #     ks_statistic = ks_w2(shuffled_g1, shuffled_g2, w1.values, w2.values)\n",
    "    #     permuted_ks_stats.append(ks_statistic)\n",
    "\n",
    "    # observed_ks_statistic = ks_w2(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # p_value = (np.sum(np.abs(permuted_ks_stats) >= np.abs(observed_ks_statistic)) + 1) / (n_permutations + 1)\n",
    "    # t = observed_ks_statistic\n",
    "\n",
    "    # t, p = ks_weighted(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # g1 = np.repeat(g1, w1)\n",
    "    # g2 = np.repeat(g2, w2)\n",
    "    t, p = mannwhitneyu(g1, g2, alternative='greater')\n",
    "    antvsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.02\n",
    "        ax.plot([0, 0, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5,0.925, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal', alternative='two-sided')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    # w1 = w1 / np.sum(w1)\n",
    "    # w2 = w2 / np.sum(w2)\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "    #                 usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    # t = res[0]\n",
    "    # p = res[1]\n",
    "    g1 = to_plot[to_plot['group'] == gps[1]][metric]\n",
    "    g2 = to_plot[to_plot['group'] == gps[2]][metric]\n",
    "    # observed_statistic = np.sum(stats.rankdata(g1) * w1) - (np.sum(w1) * (np.sum(w1) + 1) / 2)\n",
    "    # weighted_ranksum_g1 = np.sum(stats.rankdata(g1) * w1)\n",
    "    # weighted_ranksum_g2 = np.sum(stats.rankdata(g2) * w2)\n",
    "    # observed_statistic = weighted_ranksum_g1 - weighted_ranksum_g2\n",
    "\n",
    "    # npermute = 10000\n",
    "    # permuted_stats = []\n",
    "    # for _ in range(npermute):\n",
    "    #     combined_data = np.concatenate((g1, g2))\n",
    "    #     combined_weights = np.concatenate((w1, w2))\n",
    "    #     np.random.shuffle(combined_data)\n",
    "    #     permuted_statistic = np.sum(np.multiply(stats.rankdata(combined_data), combined_weights))\n",
    "    #     permuted_stats.append(permuted_statistic)\n",
    "    \n",
    "    # p = (np.sum(np.abs(permuted_stats) >= np.abs(observed_statistic)) + 1) / (npermute + 1)\n",
    "    # t = observed_statistic\n",
    "\n",
    "    # t, p = ks_weighted(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # n_permutations = 10000\n",
    "    # permuted_ks_stats = []\n",
    "\n",
    "    # for _ in range(n_permutations):\n",
    "    #     shuffled_g1 = np.random.permutation(g1.values)\n",
    "    #     shuffled_g2 = np.random.permutation(g2.values)\n",
    "    #     ks_statistic = ks_w2(shuffled_g1, shuffled_g2, w1.values, w2.values)\n",
    "    #     permuted_ks_stats.append(ks_statistic)\n",
    "\n",
    "    # observed_ks_statistic = ks_w2(g1.values, g2.values, w1.values, w2.values)\n",
    "\n",
    "    # p_value = (np.sum(np.abs(permuted_ks_stats) >= np.abs(observed_ks_statistic)) + 1) / (n_permutations + 1)\n",
    "    # t = observed_ks_statistic\n",
    "\n",
    "\n",
    "    # g1 = np.repeat(g1, w1)\n",
    "    # g2 = np.repeat(g2, w2)\n",
    "    t, p = mannwhitneyu(g1, g2, alternative='greater')\n",
    "    b6vsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += .03\n",
    "        ax.plot([1, 1, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('B6 vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "\n",
    "    # BH correction\n",
    "    pvals = [antvsb6, antvsnon, b6vsnon]\n",
    "    pval_names = ['antvsb6', 'antvsnon', 'b6vsnon']\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "    print(pval_names)\n",
    "    print('Corrected p-values: ' + str(reject))\n",
    "\n",
    "\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_title(titles_to_use[i])\n",
    "    ax.set_ylabel('Wasserstein quantiles')\n",
    "\n",
    "    group_order = ['B6', 'ANT', 'NON']  # 'B6' becomes the reference group\n",
    "    to_plot['group'] = pd.Categorical(to_plot['group'], categories=group_order, ordered=True)\n",
    "\n",
    "    # model = sm.MixedLM.from_formula(metric + ' ~ group', data=to_plot, groups=to_plot['name'])\n",
    "    # formula = metric + ' ~ C(group):C(group)'\n",
    "\n",
    "    # to_plot_sub = to_plot[to_plot['group'] != 'NON']\n",
    "    # to_plot_sub = to_plot_sub[to_plot_sub['group'] != 'B6']\n",
    "    # to_plot_sub['group'] = pd.Categorical(to_plot_sub['group'], categories=['NON', 'ANT'], ordered=True)\n",
    "    to_plot_sub = to_plot\n",
    "    to_plot_sub['combined_group'] = to_plot_sub['session_id'] + '_' + to_plot_sub['name']\n",
    "    # formula = metric + ' ~ C(session_id)'\n",
    "    formula = metric + ' ~ C(group):C(group)'\n",
    "    model = sm.MixedLM.from_formula(formula, data=to_plot_sub, groups=to_plot_sub['combined_group'])\n",
    "\n",
    "    result = model.fit()\n",
    "\n",
    "    # robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "    # result = robust_model.fit()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(score)\n",
    "    print(result.summary())\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['red', 'blue', 'green']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "\n",
    "    # # every row for that score\n",
    "    to_plot_single = df[df['score'] == score]\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # # scores averaged for each session\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    # get group means + CI\n",
    "    means = to_plot.groupby('group')[metric].mean().round(2)\n",
    "    stds = to_plot.groupby('group')[metric].std()\n",
    "    n = to_plot.groupby('group')[metric].count()\n",
    "    sems = stds / np.sqrt(n)\n",
    "    sems = sems.round(2)\n",
    "    \n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gps[k]][metric], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "    # do t test \n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],usevar='unequal')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    w1 = w1 / np.mean(w1)\n",
    "    w2 = w2 / np.mean(w2)\n",
    "    res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[1]][metric],\n",
    "                    usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    antvsb6 = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot[metric] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.01\n",
    "        ax.plot([0, 0, 1, 1], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.315,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.315,0.85, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs B6: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[0]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    w1 = w1 / np.mean(w1)\n",
    "    w2 = w2 / np.mean(w2)\n",
    "    res = ttest_ind(to_plot[to_plot['group'] == gps[0]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "                    usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    antvsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += 0.02\n",
    "        ax.plot([0, 0, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5,0.925, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5,0.9, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('ANT vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "    # res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],usevar='unequal', alternative='two-sided')\n",
    "    w1 = to_plot_count[to_plot_count['group'] == gps[1]][metric]\n",
    "    w2 = to_plot_count[to_plot_count['group'] == gps[2]][metric]\n",
    "    w1 = w1 / np.mean(w1)\n",
    "    w2 = w2 / np.mean(w2)\n",
    "    res = ttest_ind(to_plot[to_plot['group'] == gps[1]][metric], to_plot[to_plot['group'] == gps[2]][metric],\n",
    "                    usevar='unequal', weights=(w1, w2), alternative='larger')\n",
    "    t = res[0]\n",
    "    p = res[1]\n",
    "    b6vsnon = p\n",
    "    if p <= 0.05:\n",
    "        mx = np.max([np.max(to_plot[to_plot['group'] == gps[0]][metric]),  np.max(to_plot[to_plot['group'] == gps[1]][metric]), np.max(to_plot[to_plot['group'] == gps[2]][metric])])\n",
    "        mx += .03\n",
    "        ax.plot([1, 1, 2, 2], [mx, mx+.05, mx+.05,mx], lw=1.5, c='k')\n",
    "        if score != 'field' and score !='binary':\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "        else:\n",
    "            ax.text(0.5+0.315/2,0.95, '*', transform=ax.transAxes, fontsize=25, color='k')\n",
    "    print('B6 vs NON: t = ' + str(t) + ', p = ' + str(p))\n",
    "\n",
    "    # BH correction\n",
    "    pvals = [antvsb6, antvsnon, b6vsnon]\n",
    "    pval_names = ['antvsb6', 'antvsnon', 'b6vsnon']\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "    print(pval_names)\n",
    "    print('Corrected p-values: ' + str(reject))\n",
    "\n",
    "\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_title(titles_to_use[i])\n",
    "    ax.set_ylabel('Wasserstein distances (cm)')\n",
    "\n",
    "    group_order = ['B6', 'ANT', 'NON']  # 'B6' becomes the reference group\n",
    "    to_plot['group'] = pd.Categorical(to_plot['group'], categories=group_order, ordered=True)\n",
    "\n",
    "    model = sm.MixedLM.from_formula(metric + ' ~ group', data=to_plot, groups=to_plot['name'])\n",
    "    # result = model.fit()\n",
    "\n",
    "    robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "    result = robust_model.fit()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(score)\n",
    "    print(result.summary())\n",
    "\n",
    "fig.suptitle('Averaged by Session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access fixed effects coefficients\n",
    "fixed_effects = result.params\n",
    "print(\"Fixed Effects Coefficients:\\n\", fixed_effects)\n",
    "\n",
    "# Access random effects variances\n",
    "random_effects_variances = result.cov_re\n",
    "print(\"Random Effects Variances:\\n\", random_effects_variances)\n",
    "\n",
    "# Predicted values\n",
    "predicted_values = result.fittedvalues\n",
    "print(\"Predicted Values:\\n\", predicted_values)\n",
    "\n",
    "# Likelihood ratio test for random effects significance\n",
    "print(\"Likelihood Ratio Test for Random Effects Significance:\\n\")\n",
    "print(result.summary().tables[1])\n",
    "\n",
    "# Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "print(\"AIC:\", result.aic)\n",
    "print(\"BIC:\", result.bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "\n",
    "    # # every row for that score\n",
    "    # to_plot = df[df['score'] == score]\n",
    "    # to_plot_shuffle = to_plot.copy()\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # scores averaged for each session\n",
    "    to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            # means = to_plot.groupby('session_id')[metric].mean().round(2)\n",
    "            # stds = to_plot.groupby('session_id')[metric].std()\n",
    "            # n = to_plot.groupby('session_id')[metric].count()\n",
    "            # sems = stds / np.sqrt(n)\n",
    "            # sems = sems.round(2)\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "                        \n",
    "            # lbls.append(str(means[j]) + ' ± ' + str(sems[j]) + ' cm, N = ' + str(n[j]))\n",
    "\n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "            \n",
    "    ax.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        if empirical[j] > np.mean(shuffled):\n",
    "            tag = 'greater'\n",
    "        elif empirical[j] < np.mean(shuffled):\n",
    "            tag = 'lower'\n",
    "        lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        else:\n",
    "            lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Session')\n",
    "    ax.set_title(titles_to_use[i])\n",
    "    ax.set_ylabel('Wasserstein distnaces (cm)')\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary', 'centroid', 'firing_rate']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1], [2,0], [2,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary', 'Centroid', 'Firing Rate']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(23, 46))\n",
    "gs_main = gridspec.GridSpec(3, 2, width_ratios=[1,1], height_ratios=[1,1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "# metric = 'obj_w'\n",
    "metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    # scores averaged for each session\n",
    "    if score != 'firing_rate':\n",
    "        to_plot = df[df['score'] == score]\n",
    "        # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "    else:\n",
    "        to_plot = df[df['score'] == 'whole']\n",
    "        # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(5, 1, subplot_spec=gs_main[row, col], height_ratios=[12,12,1,1,1], hspace=0.1)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    axf = ax\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    means = []\n",
    "    sems = []\n",
    "    n = []\n",
    "    comps = {}\n",
    "    comp_maxs = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        if score != 'firing_rate':\n",
    "            mtouse = metric \n",
    "        else:\n",
    "            mtouse = 'firing_rate'\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gp_labels[k]][mtouse], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "        comp_maxs.append(np.max(to_plot[to_plot['group'] == gp_labels[k]][mtouse]))\n",
    "        for k2 in range(3):\n",
    "            if k2 != k:\n",
    "                lbl_pair = np.sort([gp_labels[k], gp_labels[k2]])\n",
    "                comp_group = lbl_pair[0] + '_' + lbl_pair[1]\n",
    "                if comp_group not in comps.keys():\n",
    "                    comps[comp_group] = np.nan\n",
    "\n",
    "                    if 'B6' in comp_group and 'ANT' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'ANT'])]\n",
    "                        group_order = ['B6', 'ANT']  # 'B6' becomes the reference group\n",
    "                    elif 'B6' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'NON'])]\n",
    "                        group_order = ['B6', 'NON']\n",
    "                    elif 'ANT' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['NON', 'ANT'])]\n",
    "                        group_order = ['NON', 'ANT']\n",
    "                    # model_data['group'] = pd.Categorical(model_data['group'], categories=group_order, ordered=True)\n",
    "                    model_data.loc[:, 'group'] = pd.Categorical(model_data.loc[:, 'group'], categories=group_order, ordered=True)\n",
    "\n",
    "                    if score != 'firing_rate':\n",
    "                        # model = sm.MixedLM.from_formula(metric + ' ~ group', data=model_data, groups=model_data['name'])\n",
    "\n",
    "                        # https://stats.stackexchange.com/questions/31300/dealing-with-0-1-values-in-a-beta-regression\n",
    "                        # 𝑥′=𝑥(𝑁−1)+𝑠𝑁\n",
    "                        # where N is the sample size and s is a constant between 0 and 1. \n",
    "                        # From a Bayesian standpoint, s acts as if we are taking a prior into account. \n",
    "                        # A reasonable choice for s would be .5.\n",
    "\n",
    "                        model_data['obj_q'] = (model_data['obj_q'] * (len(model_data['obj_q']) - 1) + .5) / len(model_data['obj_q'])\n",
    "                        model = BetaModel.from_formula(metric + ' ~ group', data=model_data, groups=model_data['name'])\n",
    "\n",
    "                    else:\n",
    "                        # model = sm.MixedLM.from_formula('firing_rate ~ group', data=model_data, groups=model_data['name'])\n",
    "\n",
    "                        model_data['obj_q'] = (model_data['obj_q'] * (len(model_data['obj_q']) - 1) + .5) / len(model_data['obj_q'])\n",
    "                        model = BetaModel.from_formula('firing_rate ~ group', data=model_data, groups=model_data['name'])\n",
    "                    result = model.fit()\n",
    "                    # result = model.fit()\n",
    "                    # robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "                    # result = robust_model.fit()\n",
    "                    emp_coeff = result.params[1]\n",
    "\n",
    "                                        # Create a dictionary to map animal names to groups\n",
    "                    itms = model_data.groupby(['name','group']).groups.keys()\n",
    "                    unique_animal_group_pairs_dict = {itm[0]: itm[1] for itm in itms}\n",
    "\n",
    "\n",
    "                    bootstrap_coeffs = []\n",
    "                    prev_sample = None\n",
    "                    for b in range(1000):\n",
    "                        resampled_animal_group_pairs = np.random.choice(list(unique_animal_group_pairs_dict.keys()), size=len(unique_animal_group_pairs_dict), replace=True)\n",
    "                        sample = None\n",
    "                        for pr in resampled_animal_group_pairs:\n",
    "                            if sample is None:\n",
    "                                sample = model_data[model_data['name'] == pr]\n",
    "                                sample = sample.sample(frac=1, replace=True)\n",
    "                            else:\n",
    "                                sp = model_data[model_data['name'] == pr]\n",
    "                                sp = sp.sample(frac=1, replace=True)\n",
    "                                sample = pd.concat([sample, sp])\n",
    "                        # check sample and prev_sample are NOT the same\n",
    "                        if prev_sample is not None:\n",
    "                            assert not prev_sample.equals(sample), 'Sample and previous sample are the same'\n",
    "\n",
    "                        # model = sm.MixedLM.from_formula(metric + ' ~ group', data=sample, groups=sample['name'])\n",
    "                        # robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "                        if score != 'firing_rate':\n",
    "                            model = BetaModel.from_formula(metric + '~ group', data=sample, groups=sample['name'])\n",
    "                        else:\n",
    "                            model = BetaModel.from_formula('firing_rate ~ group', data=sample, groups=sample['name'])\n",
    "                        result = model.fit()\n",
    "                        coeff = result.params[1]\n",
    "                        bootstrap_coeffs.append(coeff)\n",
    "                    p = np.sum(np.abs(bootstrap_coeffs) > np.abs(emp_coeff)) / len(bootstrap_coeffs)\n",
    "                    comps[comp_group] = p\n",
    "\n",
    "        means.append(np.mean(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "        sems.append(np.std(to_plot[to_plot['group'] == gp_labels[k]][metric]) / np.sqrt(len(to_plot[to_plot['group'] == gp_labels[k]][metric])))\n",
    "        n.append(len(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        # lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "        lbls.append(gp_labels[k])\n",
    "        #  + ': N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gp_labels)\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('EMD quantile')\n",
    "\n",
    "    # benjamini hochberg correction\n",
    "    kys, vals = zip(*comps.items())\n",
    "    accepted, pvals_corrected, _, _ = multipletests(vals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    \n",
    "    p_count = 0\n",
    "    for comp_key, val in comps.items():\n",
    "        comparison = comps[comp_key]\n",
    "        if 'ANT' in comp_key and 'B6' in comp_key:\n",
    "            nme = [0,1]\n",
    "        elif 'ANT' in comp_key and 'NON' in comp_key:\n",
    "            nme = [0,2]\n",
    "        elif 'B6' in comp_key and 'NON' in comp_key:\n",
    "            nme = [1,2]\n",
    "\n",
    "        # if accepted[k]:\n",
    "        barplot_annotate_brackets(nme[0],nme[1],pvals_corrected[p_count],[0,1,2], comp_maxs, maxasterix=5)\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # every row for that score\n",
    "    if score != 'firing_rate':\n",
    "        to_plot = df[df['score'] == score]\n",
    "        # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    else:\n",
    "        to_plot = df[df['score'] == 'whole']\n",
    "        # to_plot = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        metric = 'firing_rate'\n",
    "        # to_plot = to_plot[to_plot['depth']]\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    group_ses_frs = {'ANT': [], 'B6': [], 'NON': []}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        # ses_frs = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]][metric]\n",
    "            # ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]]['information']\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            for j2 in range(3):\n",
    "                to_plot_now2 = to_plot[to_plot['group'] == gp_labels[j2]]\n",
    "                if j != j2:\n",
    "                    # welch's t-test\n",
    "                    if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 1 and len(to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric]) > 1:\n",
    "                        # res = ttest_ind(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                        #                     to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                        #                     usevar='unequal', alternative='two-sided')\n",
    "                        # pvalue = res[1]\n",
    "\n",
    "                        # mann whitney u test\n",
    "                        _, pvalue = mannwhitneyu(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                                           to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                                          alternative='two-sided')\n",
    "                         \n",
    "                    \n",
    "                        sorted_labels = np.sort([gp_labels[j], gp_labels[j2]])\n",
    "                        pair_id = sorted_labels[0] + '_' + sorted_labels[1]\n",
    "                        if pair_id not in session_comps:\n",
    "                            session_comps[pair_id] = []\n",
    "                        if pair_id not in ses_visited:\n",
    "                            session_comps[pair_id].append(pvalue)\n",
    "                            # session_comps[pair_id].append(pvalue)\n",
    "                            ses_visited.append(pair_id)\n",
    "            if len(ses_fr) > 0 and np.mean(ses_fr) == np.mean(ses_fr):       \n",
    "                group_ses_frs[gp_labels[j]].append(np.mean(ses_fr))\n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    from scipy import interpolate\n",
    "    from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT_B6 = np.array(session_comps['ANT_B6'])\n",
    "    pvalsB6_NON = np.array(session_comps['B6_NON'])\n",
    "    pvalsANT_NON = np.array(session_comps['ANT_NON'])\n",
    "    pvals = np.concatenate((pvalsANT_B6, pvalsB6_NON, pvalsANT_NON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant_b6 = np.ones(len(pvalsANT_B6))\n",
    "    bh_ant_b6[pvals_corrected[:len(pvalsANT_B6)] <= 0.05] = 0\n",
    "    bh_b6_non = np.ones(len(pvalsB6_NON))\n",
    "    bh_b6_non[pvals_corrected[len(pvalsANT_B6):len(pvalsANT_B6)+len(pvalsB6_NON)] <= 0.05] = 0\n",
    "    bh_ant_non = np.ones(len(pvalsANT_NON))\n",
    "    bh_ant_non[pvals_corrected[len(pvalsANT_B6)+len(pvalsB6_NON):] <= 0.05] = 0\n",
    "\n",
    "    # plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    # bh_ant_b6 = np.hstack((bh_ant_b6, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ant_fr = group_ses_frs['ANT']\n",
    "    ant_interp = np.linspace(0, len(ant_fr)-1, 1000)\n",
    "    ant_fr_smooth = np.interp(ant_interp, np.arange(len(ant_fr)), ant_fr)\n",
    "    im = ax.imshow(np.expand_dims(ant_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('ANT', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] -.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('ANT-B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    # bh_b6_non = np.hstack((bh_b6_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_b6_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    b6_fr = group_ses_frs['B6']\n",
    "    b6_interp = np.linspace(0, len(b6_fr)-1, 1000)\n",
    "    b6_fr_smooth = np.interp(b6_interp, np.arange(len(b6_fr)), b6_fr)\n",
    "    im = ax.imshow(np.expand_dims(b6_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('B6', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1]-.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('B6-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[4])\n",
    "    # # bh_ant_non = np.hstack((bh_ant_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    non_fr = group_ses_frs['NON']\n",
    "    non_interp = np.linspace(0, len(non_fr)-1, 1000)\n",
    "    non_fr_smooth = np.interp(non_interp, np.arange(len(non_fr)), non_fr)\n",
    "    im = ax.imshow(np.expand_dims(non_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('NON', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] - .5))\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    # ylbl = ax.set_ylabel('ANT-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    lbl_colors = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        # if empirical[j] > np.mean(shuffled):\n",
    "        #     tag = 'greater'\n",
    "        # elif empirical[j] < np.mean(shuffled):\n",
    "        #     tag = 'lower'\n",
    "        # lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        # lbls.append(lbl)\n",
    "\n",
    "        # lbl = gp_labels[j] + ' \n",
    "        lbl = 'slope: ' + str(np.round(memp, 2)) \n",
    "        lbl_colors.append('k')\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbl_colors[cc] = 'r'\n",
    "            # lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        # else:\n",
    "            # lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax1.legend(bps, lbls, loc='upper right')\n",
    "    # color label text in legend\n",
    "    for text, color in zip(ax1.legend_.get_texts(), lbl_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    # ax1.set_title(score)\n",
    "    # ax.set_xlabel('Session')\n",
    "    axf.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('EMD quantile')\n",
    "              \n",
    "    # ax1.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    # ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-1.25/2, len(gps) * 3 - 1.25/2])\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "    ax1.set_ylim(-0.05,)\n",
    "    # ax1.set_xlim([-.5/2, len(gps) * 3 - .5/2])\n",
    "\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances', fontweight='bold')\n",
    "# fig.suptitle('Averaged by session', fontweight='bold')\n",
    "gs_main.tight_layout(fig, rect=[0, 0, 1, 0.98])\n",
    "# fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bootstrap_coeffs, bins=100)\n",
    "plt.vlines(emp_coeff,0,100, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary', 'centroid', 'firing_rate']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1], [2,0], [2,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary', 'Centroid', 'Firing Rate']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(23, 46))\n",
    "gs_main = gridspec.GridSpec(3, 2, width_ratios=[1,1], height_ratios=[1,1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "np.random.seed(0)\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    # scores averaged for each session\n",
    "    if score != 'firing_rate':\n",
    "        to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "    else:\n",
    "        to_plot = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(5, 1, subplot_spec=gs_main[row, col], height_ratios=[12,12,1,1,1], hspace=0.1)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    axf = ax\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    means = []\n",
    "    sems = []\n",
    "    n = []\n",
    "    comps = {}\n",
    "    comp_maxs = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        if score != 'firing_rate':\n",
    "            mtouse = metric \n",
    "        else:\n",
    "            mtouse = 'firing_rate'\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gp_labels[k]][mtouse], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "        comp_maxs.append(np.max(to_plot[to_plot['group'] == gp_labels[k]][mtouse]))\n",
    "        for k2 in range(3):\n",
    "            if k2 != k:\n",
    "                lbl_pair = np.sort([gp_labels[k], gp_labels[k2]])\n",
    "                comp_group = lbl_pair[0] + '_' + lbl_pair[1]\n",
    "                if comp_group not in comps.keys():\n",
    "                    comps[comp_group] = np.nan\n",
    "\n",
    "                    if 'B6' in comp_group and 'ANT' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'ANT'])]\n",
    "                        group_order = ['B6', 'ANT']  # 'B6' becomes the reference group\n",
    "                    elif 'B6' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'NON'])]\n",
    "                        group_order = ['B6', 'NON']\n",
    "                    elif 'ANT' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['NON', 'ANT'])]\n",
    "                        group_order = ['NON', 'ANT']\n",
    "                    # model_data['group'] = pd.Categorical(model_data['group'], categories=group_order, ordered=True)\n",
    "                    model_data.loc[:, 'group'] = pd.Categorical(model_data.loc[:, 'group'], categories=group_order, ordered=True)\n",
    "\n",
    "                    if score != 'firing_rate':\n",
    "                        model = sm.MixedLM.from_formula(metric + ' ~ group', data=model_data, groups=model_data['name'])\n",
    "                        # model = BetaModel.from_formula(metric + ' ~ group', data=model_data, groups=model_data['name'])\n",
    "                    else:\n",
    "                        model = sm.MixedLM.from_formula('firing_rate ~ group', data=model_data, groups=model_data['name'])\n",
    "                        # model = BetaModel.from_formula('firing_rate ~ group', data=model_data, groups=model_data['name'])\n",
    "                    # result = model.fit()\n",
    "                    robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "                    result = robust_model.fit()\n",
    "                    emp_coeff = result.params[1]\n",
    "\n",
    "                    # Create a dictionary to map animal names to groups\n",
    "                    itms = model_data.groupby(['name','group']).groups.keys()\n",
    "                    unique_animal_group_pairs_dict = {itm[0]: itm[1] for itm in itms}\n",
    "\n",
    "\n",
    "                    bootstrap_coeffs = []\n",
    "                    prev_sample = None\n",
    "                    for b in range(1000):\n",
    "                        resampled_animal_group_pairs = np.random.choice(list(unique_animal_group_pairs_dict.keys()), size=len(unique_animal_group_pairs_dict), replace=True)\n",
    "                        sample = None\n",
    "                        for pr in resampled_animal_group_pairs:\n",
    "                            if sample is None:\n",
    "                                sample = model_data[model_data['name'] == pr]\n",
    "                                sample = sample.sample(frac=1, replace=True)\n",
    "                            else:\n",
    "                                sp = model_data[model_data['name'] == pr]\n",
    "                                sp = sp.sample(frac=1, replace=True)\n",
    "                                sample = pd.concat([sample, sp])\n",
    "                        # check sample and prev_sample are NOT the same\n",
    "                        if prev_sample is not None:\n",
    "                            assert not prev_sample.equals(sample), 'Sample and previous sample are the same'\n",
    "                        # bootstrap for p-value\n",
    "                        # sample = model_data.sample(frac=1, replace=True) \n",
    "                        if score != 'firing_rate':\n",
    "                            model = sm.MixedLM.from_formula(metric + ' ~ group', data=sample, groups=sample['name'])\n",
    "                        else:\n",
    "                            model = sm.MixedLM.from_formula('firing_rate ~ group', data=sample, groups=sample['name'])\n",
    "                        robust_model = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT())\n",
    "                        result = robust_model.fit()\n",
    "                        coeff = result.params[1]\n",
    "                        bootstrap_coeffs.append(coeff)\n",
    "                        prev_sample = sample\n",
    "                    p = np.sum(np.abs(bootstrap_coeffs) > np.abs(emp_coeff)) / len(bootstrap_coeffs)\n",
    "                    comps[comp_group] = p\n",
    "\n",
    "        means.append(np.mean(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "        sems.append(np.std(to_plot[to_plot['group'] == gp_labels[k]][metric]) / np.sqrt(len(to_plot[to_plot['group'] == gp_labels[k]][metric])))\n",
    "        n.append(len(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        # lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "        lbls.append(gp_labels[k])\n",
    "        #  + ': N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gp_labels)\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('EMD (cm)')\n",
    "\n",
    "    print('comps here')\n",
    "    print(comps)\n",
    "\n",
    "\n",
    "    # benjamini hochberg correction\n",
    "    kys, vals = zip(*comps.items())\n",
    "    accepted, pvals_corrected, _, _ = multipletests(vals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    \n",
    "    p_count = 0\n",
    "    for comp_key, val in comps.items():\n",
    "        comparison = comps[comp_key]\n",
    "        if 'ANT' in comp_key and 'B6' in comp_key:\n",
    "            nme = [0,1]\n",
    "        elif 'ANT' in comp_key and 'NON' in comp_key:\n",
    "            nme = [0,2]\n",
    "        elif 'B6' in comp_key and 'NON' in comp_key:\n",
    "            nme = [1,2]\n",
    "\n",
    "        # if accepted[k]:\n",
    "        barplot_annotate_brackets(nme[0],nme[1],pvals_corrected[p_count],[0,1,2], comp_maxs, maxasterix=5)\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # every row for that score\n",
    "    if score != 'firing_rate':\n",
    "        # to_plot = df[df['score'] == score]\n",
    "        to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    else:\n",
    "        # to_plot = df[df['score'] == 'whole']\n",
    "        to_plot = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        metric = 'firing_rate'\n",
    "        # to_plot = to_plot[to_plot['depth']]\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    group_ses_frs = {'ANT': [], 'B6': [], 'NON': []}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        # ses_frs = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]][metric]\n",
    "            # ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]]['information']\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            for j2 in range(3):\n",
    "                to_plot_now2 = to_plot[to_plot['group'] == gp_labels[j2]]\n",
    "                if j != j2:\n",
    "                    # welch's t-test\n",
    "                    if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 1 and len(to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric]) > 1:\n",
    "                        # res = ttest_ind(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                        #                     to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                        #                     usevar='unequal', alternative='two-sided')\n",
    "                        # pvalue = res[1]\n",
    "\n",
    "                        # mann whitney u test\n",
    "                        _, pvalue = mannwhitneyu(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                                           to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                                          alternative='two-sided')\n",
    "                         \n",
    "                    \n",
    "                        sorted_labels = np.sort([gp_labels[j], gp_labels[j2]])\n",
    "                        pair_id = sorted_labels[0] + '_' + sorted_labels[1]\n",
    "                        if pair_id not in session_comps:\n",
    "                            session_comps[pair_id] = []\n",
    "                        if pair_id not in ses_visited:\n",
    "                            session_comps[pair_id].append(pvalue)\n",
    "                            # session_comps[pair_id].append(pvalue)\n",
    "                            ses_visited.append(pair_id)\n",
    "            if len(ses_fr) > 0 and np.mean(ses_fr) == np.mean(ses_fr):       \n",
    "                group_ses_frs[gp_labels[j]].append(np.mean(ses_fr))\n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    from scipy import interpolate\n",
    "    from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT_B6 = np.array(session_comps['ANT_B6'])\n",
    "    pvalsB6_NON = np.array(session_comps['B6_NON'])\n",
    "    pvalsANT_NON = np.array(session_comps['ANT_NON'])\n",
    "    pvals = np.concatenate((pvalsANT_B6, pvalsB6_NON, pvalsANT_NON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant_b6 = np.ones(len(pvalsANT_B6))\n",
    "    bh_ant_b6[pvals_corrected[:len(pvalsANT_B6)] <= 0.05] = 0\n",
    "    bh_b6_non = np.ones(len(pvalsB6_NON))\n",
    "    bh_b6_non[pvals_corrected[len(pvalsANT_B6):len(pvalsANT_B6)+len(pvalsB6_NON)] <= 0.05] = 0\n",
    "    bh_ant_non = np.ones(len(pvalsANT_NON))\n",
    "    bh_ant_non[pvals_corrected[len(pvalsANT_B6)+len(pvalsB6_NON):] <= 0.05] = 0\n",
    "\n",
    "    # plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    # bh_ant_b6 = np.hstack((bh_ant_b6, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ant_fr = group_ses_frs['ANT']\n",
    "    ant_interp = np.linspace(0, len(ant_fr)-1, 1000)\n",
    "    ant_fr_smooth = np.interp(ant_interp, np.arange(len(ant_fr)), ant_fr)\n",
    "    im = ax.imshow(np.expand_dims(ant_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('ANT', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] -.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('ANT-B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    # bh_b6_non = np.hstack((bh_b6_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_b6_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    b6_fr = group_ses_frs['B6']\n",
    "    b6_interp = np.linspace(0, len(b6_fr)-1, 1000)\n",
    "    b6_fr_smooth = np.interp(b6_interp, np.arange(len(b6_fr)), b6_fr)\n",
    "    im = ax.imshow(np.expand_dims(b6_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('B6', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1]-.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('B6-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[4])\n",
    "    # # bh_ant_non = np.hstack((bh_ant_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    non_fr = group_ses_frs['NON']\n",
    "    non_interp = np.linspace(0, len(non_fr)-1, 1000)\n",
    "    non_fr_smooth = np.interp(non_interp, np.arange(len(non_fr)), non_fr)\n",
    "    im = ax.imshow(np.expand_dims(non_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('NON', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] - .5))\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    # ylbl = ax.set_ylabel('ANT-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    lbl_colors = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        # if empirical[j] > np.mean(shuffled):\n",
    "        #     tag = 'greater'\n",
    "        # elif empirical[j] < np.mean(shuffled):\n",
    "        #     tag = 'lower'\n",
    "        # lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        # lbls.append(lbl)\n",
    "\n",
    "        # lbl = gp_labels[j] + ' \n",
    "        lbl = 'slope: ' + str(np.round(memp, 2)) \n",
    "        lbl_colors.append('k')\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbl_colors[cc] = 'r'\n",
    "            # lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        # else:\n",
    "            # lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax1.legend(bps, lbls, loc='upper right')\n",
    "    # color label text in legend\n",
    "    for text, color in zip(ax1.legend_.get_texts(), lbl_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    # ax1.set_title(score)\n",
    "    # ax.set_xlabel('Session')\n",
    "    axf.set_title(titles_to_use[i], fontweight='bold')\n",
    "    ax1.set_ylabel('EMD (cm)')\n",
    "              \n",
    "    # ax1.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    # ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-1.25/2, len(gps) * 3 - 1.25/2])\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "    ax1.set_ylim(-0.05,)\n",
    "    # ax1.set_xlim([-.5/2, len(gps) * 3 - .5/2])\n",
    "\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances', fontweight='bold')\n",
    "# fig.suptitle('Averaged by session', fontweight='bold')\n",
    "gs_main.tight_layout(fig, rect=[0, 0, 1, 0.98])\n",
    "# fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ses_avg_whole = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "indiv_whole = df[df['score'] == 'whole']\n",
    "\n",
    "ses_avg_sd = df[df['score'] == 'spike_density'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "indiv_sd = df[df['score'] == 'spike_density']\n",
    "\n",
    "ses_avg_field = df[df['score'] == 'field'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "indiv_field = df[df['score'] == 'field']\n",
    "\n",
    "ses_avg_binary = df[df['score'] == 'binary'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "indiv_binary = df[df['score'] == 'binary']\n",
    "\n",
    "ses_avg_centroid = df[df['score'] == 'centroid'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "indiv_centroid = df[df['score'] == 'centroid']\n",
    "\n",
    "# all above are for obj_w AND obj_q. First one ('whole') also includes firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list(unique_animal_group_pairs_dict.keys()), size=len(unique_animal_group_pairs_dict), replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_animal_group_pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_animal_group_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_animal_group_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                #    unique_animal_group_pairs = list(zip(model_data['name'].unique(), model_data['group'].unique()))\n",
    "                #     unique_animal_group_pairs_dict = {animal: group for animal, group in unique_animal_group_pairs}\n",
    "\n",
    "\n",
    "                #     bootstrap_coeffs = []\n",
    "                #     for b in range(1000):\n",
    "                #         resampled_animal_group_pairs = np.random.choice(['ANT','NON','B6'], size=len(unique_animal_group_pairs_dict), replace=True)\n",
    "                #         sample = model_data[model_data['group'].isin(resampled_animal_group_pairs)]\n",
    "                #         print(sample)\n",
    "                #         sample = sample.groupby(['name']).apply(lambda x: x.sample(frac=1, replace=True)).reset_index(drop=True)\n",
    "                #         # bootstrap for p-value\n",
    "\n",
    "\n",
    "unique_animal_group_pairs           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.othermod.betareg import BetaModel\n",
    "import numpy as np\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "groups = np.random.choice(['ANT', 'B6'], size=n_samples)\n",
    "names = np.random.choice(['A', 'B', 'C'], size=n_samples)\n",
    "firing_rates = np.random.beta(2, 5, size=n_samples)  # Simulated beta-distributed firing rates\n",
    "\n",
    "# Create a DataFrame for the data\n",
    "model_data = pd.DataFrame({'firing_rate': firing_rates, 'group': groups, 'name': names})\n",
    "\n",
    "# Fit a beta regression model\n",
    "model = BetaModel.from_formula('firing_rate ~ group', data=model_data, groups=model_data['name'])\n",
    "\n",
    "result = RLM(model.endog, model.exog, M=sm.robust.norms.HuberT()).fit()\n",
    "# Summary of the model\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bootstrap_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11517986/indicating-the-statistically-significant-difference-in-bar-graph\n",
    "\n",
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05\n",
    "        # ** is p < 0.005\n",
    "        # *** is p < 0.0005\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        while data < p:\n",
    "            text += '*'\n",
    "            p /= 10.\n",
    "\n",
    "            if maxasterix and len(text) == maxasterix:\n",
    "                break\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black')\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=1, replace=True) \n",
    "print(len(sample), len(df))\n",
    "print(sample['group'].value_counts(), df['group'].value_counts())\n",
    "# check order \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_now.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    window_size = 3  # Adjust the window size as needed\n",
    "    ant_fr_smooth = np.convolve(ant_fr, np.ones(window_size)/window_size, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_fr_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import shapiro, kstest, mannwhitneyu, ttest_ind, ttest_rel, wilcoxon, ks_2samp, anderson_ksamp, anderson\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs_main = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "# metric = 'obj_w'\n",
    "metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(4, 1, subplot_spec=gs_main[row, col], height_ratios=[12,1,1,1])  \n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # # every row for that score\n",
    "    # to_plot = df[df['score'] == score]\n",
    "    # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # scores averaged for each session\n",
    "    to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            # bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "            #             notch=False, patch_artist=True,\n",
    "            #             boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "            #             capprops=dict(color='k'),\n",
    "            #             whiskerprops=dict(color='k'),\n",
    "            #             flierprops=dict(color='k', markeredgecolor='k'),\n",
    "            #             medianprops=dict(color='k'),\n",
    "            #             showmeans=False, \n",
    "            #             meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            # if k == 0:\n",
    "            #     bps.append(bp['boxes'][0])  \n",
    "\n",
    "            # kde = norm.pdf(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), np.std(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]))\n",
    "            # ax.plot([k*3+j*.5]+kde*3, to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "\n",
    "            if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 0:\n",
    "                kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(to_plot_now[to_plot_now['session_id'] == gps[k]][metric].values.reshape(-1, 1))\n",
    "                density = np.exp(kde.score_samples(np.linspace(0, 1, 100).reshape(-1, 1)))\n",
    "                ax.plot([k*3+j*.5]+density, np.linspace(0, 1, 100), color=gp_colors[j], alpha=.5)\n",
    "                # ax.plot([k*3+j*.5]+density*3, to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "                # ax.plot([k*3+j*.5]+kde.pdf(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "                if gp_labels[j] == 'ANT':\n",
    "                    ax.text(k*3+j*.5, 0.9, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "                elif gp_labels[j] == 'B6':\n",
    "                    median_index = np.argsort(density)[len(density) // 2]\n",
    "                    ax.text(k*3+j*.5, 0.5, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "                elif gp_labels[j] == 'NON':\n",
    "                    ax.text(k*3+j*.5, 0.1, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "\n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 3:\n",
    "                # shapiro-wilk test\n",
    "                stat, p = shapiro(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "                \n",
    "                # KS test\n",
    "                # stat, p = kstest(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], 'norm', args=(np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), np.std(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])))\n",
    "\n",
    "                # stat, crit, sig = anderson(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], dist='norm')\n",
    "                # print(gp_labels[j], gps[k], stat, crit, sig)\n",
    "                # if result.statistic > result.critical_values[result.significance_level == 5.0]:\n",
    "                #     p = 0\n",
    "                # else:\n",
    "                #     p = 1\n",
    "                    \n",
    "                if gp_labels[j] not in session_comps:\n",
    "                    session_comps[gp_labels[j]] = []\n",
    "                session_comps[gp_labels[j]].append(p)\n",
    "            # else:\n",
    "            #     if gp_labels[j] not in session_comps:\n",
    "            #         session_comps[gp_labels[j]] = []\n",
    "            #     session_comps[gp_labels[j]].append(np.nan)\n",
    "                \n",
    "                        \n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT = np.array(session_comps['ANT'])\n",
    "    pvalsB6 = np.array(session_comps['B6'])\n",
    "    pvalsNON = np.array(session_comps['NON'])\n",
    "    pvals = np.concatenate((pvalsANT, pvalsB6, pvalsNON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant = np.ones(len(pvalsANT))\n",
    "    bh_ant[pvals_corrected[:len(pvalsANT)] <= 0.05] = 0\n",
    "    bh_b6 = np.ones(len(pvalsB6))\n",
    "    bh_b6[pvals_corrected[len(pvalsANT):len(pvalsANT)+len(pvalsB6)] <= 0.05] = 0\n",
    "    bh_non = np.ones(len(pvalsNON))\n",
    "    bh_non[pvals_corrected[len(pvalsANT)+len(pvalsB6):] <= 0.05] = 0\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    # bh_ant = np.hstack((bh_ant, [1]))\n",
    "    ax.imshow(np.expand_dims(bh_ant, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('ANT', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    bh_b6 = np.hstack((bh_b6, [.5, .5]))\n",
    "    ax.imshow(np.expand_dims(bh_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    bh_non = np.hstack((bh_non, [.5]))\n",
    "    # bh_non = np.ones(7)\n",
    "    # bh_non[0] = 0\n",
    "    ax.imshow(np.expand_dims(bh_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ylbl = ax.set_ylabel('NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    " \n",
    "    ax1.set_title(score)\n",
    "    ax1.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('Quantile')\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.expand_dims(bh_non, 0), extent=[0, len(gps), 0, 1])\n",
    "plt.xticks(np.arange(len(gps)) + 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric], bins=20)\n",
    "# plt.show()\n",
    "# GRIN013_H16_M33_S54\n",
    "# sns.distplot(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric], bins=20)\n",
    "# ashapiorwilk\n",
    "# p = shapiro(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric])\n",
    "# normaltest\n",
    "# from scipy.stats import normaltest\n",
    "# p = normaltest(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric])\n",
    "# jarque_bera\n",
    "# from scipy.stats import jarque_bera\n",
    "# p = jarque_bera(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric])\n",
    "# lilliefors\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "p = lilliefors(to_plot_now[to_plot_now['session_id'] == 'session_1'][metric])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import shapiro, kstest, mannwhitneyu, ttest_ind, ttest_rel, wilcoxon, ks_2samp, anderson_ksamp, anderson\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "gs_main = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(4, 1, subplot_spec=gs_main[row, col], height_ratios=[12,1,1,1])  \n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # # every row for that score\n",
    "    # to_plot = df[df['score'] == score]\n",
    "    # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # scores averaged for each session\n",
    "    to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            # bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "            #             notch=False, patch_artist=True,\n",
    "            #             boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "            #             capprops=dict(color='k'),\n",
    "            #             whiskerprops=dict(color='k'),\n",
    "            #             flierprops=dict(color='k', markeredgecolor='k'),\n",
    "            #             medianprops=dict(color='k'),\n",
    "            #             showmeans=False, \n",
    "            #             meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            # if k == 0:\n",
    "            #     bps.append(bp['boxes'][0])  \n",
    "\n",
    "            # kde = norm.pdf(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), np.std(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]))\n",
    "            # ax.plot([k*3+j*.5]+kde*3, to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "\n",
    "            if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 0:\n",
    "                kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(to_plot_now[to_plot_now['session_id'] == gps[k]][metric].values.reshape(-1, 1))\n",
    "                lspcs = np.linspace(np.min(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), np.max(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), 20)\n",
    "                density = np.exp(kde.score_samples(lspc.reshape(-1, 1)))\n",
    "                ax.plot([k*3+j*.5]+density*5, lspc, color=gp_colors[j], alpha=.5)\n",
    "                # hide xtick lines\n",
    "                ax.xaxis.set_ticks_position('none')\n",
    "                # ax.plot([k*3+j*.5]+density*3, to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "                # ax.plot([k*3+j*.5]+kde.pdf(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), to_plot_now[to_plot_now['session_id'] == gps[k]][metric], color=gp_colors[j], alpha=.5)\n",
    "                if gp_labels[j] == 'ANT':\n",
    "                    ax.text(k*3+j*.5, 50, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "                elif gp_labels[j] == 'B6':\n",
    "                    median_index = np.argsort(density)[len(density) // 2]\n",
    "                    ax.text(k*3+j*.5, 30, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "                elif gp_labels[j] == 'NON':\n",
    "                    ax.text(k*3+j*.5, 10, 'N='+str(len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])), color=gp_colors[j], fontsize=8, fontweight='bold')\n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 3:\n",
    "                # shapiro-wilk test\n",
    "                stat, p = shapiro(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "                \n",
    "                # KS test\n",
    "                # stat, p = kstest(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], 'norm', args=(np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]), np.std(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])))\n",
    "\n",
    "                # anderson-darling test\n",
    "                # stat, crit, sig = anderson(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "                # print(gp_labels[j], gps[k], stat, crit, sig)\n",
    "                # if result.statistic > result.critical_values[result.significance_level == 5.0]:\n",
    "                #     p = 0\n",
    "                # else:\n",
    "                #     p = 1\n",
    "                    \n",
    "                if gp_labels[j] not in session_comps:\n",
    "                    session_comps[gp_labels[j]] = []\n",
    "                session_comps[gp_labels[j]].append(p)\n",
    "                        \n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT = np.array(session_comps['ANT'])\n",
    "    pvalsB6 = np.array(session_comps['B6'])\n",
    "    pvalsNON = np.array(session_comps['NON'])\n",
    "    pvals = np.concatenate((pvalsANT, pvalsB6, pvalsNON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant = np.ones(len(pvalsANT))\n",
    "    bh_ant[pvals_corrected[:len(pvalsANT)] <= 0.05] = 0\n",
    "    bh_b6 = np.ones(len(pvalsB6))\n",
    "    bh_b6[pvals_corrected[len(pvalsANT):len(pvalsANT)+len(pvalsB6)] <= 0.05] = 0\n",
    "    bh_non = np.ones(len(pvalsNON))\n",
    "    bh_non[pvals_corrected[len(pvalsANT)+len(pvalsB6):] <= 0.05] = 0\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    bh_ant = np.hstack((bh_ant, [.5]))\n",
    "    ax.imshow(np.expand_dims(bh_ant, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('ANT', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    bh_b6 = np.hstack((bh_b6, [.5,.5]))\n",
    "    ax.imshow(np.expand_dims(bh_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    bh_non = np.hstack((bh_non, [.5]))\n",
    "    ax.imshow(np.expand_dims(bh_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    ax.set_yticks([])\n",
    "    ylbl = ax.set_ylabel('NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    " \n",
    "    ax1.set_title(score)\n",
    "    ax1.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('EMD (cm)')\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import anderson\n",
    "\n",
    "# Sample data\n",
    "data = np.array([10, 15, 20, 25, 30])\n",
    "\n",
    "# Perform the Anderson-Darling test against a normal distribution\n",
    "result = anderson(data, dist='norm')\n",
    "\n",
    "# Set significance level (choose based on your desired level)\n",
    "significance_level = 0.05\n",
    "\n",
    "# Determine if the test is significant\n",
    "is_significant = result.statistic > result.critical_values[result.significance_level == significance_level]\n",
    "\n",
    "# Print the result\n",
    "if is_significant:\n",
    "    print(\"Test is Significant (Departure from Normality): 1\")\n",
    "else:\n",
    "    print(\"Test is Not Significant (Consistent with Normality): 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(density, np.arange(len(density)), color=gp_colors[j], alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: p-values and corresponding N values\n",
    "p_values = np.array([0.05, 0.001, 0.2, 0.0001, 0.02, 0.1, 0.03])\n",
    "N_values = np.array([100, 200, 50, 300, 150, 80, 120])\n",
    "\n",
    "# Create a 2D grid for the p-values\n",
    "grid = p_values.reshape(1, -1)  # Reshape to a single row\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the grid of squares with imshow\n",
    "im = ax.imshow(grid, cmap='binary', aspect='auto')\n",
    "\n",
    "# Loop through each cell and add the N value as text label\n",
    "for i in range(len(p_values)):\n",
    "    ax.text(i, 0, f'N={N_values[i]}', color='black', ha='center', va='center')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(np.arange(len(p_values)))\n",
    "ax.set_yticks([])\n",
    "ax.set_xticklabels([])  # Remove x-axis tick labels\n",
    "ax.set_title('P-Values with Corresponding N Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs_main = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(4, 1, subplot_spec=gs_main[row, col], height_ratios=[12,1,1,1])  \n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # # every row for that score\n",
    "    # to_plot = df[df['score'] == score]\n",
    "    # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # scores averaged for each session\n",
    "    to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            to_plot_count_now = to_plot_count[to_plot_count['group'] == gp_labels[j]]\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            for j2 in range(3):\n",
    "                to_plot_now2 = to_plot[to_plot['group'] == gp_labels[j2]]\n",
    "                to_plot_count_now2 = to_plot_count[to_plot_count['group'] == gp_labels[j2]]\n",
    "                if j != j2:\n",
    "                    # welch's t-test\n",
    "                    if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 1 and len(to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric]) > 1:\n",
    "                        w1 = to_plot_count_now[to_plot_count_now['session_id'] == gps[k]][metric]\n",
    "                        w2 = to_plot_count_now2[to_plot_count_now2['session_id'] == gps[k]][metric]\n",
    "                        w1 = w1 / np.mean(w1)\n",
    "                        w2 = w2 / np.mean(w2)\n",
    "                        res = ttest_ind(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                                            to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                                            usevar='unequal', alternative='two-sided', weights=(w1, w2))\n",
    "                        pvalue = res[1]\n",
    "                        # mann-whitney u test\n",
    "                        # g1 = to_plot_now[to_plot_now['session_id'] == gps[k]][metric]\n",
    "                        # g2 = to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric]\n",
    "                        # w1 = to_plot_count_now[to_plot_count_now['session_id'] == gps[k]][metric]\n",
    "                        # w2 = to_plot_count_now2[to_plot_count_now2['session_id'] == gps[k]][metric]\n",
    "                        # w1 = w1 / np.mean(w1)\n",
    "                        # w2 = w2 / np.mean(w2)\n",
    "                        # g1 = np.repeat(g1, w1)\n",
    "                        # g2 = np.repeat(g2, w2)\n",
    "                        # _, pvalue = mannwhitneyu(g1,\n",
    "                        #                         g2,\n",
    "                        #                         alternative='two-sided')\n",
    "                        sorted_labels = np.sort([gp_labels[j], gp_labels[j2]])\n",
    "                        pair_id = sorted_labels[0] + '_' + sorted_labels[1]\n",
    "                        if pair_id not in session_comps:\n",
    "                            session_comps[pair_id] = []\n",
    "                        if pair_id not in ses_visited:\n",
    "                            session_comps[pair_id].append(pvalue)\n",
    "                            ses_visited.append(pair_id)\n",
    "                        \n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT_B6 = np.array(session_comps['ANT_B6'])\n",
    "    pvalsB6_NON = np.array(session_comps['B6_NON'])\n",
    "    pvalsANT_NON = np.array(session_comps['ANT_NON'])\n",
    "    pvals = np.concatenate((pvalsANT_B6, pvalsB6_NON, pvalsANT_NON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant_b6 = np.ones(len(pvalsANT_B6))\n",
    "    bh_ant_b6[pvals_corrected[:len(pvalsANT_B6)] <= 0.05] = 0\n",
    "    bh_b6_non = np.ones(len(pvalsB6_NON))\n",
    "    bh_b6_non[pvals_corrected[len(pvalsANT_B6):len(pvalsANT_B6)+len(pvalsB6_NON)] <= 0.05] = 0\n",
    "    bh_ant_non = np.ones(len(pvalsANT_NON))\n",
    "    bh_ant_non[pvals_corrected[len(pvalsANT_B6)+len(pvalsB6_NON):] <= 0.05] = 0\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    bh_ant_b6 = np.hstack((bh_ant_b6, [.5, .5]))\n",
    "    ax.imshow(np.expand_dims(bh_ant_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('ANT-B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    bh_b6_non = np.hstack((bh_b6_non, [.5, .5]))\n",
    "    ax.imshow(np.expand_dims(bh_b6_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('B6-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    bh_ant_non = np.hstack((bh_ant_non, [.5]))\n",
    "    ax.imshow(np.expand_dims(bh_ant_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ylbl = ax.set_ylabel('ANT-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    lbl_colors = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        # if empirical[j] > np.mean(shuffled):\n",
    "        #     tag = 'greater'\n",
    "        # elif empirical[j] < np.mean(shuffled):\n",
    "        #     tag = 'lower'\n",
    "        # lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        # lbls.append(lbl)\n",
    "        lbl = gp_labels[j] + ' slope: ' + str(np.round(memp, 2)) \n",
    "        lbl_colors.append('k')\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbl_colors[cc] = 'r'\n",
    "            # lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        # else:\n",
    "            # lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax1.legend(bps, lbls, loc='upper right')\n",
    "    # color label text in legend\n",
    "    for text, color in zip(ax1.legend_.get_texts(), lbl_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    ax1.set_title(score)\n",
    "    # ax.set_xlabel('Session')\n",
    "    ax1.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('EMD (cm)')\n",
    "              \n",
    "    # ax1.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    # ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-1.25/2, len(gps) * 3 - 1.25/2])\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-.5/2, len(gps) * 3 - .5/2])\n",
    "\n",
    "\n",
    "# fig.suptitle('All indiv. cell-session appearances')\n",
    "fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs_main = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(4, 1, subplot_spec=gs_main[row, col], height_ratios=[12,1,1,1])  \n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # every row for that score\n",
    "    to_plot = df[df['score'] == score]\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # # scores averaged for each session\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    session_comps = {}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "            for j2 in range(3):\n",
    "                to_plot_now2 = to_plot[to_plot['group'] == gp_labels[j2]]\n",
    "                if j != j2:\n",
    "                    # welch's t-test\n",
    "                    if len(to_plot_now[to_plot_now['session_id'] == gps[k]][metric]) > 1 and len(to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric]) > 1:\n",
    "                        # res = ttest_ind(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                        #                     to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                        #                     usevar='unequal', alternative='two-sided')\n",
    "                        # pvalue = res[1]\n",
    "                        # mann-whitney u test\n",
    "                        _, pvalue = mannwhitneyu(to_plot_now[to_plot_now['session_id'] == gps[k]][metric],\n",
    "                                                to_plot_now2[to_plot_now2['session_id'] == gps[k]][metric],\n",
    "                                                alternative='two-sided')\n",
    "                        sorted_labels = np.sort([gp_labels[j], gp_labels[j2]])\n",
    "                        pair_id = sorted_labels[0] + '_' + sorted_labels[1]\n",
    "                        if pair_id not in session_comps:\n",
    "                            session_comps[pair_id] = []\n",
    "                        if pair_id not in ses_visited:\n",
    "                            session_comps[pair_id].append(pvalue)\n",
    "                            ses_visited.append(pair_id)\n",
    "                        \n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    # benjamini-hochberg correction\n",
    "    pvalsANT_B6 = np.array(session_comps['ANT_B6'])\n",
    "    pvalsB6_NON = np.array(session_comps['B6_NON'])\n",
    "    pvalsANT_NON = np.array(session_comps['ANT_NON'])\n",
    "    pvals = np.concatenate((pvalsANT_B6, pvalsB6_NON, pvalsANT_NON))\n",
    "    reject, pvals_corrected, alphacSidak, alphacBonf = multitest.multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    bh_ant_b6 = np.ones(len(pvalsANT_B6))\n",
    "    bh_ant_b6[pvals_corrected[:len(pvalsANT_B6)] <= 0.05] = 0\n",
    "    bh_b6_non = np.ones(len(pvalsB6_NON))\n",
    "    bh_b6_non[pvals_corrected[len(pvalsANT_B6):len(pvalsANT_B6)+len(pvalsB6_NON)] <= 0.05] = 0\n",
    "    bh_ant_non = np.ones(len(pvalsANT_NON))\n",
    "    bh_ant_non[pvals_corrected[len(pvalsANT_B6)+len(pvalsB6_NON):] <= 0.05] = 0\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    bh_ant_b6 = np.hstack((bh_ant_b6, [.5, .5]))\n",
    "    ax.imshow(np.expand_dims(bh_ant_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('ANT-B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    bh_b6_non = np.hstack((bh_b6_non, [.5,.5]))\n",
    "    ax.imshow(np.expand_dims(bh_b6_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('B6-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    bh_ant_non = np.hstack((bh_ant_non, [.5]))\n",
    "    ax.imshow(np.expand_dims(bh_ant_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ax.set_yticks([])\n",
    "    ylbl = ax.set_ylabel('ANT-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    lbl_colors = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        # if empirical[j] > np.mean(shuffled):\n",
    "        #     tag = 'greater'\n",
    "        # elif empirical[j] < np.mean(shuffled):\n",
    "        #     tag = 'lower'\n",
    "        # lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        # lbls.append(lbl)\n",
    "        lbl = gp_labels[j] + ' slope: ' + str(np.round(memp, 2)) \n",
    "        lbl_colors.append('k')\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbl_colors[cc] = 'r'\n",
    "            # lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        # else:\n",
    "            # lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax1.legend(bps, lbls, loc='upper right')\n",
    "    # color label text in legend\n",
    "    for text, color in zip(ax1.legend_.get_texts(), lbl_colors):\n",
    "        text.set_color(color)\n",
    "\n",
    "    ax1.set_title(score)\n",
    "    # ax.set_xlabel('Session')\n",
    "    ax1.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('Quantile')\n",
    "              \n",
    "    # ax1.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    # ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-1.25/2, len(gps) * 3 - 1.25/2])\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-.5/2, len(gps) * 3 - .5/2])\n",
    "\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances')\n",
    "# fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_b6_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = [1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 6, 6]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Replace boxplots with vertical KDE plots\n",
    "sns.kdeplot(data, ax=ax, vertical=True, color='blue')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_title('Distribution of Data (KDE)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_ant_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_comps['B6_NON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalsANT_B6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_ant_b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Create a figure and a GridSpec to control subplot layout\n",
    "fig = plt.figure(figsize=(10, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a GridSpec for the larger square plot\n",
    "gs_main = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        # Subplot arrangements for each quadrant\n",
    "        gs_sub = gridspec.GridSpecFromSubplotSpec(4, 1, subplot_spec=gs_main[row, col], height_ratios=[1, 0.5, 0.5, 0.5])  # Adjust height_ratios as needed\n",
    "        \n",
    "        # Create the larger subplot\n",
    "        ax1 = plt.subplot(gs_sub[0])\n",
    "        ax1.plot([1, 2, 3], [4, 5, 6])\n",
    "        ax1.set_title('Larger Subplot')\n",
    "\n",
    "        ax2 = plt.subplot(gs_sub[1], sharex=ax1)\n",
    "        ax2.plot([1, 2, 3], [2, 1, 3])\n",
    "\n",
    "        ax3 = plt.subplot(gs_sub[2], sharex=ax1)\n",
    "        ax3.plot([1, 2, 3], [0.5, 1.5, 0.7])\n",
    "\n",
    "        ax4 = plt.subplot(gs_sub[3], sharex=ax1)\n",
    "        ax4.plot([1, 2, 3], [3, 2, 1])\n",
    "\n",
    "        plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "        plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "        plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "np.random.seed(0)\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    # to_plot_shuffle['group'] = np.random.permutation(to_plot_shuffle['group'].loc[:].values)\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    # shuffle the vals\n",
    "    # for i in range(shuffle_count):\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    # print(metric, sesgp, mn, gplbl)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "\n",
    "# metric = 'obj_w'\n",
    "metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "\n",
    "    # # every row for that score\n",
    "    to_plot = df[df['score'] == score]\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # scores averaged for each session\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            # means = to_plot.groupby('session_id')[metric].mean().round(2)\n",
    "            # stds = to_plot.groupby('session_id')[metric].std()\n",
    "            # n = to_plot.groupby('session_id')[metric].count()\n",
    "            # sems = stds / np.sqrt(n)\n",
    "            # sems = sems.round(2)\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "                        \n",
    "            # lbls.append(str(means[j]) + ' ± ' + str(sems[j]) + ' cm, N = ' + str(n[j]))\n",
    "\n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "            \n",
    "    ax.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    ax.set_xticklabels(gps)\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    empirical = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        memp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        empirical.append(memp)\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            shuffled.append(mshuffled)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        # print('Group: ' + gp_labels[j])\n",
    "        # print('Empirical: ' + str(empirical[j]))\n",
    "        # print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        if empirical[j] > np.mean(shuffled):\n",
    "            tag = 'greater'\n",
    "        elif empirical[j] < np.mean(shuffled):\n",
    "            tag = 'lower'\n",
    "        lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        else:\n",
    "            lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Session')\n",
    "    ax.set_title(titles_to_use[i])\n",
    "    ax.set_ylabel('Wasserstein distnaces (cm)')\n",
    "\n",
    "fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group per angle \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "# scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "scores_to_use = ['spike_density' for i in range(4)]\n",
    "titles_to_use = ['Spike density' for i in range(4)]\n",
    "obj_angles = [0, 90, 180, 270]\n",
    "# titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['ANT', 'B6', 'NON']\n",
    "gp_colors = ['r', 'b', 'g']\n",
    "\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "\n",
    "metric = 'obj_w'\n",
    "# metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    df_angle = df[df['obj_a'] == obj_angles[i]]\n",
    "    # # every row for that score\n",
    "    to_plot = df_angle[df_angle['score'] == score]\n",
    "    # # scores averaged for each animal\n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name']).count().reset_index()\n",
    "    # # scores averaged for each session\n",
    "    # to_plot = df_angle[df_angle['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    # to_plot_count = df_angle[df_angle['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "    # scores averaged for each neuron \n",
    "    # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).mean().reset_index()\n",
    "    # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','tetrode', 'unit_id']).count().reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # plot boxplot for each group\n",
    "    # bp = sns.boxplot(x='group', y='obj_w', data=to_plot, ax=ax)\n",
    "    # sns.swarmplot(x='group', y='obj_w', data=to_plot, ax=ax, color='black', alpha=0.5)\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    mns = [[] for j in range(3)]\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            # means = to_plot.groupby('session_id')[metric].mean().round(2)\n",
    "            # stds = to_plot.groupby('session_id')[metric].std()\n",
    "            # n = to_plot.groupby('session_id')[metric].count()\n",
    "            # sems = stds / np.sqrt(n)\n",
    "            # sems = sems.round(2)\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "                        \n",
    "            # lbls.append(str(means[j]) + ' ± ' + str(sems[j]) + ' cm, N = ' + str(n[j]))\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    ax.set_xticklabels(gps)\n",
    "    \n",
    "    lbls = []\n",
    "    for j in range(len(mns)):\n",
    "        # polyfit \n",
    "\n",
    "        m, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        lbl = 'Mean slope: ' + str(np.round(m, 2)) + ' cm/session'\n",
    "        lbls.append(lbl)\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_title(score)\n",
    "    ax.set_xlabel('Session')\n",
    "    ax.set_title(titles_to_use[i] + ' ' + str(obj_angles[i]) + '°')\n",
    "    ax.set_ylabel('Wasserstein distnaces (cm)')\n",
    "\n",
    "fig.suptitle('All individual cell-session appearances')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
