{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "remap_path = os.getcwd()\n",
    "prototype_path = os.path.abspath(os.path.join(remap_path, os.pardir))\n",
    "project_path = os.path.abspath(os.path.join(prototype_path, os.pardir))\n",
    "lab_path = os.path.abspath(os.path.join(project_path, os.pardir))\n",
    "sys.path.append(lab_path)\n",
    "os.chdir(lab_path)\n",
    "sys.path.append(lab_path)\n",
    "print(lab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANT_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\ANT\"\n",
    "# NON_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\NON\"\n",
    "# B6_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\B6\"\n",
    "# dfs = []\n",
    "# for path in [ANT_path, NON_path, B6_path]:\n",
    "#     for animal_dir in os.listdir(path):\n",
    "#         for csv_file in os.listdir(path + r'\\\\' + animal_dir):\n",
    "#             if csv_file.endswith('.xlsx'):\n",
    "#                 file_df = pd.read_excel(path + r'\\\\' + animal_dir + r'\\\\' + csv_file)\n",
    "#                 # add column for genotype\n",
    "#                 if path == ANT_path:\n",
    "#                     file_df['group'] = 'ANT'\n",
    "#                 elif path == NON_path:\n",
    "#                     file_df['group'] = 'NON'\n",
    "#                 elif path == B6_path:\n",
    "#                     file_df['group'] = 'B6'\n",
    "#                 dfs.append(file_df)\n",
    "\n",
    "# df_remapping = pd.concat(dfs, ignore_index=True)\n",
    "# df_remapping.columns\n",
    "\n",
    "df_remapping = pd.read_excel(r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc_remapping_joined\\full_remapping_swap_modified_combined.xlsx\")\n",
    "df_remapping.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be empty\n",
    "df_remapping[df_remapping['cylinder'] == False]['signature'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Match remapping csvs to neurofunc csvss\n",
    "\n",
    "# ANT_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\ANT\"\n",
    "# NON_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\NON\"\n",
    "# B6_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\B6\"\n",
    "# dfs = []\n",
    "# for path in [ANT_path, NON_path, B6_path]:\n",
    "#     for csv_file in os.listdir(path):\n",
    "#         if csv_file.endswith('.xlsx') and '~$' not in csv_file:\n",
    "#             print(csv_file)\n",
    "#             file_df = pd.read_excel(path + r'\\\\' + csv_file)\n",
    "#             name = csv_file.split('_')[0]\n",
    "#             for i, row in file_df.iterrows():\n",
    "#                 signature = row['Session']\n",
    "#                 # print(name)\n",
    "#                 if name not in signature:\n",
    "#                     signature = name + '_' + signature\n",
    "#                     file_df.at[i, 'Session'] = signature\n",
    "\n",
    "#             # add column for genotype\n",
    "#             if path == ANT_path:\n",
    "#                 file_df['group'] = 'ANT'\n",
    "#             elif path == NON_path:\n",
    "#                 file_df['group'] = 'NON'\n",
    "#             elif path == B6_path:\n",
    "#                 file_df['group'] = 'B6'\n",
    "#             dfs.append(file_df)\n",
    "\n",
    "# df_neurofunc = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_neurofunc = pd.read_excel(r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc_remapping_joined\\full_neurofunc_swap_modified_combined.xlsx\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "NON_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\NON\"\n",
    "ANT_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\ANT\"\n",
    "B6_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\B6\"\n",
    "# insert column for date_time into df_neurofunc\n",
    "df_neurofunc['date_time'] = pd.NaT\n",
    "df_neurofunc['ses_ct'] = pd.NaT\n",
    "\n",
    "for path in [ANT_set_path, NON_set_path, B6_set_path]:\n",
    "    for animal_dir in os.listdir(path):\n",
    "        for set_dir in os.listdir(path + r'\\\\' + animal_dir):\n",
    "            date_times = {}\n",
    "            # sigs = []\n",
    "            for set_file in os.listdir(path + r'\\\\' + animal_dir + r'\\\\' + set_dir):\n",
    "                # print(set_file)\n",
    "                assert set_file.endswith('.set')\n",
    "                ses_signature = set_file.split('.')[0]\n",
    "\n",
    "                # sigs.append(ses_signature)\n",
    "                # print(set_file)\n",
    "                with open(path + r'\\\\' + animal_dir + r'\\\\' + set_dir + r'\\\\' + set_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if 'trial_time' in str(line):\n",
    "                            # trial_time = line.decode(encoding='UTF-8').split(\" \")[1]\n",
    "                            trial_time = line.split(\" \")[1]\n",
    "                        if 'trial_date' in str(line):\n",
    "                            trial_date = line.split(\" \")[2:]\n",
    "\n",
    "                    day, month, year = trial_date\n",
    "                    month = datetime.strptime(str(month), '%b').month\n",
    "                    hour, minute, second = trial_time.split(':')\n",
    "                    date_time = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))\n",
    "\n",
    "                # assert ses_signature not in date_times.keys(), 'Session signature {} already in dictionary'.format(ses_signature)\n",
    "                # assert date_time not in date_times.values(), 'Date time {} already in dictionary'.format(date_time) \n",
    "                date_times[ses_signature] = date_time\n",
    "\n",
    "                print(date_time, ses_signature)\n",
    "\n",
    "            # sort dictionary by date_time\n",
    "            sigs = list(date_times.keys())\n",
    "            date_times = list(date_times.values())\n",
    "            sorted_sigs = [x for _, x in sorted(zip(date_times, sigs))]\n",
    "            date_times = sorted(date_times)\n",
    "            print(date_times, sorted_sigs)\n",
    "            ses_ct = 1\n",
    "            for ses in sorted_sigs:\n",
    "\n",
    "\n",
    "                if len(df_neurofunc[df_neurofunc['Session'] == ses]) == 0 and len(sorted_sigs) > 1:\n",
    "                    print('Session {} not found in df_neurofunc'.format(ses))\n",
    "                # else:\n",
    "                    # add date time column\n",
    "                else:\n",
    "                    if 'ANT-135a-7_20180604' in ses:\n",
    "                        print('gotohere')\n",
    "                        print(ses_ct - 1)\n",
    "                        df_neurofunc.loc[(df_neurofunc['Session'] == ses) & (df_neurofunc['Tetrode'] != 7), 'date_time'] = date_times[ses_ct - 1]\n",
    "                        df_neurofunc.loc[(df_neurofunc['Session'] == ses) & (df_neurofunc['Tetrode'] != 7), 'ses_ct'] = ses_ct \n",
    "                        df_neurofunc.loc[(df_neurofunc['Session'] == ses) & (df_neurofunc['Tetrode'] == 7), 'date_time'] = date_times[ses_ct - 1]\n",
    "                        df_neurofunc.loc[(df_neurofunc['Session'] == ses) & (df_neurofunc['Tetrode'] == 7), 'ses_ct'] = ses_ct - 1\n",
    "                        ses_ct += 1\n",
    "        \n",
    "                    else:\n",
    "                        df_neurofunc.loc[df_neurofunc['Session'] == ses, 'date_time'] = date_times[ses_ct - 1]\n",
    "                        df_neurofunc.loc[df_neurofunc['Session'] == ses, 'ses_ct'] = ses_ct\n",
    "                        ses_ct += 1\n",
    "\n",
    "\n",
    "                    \n",
    "# cast ses_ct as int\n",
    "df_neurofunc['ses_ct'] = df_neurofunc['ses_ct'].astype(int)\n",
    "print(df_neurofunc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be 1 to 7\n",
    "assert len(df_neurofunc['ses_ct'].unique()) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be empty\n",
    "assert len(df_neurofunc[df_neurofunc['date_time'].isna()]['Session'].unique()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Session where date_time is NaT\n",
    "print(df_neurofunc[df_neurofunc['date_time'].isna()]['Session'].unique())\n",
    "print(len(df_neurofunc[df_neurofunc['date_time'].isna()]['Session'].unique()), len(df_neurofunc['Session'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to match neurofunc and remapping\n",
    "import re\n",
    "from scripts.batch_map.LEC_naming import LEC_naming_format, extract_name_lec\n",
    "\n",
    "def _check_single_format(filename, fformat, fxn):\n",
    "    if re.match(str(fformat), str(filename)) is not None:\n",
    "        return list(fxn(filename))\n",
    "\n",
    "# For neurofunc need to add extract, date, depth, name, stim \n",
    "# iterate thru rows of df_neurofunc and extract from signature\n",
    "for i, row in df_neurofunc.iterrows():\n",
    "    # extract\n",
    "    fname = row['Session']\n",
    "    \n",
    "    if 'CAGE' in fname:\n",
    "        # drop row\n",
    "        df_neurofunc.drop(i, inplace=True)\n",
    "    else:\n",
    "        print(fname)\n",
    "        group, name = extract_name_lec(fname)\n",
    "        formats = LEC_naming_format[group][name]['object']\n",
    "\n",
    "        for fformat in list(formats.keys()):\n",
    "            checked = _check_single_format(fname, fformat, formats[fformat])\n",
    "            if checked is not None:\n",
    "                break\n",
    "\n",
    "        stim, depth, name, date = checked\n",
    "        \n",
    "        df_neurofunc.at[i, 'date'] = date\n",
    "        df_neurofunc.at[i, 'depth'] = str(int(depth))\n",
    "        df_neurofunc.at[i, 'name'] = name\n",
    "        df_neurofunc.at[i, 'stim'] = stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurofunc_row_identifiers = ['name','Tetrode', 'Cell ID', 'date', 'depth', 'stim']\n",
    "# remapping_row_identifiers = ['name','tetrode', 'unit_id', 'date', 'depth','object_location']\n",
    "\n",
    "# df_remapping_to_merge = df_remapping[df_remapping['score'] == 'whole']\n",
    "\n",
    "# for row in neurofunc_row_identifiers:\n",
    "#     df_neurofunc[row] = df_neurofunc[row].astype(str)\n",
    "# for row in remapping_row_identifiers:\n",
    "#     df_remapping_to_merge[row] = df_remapping_to_merge[row].astype(str)\n",
    "\n",
    "# matched_df = None\n",
    "# for i, row in df_remapping_to_merge.iterrows():\n",
    "#     # find row in df_neurofunc that mathces using identifiers\n",
    "#     for j, row2 in df_neurofunc.iterrows():\n",
    "#         matched = True\n",
    "#         for k in range(len(neurofunc_row_identifiers)):\n",
    "#             print('pair ' + str(neurofunc_row_identifiers[k]))\n",
    "#             print('new2')\n",
    "#             print(row2[neurofunc_row_identifiers[k]])\n",
    "#             print('new')\n",
    "#             print(row[remapping_row_identifiers[k]])\n",
    "            \n",
    "#             if row2[neurofunc_row_identifiers[k]] != row[remapping_row_identifiers[k]]:\n",
    "#                 matched = False\n",
    "\n",
    "#         # if row2[neurofunc_row_identifiers] == row[remapping_row_identifiers]:\n",
    "#         if matched == True:\n",
    "#             if matched_df is None:\n",
    "#                 row = pd.concat([row, row2], axis=0)\n",
    "#                 matched_df = row\n",
    "#             else:\n",
    "#                 row = pd.concat([row, row2], axis=0)\n",
    "#                 matched_df = pd.concat([matched_df, row], axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the common columns\n",
    "neurofunc_row_identifiers = ['name', 'Tetrode', 'Cell ID', 'date', 'depth', 'stim']\n",
    "remapping_row_identifiers = ['name', 'tetrode', 'unit_id', 'date', 'depth', 'object_location']\n",
    "\n",
    "for row in neurofunc_row_identifiers:\n",
    "    df_neurofunc[row] = df_neurofunc[row].astype(str)\n",
    "for row in remapping_row_identifiers:\n",
    "    df_remapping[row] = df_remapping[row].astype(str)\n",
    "df_neurofunc['Session'] = df_neurofunc['Session'].astype(str)\n",
    "df_remapping['signature'] = df_remapping['signature'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_df_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\Sifting_Through_Cells_Modified.xlsx\"\n",
    "swap_df_ANT = pd.read_excel(swap_df_path, sheet_name='ANT')\n",
    "swap_df_B6 = pd.read_excel(swap_df_path, sheet_name='B6')\n",
    "swap_df_NON = pd.read_excel(swap_df_path, sheet_name='NON')\n",
    "swap_df = pd.concat([swap_df_ANT, swap_df_B6, swap_df_NON])\n",
    "swap_df['CATEGORY'] = swap_df['CATEGORY'].astype(str)\n",
    "swap_df['CATEGORY'] = swap_df['CATEGORY'].str.replace('.png', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store the merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate through each row in df_remapping\n",
    "counter = 0\n",
    "remapping_rows = []\n",
    "for remapping_row in df_remapping.itertuples(index=False):\n",
    "    # Extract the identifiers from the remapping row\n",
    "    remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "\n",
    "    # Find the matching row in df_neurofunc based on the identifiers\n",
    "    matching_row = df_neurofunc.loc[\n",
    "        (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "    ]\n",
    "\n",
    "    if len(matching_row) > 1:\n",
    "\n",
    "\n",
    "        \"\"\" If unnamed row in csv then change to [7] instead of [6]. Or double check what row has seession_1, session_2, etc...\"\"\"\n",
    "        rw = remapping_row[6]\n",
    "        assert 'session' in rw.lower(), 'Remapping row is {}'.format(rw)\n",
    "        remapping_row_ses_ct = int(rw.split('_')[-1])\n",
    "\n",
    "        pre_matched = matching_row.copy()\n",
    "        matching_row = matching_row[matching_row['ses_ct'] == remapping_row_ses_ct]\n",
    "        if len(matching_row) == 0:\n",
    "            print('no match found')\n",
    "            print(remapping_row)\n",
    "            print(pre_matched)\n",
    "            print(matching_row)\n",
    "\n",
    "    # Check if a matching row was found\n",
    "    if len(matching_row) == 0:\n",
    "        print(\"Found {} matching rows for {}\".format(len(matching_row), remapping_identifiers))\n",
    "        print(matching_row['Session'], getattr(remapping_row, 'signature'))\n",
    "    elif len(matching_row) > 1:\n",
    "        # check all rows are matching on spatial info score \n",
    "        prev = None\n",
    "        for i, row in matching_row.iterrows():\n",
    "            if prev is None:\n",
    "                prev = row['spike_count']\n",
    "            else:\n",
    "                if prev != row['spike_count']:\n",
    "                    print('multiple matches')\n",
    "                    print(matching_row)\n",
    "        \n",
    "\n",
    "    aid = remapping_row.name\n",
    "    date = remapping_row.date\n",
    "    tet = remapping_row.tetrode\n",
    "    cell_label = remapping_row.unit_id\n",
    "    if '-' in str(date):\n",
    "        date = date.split('-')[0]\n",
    "\n",
    "    swap_df_row_id = aid + '_' + date + '_' + tet + '_' + str(cell_label)\n",
    "    swap_df_row_ids = np.where(swap_df['CATEGORY'].to_numpy() == swap_df_row_id)[0]\n",
    "\n",
    "    assert len(swap_df_row_ids) <= 1, 'Multiple rows found for {}'.format(swap_df_row_id)\n",
    "\n",
    "    if len(swap_df_row_ids) != 0:\n",
    "        df_sample = swap_df.iloc[swap_df_row_ids[0]:swap_df_row_ids[0]+1]\n",
    "        row = df_sample['SWAP/DROP/KEEP']\n",
    "        row = row.values[0]\n",
    "\n",
    "        if row != row:\n",
    "            print('row is nan for {}'.format(swap_df_row_id))\n",
    "\n",
    "        # if row != row or 'keep' not in row.lower():\n",
    "        if row != row or 'drop' in row.lower():\n",
    "            pass\n",
    "        else:\n",
    "            assert len(matching_row) == 1, 'Multiple rows found for {}'.format(matching_row)\n",
    "            merged_rows.append(matching_row)\n",
    "            remapping_rows.append(remapping_row)\n",
    "            counter += 1\n",
    "\n",
    "            if len(merged_rows) != counter:\n",
    "                print('error {} != {}'.format(len(merged_rows), counter))\n",
    "                print(matching_row)\n",
    "\n",
    "\n",
    "merged_df = pd.concat(merged_rows, ignore_index=True)\n",
    "remapping_merged_df = pd.DataFrame(remapping_rows, columns=df_remapping.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = []\n",
    "firsts = []\n",
    "seses = []\n",
    "for row in merged_rows:\n",
    "    ln = len(row)\n",
    "    first = row.iloc[0]\n",
    "    lns.append(ln)\n",
    "    if ln == 2:\n",
    "        sese = row['Session'].unique()\n",
    "        seses.append(sese)\n",
    "print(np.unique(seses))\n",
    "# should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([remapping_merged_df, merged_df], axis=1)\n",
    "print(df_full.shape)\n",
    "df_full.to_excel('df_full_LEC_keep_swapped2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_neurofunc.shape, df_remapping.shape, merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "\n",
    "#     # Find the matching row in df_neurofunc based on the identifiers\n",
    "# matching_row = df_neurofunc.loc[\n",
    "#         (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_neurofunc.groupby(['Session']).groups.keys()))\n",
    "print(len(df_remapping.groupby(['signature'])))\n",
    "for ses in df_neurofunc.groupby(['Session']).groups.keys():\n",
    "    if ses not in df_remapping.groupby(['signature']).groups.keys():\n",
    "        print(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = df_remapping[(df_remapping['signature'] == 'ANT-135a-7_20180604-ROUND-3225-180-2') & (df_remapping['tetrode'] == '7')]\n",
    "for remapping_row in spl.itertuples(index=False):\n",
    "    # Extract the identifiers from the remapping row\n",
    "    remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "    print(remapping_row)\n",
    "    # Find the matching row in df_neurofunc based on the identifiers\n",
    "    matching_row = df_neurofunc.loc[\n",
    "        (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "    ]\n",
    "    print(matching_row['ses_ct'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df remapping and df merged \n",
    "# df_full = pd.concat([df_remapping, merged_df], axis=1)\n",
    "# df_full.to_excel('df_full_LEC_only_keep.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_remapping, merged_df], axis=1)\n",
    "# drop duplicate columns\n",
    "df_full = df_full.loc[:,~df_full.columns.duplicated()]\n",
    "\n",
    "dtemp = df_full[df_full['score'] == 'whole']\n",
    "lst = dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location'])\n",
    "def _single(dtemp, grp):\n",
    "    if len(dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location']).groups[grp]) != 1:\n",
    "        print(grp)\n",
    "        print(dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location']).groups[grp])\n",
    "        # stop()\n",
    "\n",
    "list(map(lambda x: _single(dtemp, x), lst.groups))\n",
    "\n",
    "# fld_counts['field_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in merged_rows:\n",
    "    ln = len(row)\n",
    "    if ln != 1:\n",
    "        print(row['Session'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
