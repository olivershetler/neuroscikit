{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "\n",
    "PROJECT_PATH = os.getcwd()\n",
    "sys.path.append(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\LEC_full_merged_scores.xlsx')\n",
    "df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC.xlsx')\n",
    "# df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_trace_cells.xlsx')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" QUALITY CHECK DATA \"\"\" \n",
    "nan_idx = np.where(df['obj_q_0'].isna())[0]\n",
    "not_nan_idx = np.where(~df['obj_q_0'].isna())[0]\n",
    "nan_dates = (df['date'][nan_idx].unique())\n",
    "nan_names = (df['name'][nan_idx].unique())\n",
    "\n",
    "print('Number of NaN rows: ' + str(len(nan_idx)))\n",
    "print('Animals with NaN rows: ' + str(nan_names))\n",
    "print('Dates with NaN rows: ' + str(nan_dates))\n",
    "\n",
    "# remove rows with NaN values\n",
    "print('Removing nan rows')\n",
    "df = df.iloc[not_nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FILTERING \"\"\"\n",
    "\n",
    "\"\"\" REMOVE FIELD WITH LOW COVERAGE % \"\"\"\n",
    "# remove rows with field_coverage < 0.1\n",
    "# df = df[df['field_coverage'] >= 0.1]\n",
    "\n",
    "\"\"\" ONLY KEEPING MAIN FIELD \"\"\"\n",
    "# remove rows where field_id is not 1 and score is not 'whole' or 'spike_density'\n",
    "# df = df[(df['field_id'] == 1) | (df['score'] == 'whole') | (df['score'] == 'spike_density')]\n",
    "\n",
    "\"\"\" CHOOSING ANGLE FOR EACH ROW \"\"\"\n",
    "# for each row, choose lowest quantile from ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "# df['obj_q'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].min(axis=1)\n",
    "df['obj_q'] = df['obj_q_NO']\n",
    "df['obj_a'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].idxmin(axis=1)\n",
    "# convert obj_a to degrees\n",
    "df['obj_a'] = df['obj_a'].apply(lambda x: int(x.split('_')[2]))\n",
    "# use obj_wass with angle of min quantile\n",
    "df['obj_w'] = df.apply(lambda x: x['obj_wass_' + str(x['obj_a'])], axis=1)\n",
    "\n",
    "\"\"\" ASSESSING SIG FOR EACH ROW AT EACH ANGLE \"\"\"\n",
    "# obj_s_rows = ['obj_s_0', 'obj_s_90', 'obj_s_180', 'obj_s_270']\n",
    "# obj_q_rows = ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "# for i in range(len(obj_s_rows)):\n",
    "#     obj_q_x = obj_q_rows[i]\n",
    "#     df[obj_s_rows[i]] = df[obj_q_x].apply(lambda x: 1 if x < quantile_threshold else 0)\n",
    "\n",
    "\n",
    "# df2 = df[df['score'] == 'whole'].copy()\n",
    "df2 = df.copy()\n",
    "# group_by_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "# df2 = df2.groupby(group_by_cell).mean().reset_index()\n",
    "cts = df2[df2['spike_count'] > 30000]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count upper of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(30000 , str(len(df2[df2['spike_count'] > 30000])), \n",
    "                       cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_count'] < 100]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count lower of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(100 , str(len(df2[df2['spike_count'] < 100])),\n",
    "                          cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['information'] < 0.25]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spatial info of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.25 , str(len(df2[df2['information'] < 0.25])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['selectivity'] < 5]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Selectivity of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['selectivity'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['iso_dist'] < 5]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Isolation distance of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['iso_dist'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['firing_rate'] > 20]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Firing rate of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(20 , str(len(df2[df2['firing_rate'] > 20])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_width'] < 0.00005]['group'].value_counts()\n",
    "for nm in ['B6', 'NON', 'ANT']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike width of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.00005 , str(len(df2[df2['spike_width'] < 0.00005])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "\n",
    "# drop spike count column \n",
    "df2 = df2.drop(columns=['spike_count'])\n",
    "# rename spike_count.1 to spike_count\n",
    "df2 = df2.rename(columns={'spike_count.1': 'spike_count'})\n",
    "\n",
    "# df_unfiltered = df2.copy()\n",
    "# df_unfiltered.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_unfiltered.xlsx')\n",
    "\n",
    "# df2 = df2[df2['spike_count'] < 30000]\n",
    "# df2 = df2[df2['spike_count'] > 100]\n",
    "# df2 = df2[df2['information'] > 0.25]\n",
    "# df2 = df2[df2['selectivity'] > 5]\n",
    "df2 = df2[df2['iso_dist'] > 5]\n",
    "# df2 = df2[df2['firing_rate'] < 80]\n",
    "# df2 = df2[df2['spike_width'] > 0.00005]\n",
    "\n",
    "print('Remaining cells: ' + str(len(df2)) + ' of which ' + str(len(df2[df2['group'] == 'ANT'])) + ' ANT, ' + str(len(df2[df2['group'] == 'B6'])) + ' B6 and ' + str(len(df2[df2['group'] == 'NON'])) + ' NON')\n",
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df2.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_filtered.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "j = 1\n",
    "for scr in ['whole','spike_density','field','binary']:\n",
    "    mxs = []\n",
    "    axs = []\n",
    "    for i in ['B6','NON','ANT']:\n",
    "        ax = fig.add_subplot(4, 3, j)\n",
    "\n",
    "        df_to_use = df2[df2['group'] == i]\n",
    "        # plt.hist(df_to_use[df_to_use['score'] == 'whole']['obj_w'], bins=100)\n",
    "\n",
    "        # if i == 'B6':\n",
    "        #     desired_order = ['B6-LEC1', 'B6-1M', 'B6-2M', 'B6-LEC2']\n",
    "        #     df_to_use.loc[:, 'name'] = pd.Categorical(df_to_use['name'], categories=desired_order, ordered=True)\n",
    "        # elif i == 'NON':\n",
    "        #     desired_order = ['NON-INT-01', 'NON-88-1', 'NON-73-6', 'NON-INT-02', 'NON-INT-03']\n",
    "        #     df_to_use.loc[:, 'name'] = pd.Categorical(df_to_use['name'], categories=desired_order, ordered=True)\n",
    "\n",
    "            \n",
    "        sns.histplot(data=df_to_use[df_to_use['score'] == scr], x='obj_q', bins=50, hue='name', kde=False, ax=ax, stat='density', common_norm=False)\n",
    "        ax.set_title(i)\n",
    "        ax.set_xlabel(str(scr) + ' obj_q')\n",
    "        mxs.append(df_to_use[df_to_use['score'] == scr]['obj_q'].max())\n",
    "        axs.append(ax)\n",
    "\n",
    "        j += 1\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.set_xlim(0, max(mxs))\n",
    "\n",
    "fig.suptitle('Animal quantile distributions for each score', x=0.5, fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "dfuse = df2[df2['score'] == 'whole']\n",
    "obj_order = ['0', '90', '180', '270', 'NO']\n",
    "grp_order = ['B6', 'NON', 'ANT']\n",
    "# reorder dfuse based on object_location\n",
    "dfuse.loc[:, 'object_location'] = pd.Categorical(dfuse['object_location'], categories=obj_order, ordered=True)\n",
    "dfuse.loc[:, 'group'] = pd.Categorical(dfuse['group'], categories=grp_order, ordered=True)\n",
    "sns.histplot(data=dfuse, x='group', multiple='dodge', shrink=0.8, stat='density', common_norm=True, hue='object_location')\n",
    "ttle = '% of sessions for each object location across all groups'\n",
    "fig.suptitle(ttle, x=0.5, fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "for grp in ['B6', 'NON', 'ANT']:\n",
    "    dfuse = df2[df2['score'] == 'whole']\n",
    "    dfuse = dfuse[dfuse['group'] == grp]\n",
    "    obj_order = ['0', '90', '180', '270', 'NO']\n",
    "    # reorder dfuse based on object_location\n",
    "    dfuse.loc[:, 'object_location'] = pd.Categorical(dfuse['object_location'], categories=obj_order, ordered=True)\n",
    "    sns.histplot(data=dfuse, x='group', multiple='dodge', shrink=0.8, stat='density', common_norm=True, hue='object_location')\n",
    "    ttle = '% of sessions for each object location within a group'\n",
    "clr_palette_store = sns.color_palette()\n",
    "clr_palette_settings = sns.color_palette('Set2')\n",
    "fig.suptitle(ttle, x=0.5, fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for grp in ['B6', 'NON', 'ANT']:\n",
    "    fig = plt.figure(figsize=(14, 2))\n",
    "    for ses in ['session_1', 'session_2', 'session_3', 'session_4', 'session_5', 'session_6', 'session_7']:\n",
    "        ax = fig.add_subplot(1, 7, int(ses.split('_')[1]))\n",
    "        if grp == 'B6' and ses == 'session_7':\n",
    "            pass\n",
    "        else:\n",
    "            dfuse = df2[df2['score'] == 'whole']\n",
    "            dfuse = dfuse[dfuse['group'] == grp]\n",
    "            dfuse = dfuse[dfuse['session_id'] == ses]\n",
    "            obj_order = ['0', '90', '180', '270', 'NO']\n",
    "            # reorder dfuse based on object_location\n",
    "            dfuse.loc[:, 'object_location'] = pd.Categorical(dfuse['object_location'], categories=obj_order, ordered=True)\n",
    "            sns.histplot(data=dfuse, x='group', multiple='dodge', shrink=0.8, stat='density', common_norm=True, hue='object_location')\n",
    "            ax.set_title(ses)\n",
    "            # turn off legend\n",
    "            ax.get_legend().remove()\n",
    "            # turn off x label\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "    ttle = '% of object location within that session for {}'.format(grp)\n",
    "    fig.suptitle(ttle, x=0.5, fontweight='bold')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of object locations\n",
    "obj_order = ['0', '90', '180', '270', 'NO']\n",
    "\n",
    "# Create a figure for the groups\n",
    "for grp in ['B6', 'NON', 'ANT']:\n",
    "    # Filter the DataFrame df2 for the current group\n",
    "    dfuse = df2[df2['score'] == 'whole']\n",
    "    dfuse = dfuse[dfuse['group'] == grp]\n",
    "    total_count = len(dfuse)\n",
    "\n",
    "    # Iterate through sessions and create subplots\n",
    "    fig, axes = plt.subplots(1, len(['session_1', 'session_2', 'session_3', 'session_4', 'session_5', 'session_6']), \n",
    "                             figsize=(14, 2))\n",
    "    \n",
    "    for i, ses in enumerate(['session_1', 'session_2', 'session_3', 'session_4', 'session_5', 'session_6']):\n",
    "        if grp == 'B6' and ses == 'session_7':\n",
    "            continue\n",
    "\n",
    "        # Filter the DataFrame for the current session\n",
    "        df_session = dfuse[dfuse['session_id'] == ses]\n",
    "\n",
    "        # Calculate the proportions of each object location for the current session\n",
    "        obj_loc_counts = [len(df_session[df_session['object_location'] == obj_loc]) for obj_loc in obj_order]\n",
    "        obj_loc_proportions = [count / total_count for count in obj_loc_counts]\n",
    "\n",
    "        # Create a bar plot for the current session\n",
    "        bcolors = clr_palette_store    \n",
    "        axes[i].bar(obj_order, obj_loc_proportions, color=bcolors, edgecolor='black', linewidth=1.2, alpha=0.8, width=1)\n",
    "        axes[i].set_title(ses)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylim(0, 0.25)\n",
    "    # Set a common ylabel for the group\n",
    "    axes[0].set_ylabel('Density')\n",
    "    \n",
    "    # Set the overall title for the group\n",
    "    plt.suptitle(f'% of object location for {grp} across all sessions', x=0.5, fontweight='bold')\n",
    "    \n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11517986/indicating-the-statistically-significant-difference-in-bar-graph\n",
    "\n",
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05\n",
    "        # ** is p < 0.005\n",
    "        # *** is p < 0.0005\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        if data <= 0.05:\n",
    "            text = '*'\n",
    "        if data <= 0.01:\n",
    "            text = '**'\n",
    "        if data <= 0.001:\n",
    "            text = '***'\n",
    "        if data <= 0.0001:\n",
    "            text = '****'\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black')\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "    if text != 'n. s.':\n",
    "        kwargs['weight'] = 'bold'\n",
    "        kwargs['fontsize'] = 25\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsNON_quantile_indiv_beta_binary.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsNON_quantile_indiv_beta_field.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsNON_quantile_indiv_beta_spike_density.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsNON_quantile_indiv_beta_whole.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsANT_quantile_indiv_beta_whole.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsANT_quantile_indiv_beta_binary.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsANT_quantile_indiv_beta_field.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\B6vsANT_quantile_indiv_beta_spike_density.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\NONvsANT_quantile_indiv_beta_whole.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\NONvsANT_quantile_indiv_beta_binary.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\NONvsANT_quantile_indiv_beta_field.csv\",\n",
    "r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\NONvsANT_quantile_indiv_beta_spike_density.csv\"]\n",
    "\n",
    "score_beta_ps = {'whole': {}, 'spike_density': {}, 'field': {}, 'binary': {}}\n",
    "\n",
    "for pth in paths:\n",
    "    fname = pth.split('\\\\')[-1]\n",
    "    score = fname.split('_')[-1].split('.')[0]\n",
    "    if 'density' in score:\n",
    "        score = 'spike_density'\n",
    "\n",
    "    comp_group = fname.split('_')[0]\n",
    "    a1 = comp_group.split('vs')[0]\n",
    "    a2 = comp_group.split('vs')[1]\n",
    "\n",
    "    a12 = np.sort([a1, a2])\n",
    "\n",
    "    cgroup = a12[0] + '_' + a12[1]\n",
    "    \n",
    "\n",
    "    data = pd.read_csv(pth)\n",
    "    score_beta_ps[score][cgroup] = data['Pvalue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_beta_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Amount of remapping per group \"\"\"\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "scores_to_use = ['whole', 'spike_density', 'field', 'binary']\n",
    "# 'field', 'binary', 'centroid', 'firing_rate']\n",
    "quad_arrange = [[0,0],[0,1],[1,0],[1,1], [2,0], [2,1]]\n",
    "titles_to_use = ['Whole-map', 'Spike Density', 'Field', 'Binary', 'Centroid', 'Firing Rate']\n",
    "gps = np.unique(df['session_id'])\n",
    "gp_labels = ['B6', 'NON', 'ANT']\n",
    "gp_colors = ['b', 'g', 'r']\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def _single_shuffle(to_plot_shuffle, sesgp, metric, gplbl):\n",
    "    vals = to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'].to_numpy()\n",
    "    np.random.shuffle(vals)\n",
    "    to_plot_shuffle.loc[to_plot_shuffle['session_id'] == sesgp, 'group'] = vals\n",
    "\n",
    "    use = to_plot_shuffle[to_plot_shuffle['session_id'] == sesgp]\n",
    "    use = use[use['group'] == gplbl][metric]\n",
    "    mn = np.mean(use)\n",
    "    return mn\n",
    "\n",
    "fig = plt.figure(figsize=(23, 46))\n",
    "gs_main = gridspec.GridSpec(3, 2, width_ratios=[1,1], height_ratios=[1,1, 1])  # Adjust width_ratios and height_ratios as needed\n",
    "\n",
    "\n",
    "# metric = 'obj_w'\n",
    "metric = 'obj_q'\n",
    "\n",
    "for i, score in enumerate(scores_to_use):\n",
    "    # scores averaged for each session\n",
    "    if score != 'firing_rate':\n",
    "        to_plot = df[df['score'] == score]\n",
    "        # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "    else:\n",
    "        to_plot = df[df['score'] == 'whole']\n",
    "        # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        # to_plot_count = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).count().reset_index()\n",
    "        # to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "    row, col = quad_arrange[i]\n",
    "    gs_sub = gridspec.GridSpecFromSubplotSpec(5, 1, subplot_spec=gs_main[row, col], height_ratios=[12,12,1,1,1], hspace=0.1)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[0])\n",
    "    axf = ax\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    means = []\n",
    "    sems = []\n",
    "    n = []\n",
    "    comps = score_beta_ps[score]\n",
    "    comp_maxs = []\n",
    "    for k in range(3):\n",
    "        c = gp_colors[k]\n",
    "        if score != 'firing_rate':\n",
    "            mtouse = metric \n",
    "        else:\n",
    "            mtouse = 'firing_rate'\n",
    "        bp = ax.boxplot(to_plot[to_plot['group'] == gp_labels[k]][mtouse], positions=[k], widths=0.5, \n",
    "                    notch=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=c, color='k'),\n",
    "                    capprops=dict(color='k'),\n",
    "                    whiskerprops=dict(color='k'),\n",
    "                    flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                    medianprops=dict(color='k'),\n",
    "                    showmeans=True, \n",
    "                    meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "        comp_maxs.append(np.max(to_plot[to_plot['group'] == gp_labels[k]][mtouse]))\n",
    "        for k2 in range(3):\n",
    "            if k2 != k:\n",
    "                lbl_pair = np.sort([gp_labels[k], gp_labels[k2]])\n",
    "                comp_group = lbl_pair[0] + '_' + lbl_pair[1]\n",
    "                if comp_group not in comps.keys():\n",
    "                    comps[comp_group] = np.nan\n",
    "\n",
    "                    if 'B6' in comp_group and 'ANT' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'ANT'])]\n",
    "                        group_order = ['B6', 'ANT']  # 'B6' becomes the reference group\n",
    "                    elif 'B6' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['B6', 'NON'])]\n",
    "                        group_order = ['B6', 'NON']\n",
    "                    elif 'ANT' in comp_group and 'NON' in comp_group:\n",
    "                        if score != 'firing_rate':\n",
    "                            to_plot_model = df[df['score'] == score]\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        else:\n",
    "                            to_plot_model = df[df['score'] == 'whole']\n",
    "                            # .groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "                        model_data = to_plot_model[to_plot_model['group'].isin(['NON', 'ANT'])]\n",
    "                        group_order = ['NON', 'ANT']\n",
    "                    # model_data['group'] = pd.Categorical(model_data['group'], categories=group_order, ordered=True)\n",
    "                    model_data.loc[:, 'group'] = pd.Categorical(model_data.loc[:, 'group'], categories=group_order, ordered=True)\n",
    "\n",
    "        means.append(np.mean(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "        sems.append(np.std(to_plot[to_plot['group'] == gp_labels[k]][metric]) / np.sqrt(len(to_plot[to_plot['group'] == gp_labels[k]][metric])))\n",
    "        n.append(len(to_plot[to_plot['group'] == gp_labels[k]][metric]))\n",
    "                                    \n",
    "        bps.append(bp['boxes'][0])\n",
    "        # lbls.append(str(means[k]) + ' ± ' + str(sems[k]) + ' cm, N = ' + str(n[k]))\n",
    "        lbls.append(gp_labels[k])\n",
    "        #  + ': N = ' + str(n[k]))\n",
    "    \n",
    "    ax.set_xticklabels(gp_labels)\n",
    "    ax.legend(bps, lbls, loc='upper right')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('EMD quantile')\n",
    "\n",
    "    # benjamini hochberg correction\n",
    "    kys, vals = zip(*comps.items())\n",
    "    # accepted, pvals_corrected, _, _ = multipletests(vals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    pvals_corrected = vals\n",
    "    accepted = np.array([True for x in range(len(vals))])\n",
    "\n",
    "    p_count = 0\n",
    "    for comp_key, val in comps.items():\n",
    "        comparison = comps[comp_key]\n",
    "        if 'ANT' in comp_key and 'B6' in comp_key:\n",
    "            nme = [0,2]\n",
    "        elif 'ANT' in comp_key and 'NON' in comp_key:\n",
    "            nme = [1,2]\n",
    "        elif 'B6' in comp_key and 'NON' in comp_key:\n",
    "            nme = [0,1]\n",
    "\n",
    "        # if accepted[k]:\n",
    "        barplot_annotate_brackets(nme[0],nme[1],pvals_corrected[p_count],[0,1,2], comp_maxs, maxasterix=5)\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "    # ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax = plt.subplot(gs_sub[1])\n",
    "    ax1 = ax\n",
    "\n",
    "    # # every row for that score\n",
    "    if score != 'firing_rate':\n",
    "        to_plot = df[df['score'] == score]\n",
    "        # to_plot = df[df['score'] == score].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "    else:\n",
    "        to_plot = df[df['score'] == 'whole']\n",
    "        # to_plot = df[df['score'] == 'whole'].groupby(['group', 'name', 'depth', 'date','stim','session_id']).mean().reset_index()\n",
    "        metric = 'firing_rate'\n",
    "        # to_plot = to_plot[to_plot['depth']]\n",
    "    to_plot_shuffle = to_plot.copy()\n",
    "\n",
    "\n",
    "\n",
    "    bps = []\n",
    "    lbls = []\n",
    "    shuffle_count = 1000\n",
    "    mns = [[] for j in range(3)]\n",
    "    mns_shuffle = [[[] for sc in range(shuffle_count)] for j in range(3)]\n",
    "    \n",
    "    group_ses_frs = {'ANT': [], 'B6': [], 'NON': []}\n",
    "    for k in range(len(gps)):\n",
    "        # c = gp_colors[k]\n",
    "        ses_visited = []\n",
    "        # ses_frs = []\n",
    "        for j in range(3):\n",
    "            # get group means + CI\n",
    "            to_plot_now = to_plot[to_plot['group'] == gp_labels[j]]\n",
    "            ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]][metric]\n",
    "            # ses_fr = to_plot_now[to_plot_now['session_id'] == gps[k]]['information']\n",
    "            bp = ax.boxplot(to_plot_now[to_plot_now['session_id'] == gps[k]][metric], positions=[k*3+j*.5], widths=0.5, \n",
    "                        notch=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=gp_colors[j],color='k'),\n",
    "                        capprops=dict(color='k'),\n",
    "                        whiskerprops=dict(color='k'),\n",
    "                        flierprops=dict(color='k', markeredgecolor='k'),\n",
    "                        medianprops=dict(color='k'),\n",
    "                        showmeans=False, \n",
    "                        meanprops=dict(markeredgecolor='k', markerfacecolor='k', markersize=10))\n",
    "            if k == 0:\n",
    "                bps.append(bp['boxes'][0])   \n",
    "\n",
    "            if len(ses_fr) > 0 and np.mean(ses_fr) == np.mean(ses_fr):       \n",
    "                group_ses_frs[gp_labels[j]].append(np.mean(ses_fr))\n",
    "\n",
    "            mn = np.mean(to_plot_now[to_plot_now['session_id'] == gps[k]][metric])\n",
    "            if mn == mn:\n",
    "                mns[j].append(mn)\n",
    "\n",
    "        for j in range(3):\n",
    "            for sc in range(shuffle_count):\n",
    "                out = _single_shuffle(to_plot_shuffle, gps[k], metric, gp_labels[j])\n",
    "                if out == out:\n",
    "                    mns_shuffle[j][sc].append(out)\n",
    "\n",
    "    # plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax = plt.subplot(gs_sub[4])\n",
    "    # bh_ant_b6 = np.hstack((bh_ant_b6, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_b6, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    ant_fr = group_ses_frs['ANT']\n",
    "    ant_interp = np.linspace(0, len(ant_fr)-1, shuffle_count)\n",
    "    ant_fr_smooth = np.interp(ant_interp, np.arange(len(ant_fr)), ant_fr)\n",
    "    im = ax.imshow(np.expand_dims(ant_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('ANT', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] -.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('ANT-B6', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[2])\n",
    "    # bh_b6_non = np.hstack((bh_b6_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_b6_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    b6_fr = group_ses_frs['B6']\n",
    "    b6_interp = np.linspace(0, len(b6_fr)-1, shuffle_count)\n",
    "    b6_fr_smooth = np.interp(b6_interp, np.arange(len(b6_fr)), b6_fr)\n",
    "    im = ax.imshow(np.expand_dims(b6_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('B6', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1]-.5))\n",
    "    ax.set_yticks([])\n",
    "    # ax.set_ylabel('B6-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs_sub[3])\n",
    "    # # bh_ant_non = np.hstack((bh_ant_non, [.5]))\n",
    "    # ax.imshow(np.expand_dims(bh_ant_non, 0), cmap='Greys_r', aspect='auto', extent=[0, len(gps), 0, 1], vmin=0, vmax=1)\n",
    "    non_fr = group_ses_frs['NON']\n",
    "    non_interp = np.linspace(0, len(non_fr)-1, shuffle_count)\n",
    "    non_fr_smooth = np.interp(non_interp, np.arange(len(non_fr)), non_fr)\n",
    "    im = ax.imshow(np.expand_dims(non_fr_smooth, 0), cmap='jet', aspect='auto', extent=[0, len(gps), 0, 1])\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ylbl = ax.set_ylabel('NON', labelpad=15, rotation=0)\n",
    "    # pos = ylbl.get_position()\n",
    "    # ylbl.set_position((pos[0], pos[1] - .5))\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    # ylbl = ax.set_ylabel('ANT-NON', labelpad=15, rotation=45)\n",
    "    ax.set_xticks(np.arange(len(gps)) + 0.5)\n",
    "    ax.set_xticklabels(gps)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "\n",
    "    # mann kendall\n",
    "    import pymannkendall as mk\n",
    "    from statsmodels.stats import multitest\n",
    "    lbls = []\n",
    "    lbl_colors = []\n",
    "    empirical = []\n",
    "    slopes = []\n",
    "    ps = []\n",
    "    mns_shuffle = np.array(mns_shuffle)\n",
    "    print(mns_shuffle.shape)\n",
    "    print('Metric: ' + score)\n",
    "    for j in range(3):\n",
    "        # polyfit \n",
    "\n",
    "        # slp, c = np.polyfit(np.arange(len(mns[j])), mns[j], 1)\n",
    "        # slopes.append(slp)\n",
    "\n",
    "        memp = mk.original_test(mns[j])\n",
    "        empirical.append(memp.z)\n",
    "        slopes.append(memp.slope)\n",
    "\n",
    "\n",
    "        shuffled = []\n",
    "        for sc in range(shuffle_count):\n",
    "            # if len(mns_shuffle[j,sc]) > 0:\n",
    "            ses_dist = mns_shuffle[j,sc]\n",
    "            # mshuffled, c = np.polyfit(np.arange(len(ses_dist)), ses_dist, 1)\n",
    "            mshuffled = mk.original_test(ses_dist)\n",
    "            shuffled.append(mshuffled.z)\n",
    "                # result = mk.original_test(ses_dist)\n",
    "                # ps.append(result.p)\n",
    "\n",
    "\n",
    "        # # two sided p-value \n",
    "        pgreater = np.sum(np.array(shuffled) < empirical[j]) / len(shuffled) \n",
    "        plower = np.sum(np.array(shuffled) > empirical[j]) / len(shuffled)\n",
    "        pvalue = np.min([pgreater, plower]) * 2\n",
    "        ps.append(pvalue)\n",
    "        print('Group: ' + gp_labels[j])\n",
    "        print('Empirical: ' + str(empirical[j]))\n",
    "        print('Shuffled: ' + str(np.mean(shuffled)) + ' ± ' + str(np.std(shuffled)))\n",
    "        # print('p-value: ' + str(np.min([pgreater, plower]) * 2))\n",
    "\n",
    "\n",
    "        # if empirical[j] > np.mean(shuffled):\n",
    "        #     tag = 'greater'\n",
    "        # elif empirical[j] < np.mean(shuffled):\n",
    "        #     tag = 'lower'\n",
    "        # lbl = 'Slope is ' + str(tag) + ' than shuffled: ' + str(np.round(memp, 2)) + ' , p = ' + str(np.round(pvalue, 3)) \n",
    "        # lbls.append(lbl)\n",
    "\n",
    "        # lbl = gp_labels[j] + ' \n",
    "        lbl = 'slope: ' + str(np.round(memp.slope, 4)) \n",
    "        lbl_colors.append('k')\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # result = mk.original_test(ses_dist)\n",
    "    print(ps)\n",
    "    out = multitest.multipletests(ps, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "    cc = 0\n",
    "    for case in out[0]:\n",
    "        if case:\n",
    "            lbl_colors[cc] = 'r'\n",
    "            # lbls[cc] = lbls[cc] + ' & is sig after BH correction'\n",
    "        # else:\n",
    "            # lbls[cc] = lbls[cc] + ' & is NOT sig after BH correction'\n",
    "        cc += 1\n",
    "    print(out)\n",
    "\n",
    "    ax1.legend(bps, lbls, loc='upper right')\n",
    "    # color label text in legend\n",
    "    idx = 0\n",
    "    for text, color in zip(ax1.legend_.get_texts(), lbl_colors):\n",
    "        # text.set_color(color)\n",
    "        # set font weight to bold\n",
    "        if color == 'r':\n",
    "            text.set_weight('bold')\n",
    "        pval = out[1][idx]\n",
    "        if pval <= 0.05:\n",
    "            astk = '*'\n",
    "        if pval <= 0.01:\n",
    "            astk = '**'\n",
    "        if pval <= 0.001:\n",
    "            astk = '***'\n",
    "        if pval <= 0.0001:\n",
    "            astk = '****'\n",
    "        if pval > 0.05:\n",
    "            astk = 'n.s'\n",
    "        print(text)\n",
    "        print(text.get_text())\n",
    "        new_text = str(text.get_text()) + ' ' + str(astk)\n",
    "        text.set_text(new_text)\n",
    "        idx += 1\n",
    "\n",
    "    # ax1.set_title(score)\n",
    "    # ax.set_xlabel('Session')\n",
    "    axf.set_title(titles_to_use[i])\n",
    "    ax1.set_ylabel('EMD quantile')\n",
    "              \n",
    "    # ax1.set_xticks(np.arange(len(gps)) * 3 + 1.25/2)\n",
    "    # ax1.set_xticklabels(gps)\n",
    "    # ax1.set_xlim([-1.25/2, len(gps) * 3 - 1.25/2])\n",
    "\n",
    "    ax1.set_xticks(np.arange(len(gps)) * 3 + .5)\n",
    "    ax1.set_xticklabels(gps)\n",
    "    ax1.set_ylim(-0.05,)\n",
    "    # ax1.set_xlim([-.5/2, len(gps) * 3 - .5/2])\n",
    "\n",
    "\n",
    "fig.suptitle('All indiv. cell-session appearances', fontweight='bold')\n",
    "# fig.suptitle('Averaged by session', fontweight='bold')\n",
    "gs_main.tight_layout(fig, rect=[0, 0, 1, 0.98])\n",
    "# fig.suptitle('Averaged by session')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\LEC_full_merged_scores.xlsx')\n",
    "df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC.xlsx')\n",
    "\n",
    "# df['obj_q'] = df['obj_q_NO']\n",
    "df['obj_q'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].min(axis=1)\n",
    "df['obj_a'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].idxmin(axis=1)\n",
    "# convert obj_a to degrees\n",
    "df['obj_a'] = df['obj_a'].apply(lambda x: int(x.split('_')[2]))\n",
    "# use obj_wass with angle of min quantile\n",
    "df['obj_w'] = df.apply(lambda x: x['obj_wass_' + str(x['obj_a'])], axis=1)\n",
    "# drop spike count column \n",
    "df = df.drop(columns=['spike_count'])\n",
    "# rename spike_count.1 to spike_count\n",
    "df = df.rename(columns={'spike_count.1': 'spike_count'})\n",
    "\n",
    "consecutive_sessions_threshold = 2\n",
    "quantile_threshold = 0.2\n",
    "consecutive = False\n",
    "score = 'field'\n",
    "main_field_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist = []\n",
    "ses_cut_dict = {}\n",
    "\n",
    "for ses_limit in ['session_3', 'session_4', 'session_5', 'session_6']:\n",
    "# for ses_limit in ['session_7']:\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    scores_to_compare = ['centroid', 'field']\n",
    "    lim = ses_limit.split('_')[1]\n",
    "    ses_to_use = ['session_1', 'session_2']\n",
    "    for l in range(int(lim)):\n",
    "        ses_to_use.append('session_' + str(l+1))\n",
    "    df_use = df[df['session_id'].isin(ses_to_use)]\n",
    "    # 'session_4','session_5','session_6'])]   \n",
    "    main_centroid_df = df_use[(df_use['score'] == 'centroid') & (df_use['field_id'] == 1)]\n",
    "    main_field_df = df_use[(df_use['score'] == 'field') & (df_use['field_id'] == 1)]\n",
    "    all_field_df = df_use[df_use['score'] == 'field']\n",
    "    all_centroid_df = df_use[df_use['score'] == 'centroid']\n",
    "\n",
    "\n",
    "    assert len(main_centroid_df) == len(main_field_df)\n",
    "    assert len(all_field_df) == len(all_centroid_df)\n",
    "\n",
    "    matched = []\n",
    "    unmatched = []\n",
    "    all_matched = []\n",
    "    all_unmatched = []\n",
    "    all_ambig = []\n",
    "    all_unambig = []\n",
    "    all_diffs = []\n",
    "    for i in range(len(all_field_df)):\n",
    "        all_field_obj_a = all_field_df.iloc[i]['obj_a']\n",
    "        all_centroid_obj_a = all_centroid_df.iloc[i]['obj_a']\n",
    "\n",
    "        obj_qs = [all_field_df.iloc[i]['obj_q_0'], all_field_df.iloc[i]['obj_q_90'], all_field_df.iloc[i]['obj_q_180'], all_field_df.iloc[i]['obj_q_270']]\n",
    "        sorted_obj_qs = np.sort(obj_qs)\n",
    "        min1 = sorted_obj_qs[0]\n",
    "        min2 = sorted_obj_qs[1]\n",
    "        all_diffs.append(abs(min1 - min2))\n",
    "\n",
    "        if abs(min1 - min2) < 0.05:\n",
    "            all_ambig.append(i)\n",
    "        else:\n",
    "            all_unambig.append(i)\n",
    "\n",
    "        if all_field_obj_a == all_centroid_obj_a:\n",
    "            all_matched.append(i)\n",
    "        else:\n",
    "            all_unmatched.append(i)\n",
    "    for i in range(len(main_field_df)):\n",
    "        field_obj_a = main_field_df.iloc[i]['obj_a']\n",
    "        centroid_obj_a = main_centroid_df.iloc[i]['obj_a']\n",
    "        if field_obj_a == centroid_obj_a:\n",
    "            matched.append(i)\n",
    "        else:\n",
    "            unmatched.append(i)\n",
    "    df_matched_field = main_field_df.iloc[matched]\n",
    "    df_all_matched_field = all_field_df.iloc[all_matched]\n",
    "    df_all_ambiguous_field = all_field_df.iloc[all_ambig]\n",
    "    df_unmatched_field = main_field_df.iloc[unmatched]\n",
    "    df_all_unmatched_field = all_field_df.iloc[all_unmatched]\n",
    "    df_all_unambiguous_field = all_field_df.iloc[all_unambig]\n",
    "    df_matched_centroid = main_centroid_df.iloc[matched]\n",
    "    df_all_matched_centroid = all_centroid_df.iloc[all_matched]\n",
    "    df_unmatched_centroid = main_centroid_df.iloc[unmatched]\n",
    "    df_all_unmatched_centroid = all_centroid_df.iloc[all_unmatched]\n",
    "    assert len(df_matched_field) == len(df_matched_centroid)\n",
    "    assert len(df_all_matched_field) == len(df_all_matched_centroid)\n",
    "    assert len(df_unmatched_field) == len(df_unmatched_centroid)\n",
    "    assert len(df_all_unmatched_field) == len(df_all_unmatched_centroid)\n",
    "    print('There are {} rows where main field distance and centroid distance are in the same direction'.format(len(df_matched_field)))\n",
    "    print('There are {} rows where main field distance and centroid distance are in different directions'.format(len(df_unmatched_field)))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in the same direction'.format(len(df_all_matched_field)))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in different directions'.format(len(df_all_unmatched_field)))\n",
    "\n",
    "    ANT_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'ANT']) / (len(df_matched_field[df_matched_field['group'] == 'ANT']) + len(df_unmatched_field[df_unmatched_field['group'] == 'ANT'])))\n",
    "    ANT_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'ANT']) / (len(df_matched_field[df_matched_field['group'] == 'ANT']) + len(df_unmatched_field[df_unmatched_field['group'] == 'ANT'])))\n",
    "    B6_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'B6']) / (len(df_matched_field[df_matched_field['group'] == 'B6']) + len(df_unmatched_field[df_unmatched_field['group'] == 'B6'])))\n",
    "    B6_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'B6']) / (len(df_matched_field[df_matched_field['group'] == 'B6']) + len(df_unmatched_field[df_unmatched_field['group'] == 'B6'])))\n",
    "    NON_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'NON']) / (len(df_matched_field[df_matched_field['group'] == 'NON']) + len(df_unmatched_field[df_unmatched_field['group'] == 'NON'])))\n",
    "    NON_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'NON']) / (len(df_matched_field[df_matched_field['group'] == 'NON']) + len(df_unmatched_field[df_unmatched_field['group'] == 'NON'])))\n",
    "\n",
    "    print('There are {} rows where main field distance and centroid distance are in the same direction for ANT'.format(ANT_same_dir))\n",
    "    print('There are {} rows where main field distance and centroid distance are in different directions for ANT'.format(ANT_diff_dir))\n",
    "    print('There are {} rows where main field distance and centroid distance are in the same direction for B6'.format(B6_same_dir))\n",
    "    print('There are {} rows where main field distance and centroid distance are in different directions for B6'.format(B6_diff_dir))\n",
    "    print('There are {} rows where main field distance and centroid distance are in the same direction for NON'.format(NON_same_dir))\n",
    "    print('There are {} rows where main field distance and centroid distance are in different directions for NON'.format(NON_diff_dir))\n",
    "\n",
    "    ANT_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT'])))\n",
    "    ANT_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT'])))\n",
    "    B6_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6'])))\n",
    "    B6_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6'])))\n",
    "    NON_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON'])))\n",
    "    NON_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON'])))\n",
    "\n",
    "    print('There are {} rows where all field distance and all centroid distance are in the same direction for ANT'.format(ANT_same_dir_all))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in different directions for ANT'.format(ANT_diff_dir_all))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in the same direction for B6'.format(B6_same_dir_all))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in different directions for B6'.format(B6_diff_dir_all))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in the same direction for NON'.format(NON_same_dir_all))\n",
    "    print('There are {} rows where all field distance and all centroid distance are in different directions for NON'.format(NON_diff_dir_all))\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    group_by_unique_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "\n",
    "    object_cell_df = df[df['score'] == score]\n",
    "    object_cell_df = object_cell_df[object_cell_df['session_id'].isin(ses_to_use)]   \n",
    "    # 'session_4','session_5','session_6'])]\n",
    "\n",
    "    group_by_unique_cell_field = ['group', 'name', 'depth', 'date','tetrode', 'unit_id', 'field_id']\n",
    "\n",
    "    ANT_df = object_cell_df[object_cell_df['group'] == 'ANT'].reset_index(drop=True)\n",
    "    B6_df = object_cell_df[object_cell_df['group'] == 'B6'].reset_index(drop=True)\n",
    "    NON_df = object_cell_df[object_cell_df['group'] == 'NON'].reset_index(drop=True)\n",
    "\n",
    "    ANT_object_cell_df = ANT_df.copy() \n",
    "    B6_object_cell_df = B6_df.copy() \n",
    "    NON_object_cell_df = NON_df.copy() \n",
    "    ANT_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "    B6_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "    NON_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "    ANT_object_cell_df['cell_type'] = 'unassigned'\n",
    "    B6_object_cell_df['cell_type'] = 'unassigned'\n",
    "    NON_object_cell_df['cell_type'] = 'unassigned'\n",
    "    ANT_object_cell_df['isTrace'] = 0\n",
    "    B6_object_cell_df['isTrace'] = 0\n",
    "    NON_object_cell_df['isTrace'] = 0\n",
    "    ANT_object_cell_df['isObject'] = 0\n",
    "    B6_object_cell_df['isObject'] = 0\n",
    "    NON_object_cell_df['isObject'] = 0\n",
    "    ANT_object_cell_df['trace_a'] = None\n",
    "    B6_object_cell_df['trace_a'] = None\n",
    "    NON_object_cell_df['trace_a'] = None\n",
    "\n",
    "\n",
    "    ANT_object_cell_df.loc[ANT_object_cell_df['obj_a'].astype(str)  == ANT_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "    B6_object_cell_df.loc[B6_object_cell_df['obj_a'].astype(str)  == B6_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "    NON_object_cell_df.loc[NON_object_cell_df['obj_a'].astype(str)  == NON_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "\n",
    "    keep_trace_appearances = []\n",
    "    for df_touse in [ANT_object_cell_df, B6_object_cell_df, NON_object_cell_df]:\n",
    "        prev_angles = []\n",
    "        prev_unit_id = None\n",
    "        prev_field_id = None\n",
    "        prev_tetrode = None\n",
    "        prev_name = None\n",
    "        prev_date = None\n",
    "        prev_depth = None\n",
    "        to_keep_trace_appearance = []\n",
    "        for i, row in df_touse.iterrows():\n",
    "            curr_unit_id = row['unit_id']\n",
    "            curr_tetrode = row['tetrode']\n",
    "            curr_field_id = row['field_id']\n",
    "            curr_angle = row['obj_a']\n",
    "            curr_name = row['name']\n",
    "            curr_date = row['date']\n",
    "            curr_depth = row['depth']\n",
    "\n",
    "            if curr_unit_id == prev_unit_id and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "                if str(curr_angle) in prev_angles:\n",
    "                    to_keep_trace_appearance.append(i)\n",
    "            else:\n",
    "                prev_angles = []\n",
    "\n",
    "            prev_unit_id = curr_unit_id\n",
    "            prev_angle = curr_angle\n",
    "            prev_field_id = curr_field_id\n",
    "            prev_tetrode = curr_tetrode\n",
    "            prev_name = curr_name\n",
    "            prev_date = curr_date\n",
    "            prev_depth = curr_depth\n",
    "            prev_angles.append(row['object_location'])\n",
    "\n",
    "        keep_trace_appearances.append(to_keep_trace_appearance)\n",
    "\n",
    "    ANT_object_cell_df.loc[keep_trace_appearances[0],'isTrace'] = 1\n",
    "    B6_object_cell_df.loc[keep_trace_appearances[1],'isTrace'] = 1\n",
    "    NON_object_cell_df.loc[keep_trace_appearances[2],'isTrace'] = 1\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    import copy\n",
    "\n",
    "    group_by_unique_cell_session = ['group', 'name', 'depth', 'date', 'tetrode', 'unit_id','session_id'] # (joins field rowsfrom that ses)\n",
    "\n",
    "    for ses_cut in [lim]:\n",
    "        ses_cut = int(ses_cut)\n",
    "\n",
    "        center_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}\n",
    "\n",
    "        center_inv_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}\n",
    "\n",
    "        quality_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}   \n",
    "\n",
    "        quality_inv_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}           \n",
    "\n",
    "        ambiguous_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}       \n",
    "\n",
    "        ambiguous_inv_type_dict = {'ANT': 0,\n",
    "                        'B6':0,\n",
    "                        'NON': 0}\n",
    "\n",
    "        ses_cut_dict[ses_cut] = {}\n",
    "        ANT_object_cell_df_to_use = ANT_object_cell_df[ANT_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        B6_object_cell_df_to_use = B6_object_cell_df[B6_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        NON_object_cell_df_to_use = NON_object_cell_df[NON_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        df_all_unmatched_field_to_use = df_all_unmatched_field[df_all_unmatched_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        df_all_matched_field_to_use = df_all_matched_field[df_all_matched_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        # df_all_unmatched_field_to_use = df_all_ambiguous_field[df_all_ambiguous_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        # df_all_matched_field_to_use = df_all_unambiguous_field[df_all_unambiguous_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "        c = 0\n",
    "        for df_current in [ANT_object_cell_df_to_use, B6_object_cell_df_to_use, NON_object_cell_df_to_use]:\n",
    "            quality_dropped_identifiers = df_current[df_current['iso_dist'] < 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            quality_non_dropped_identifiers = df_current[df_current['iso_dist'] >= 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            center_dropped_identifiers = df_current[df_current['obj_q_NO'] < df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            center_non_dropped_identifiers = df_current[df_current['obj_q_NO'] >= df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            ambiguous_dropped_identifiers = df_all_unmatched_field_to_use[df_all_unmatched_field_to_use['group'] == ['ANT','B6','NON'][c]].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            ambiguous_non_dropped_identifiers = df_all_matched_field_to_use[df_all_matched_field_to_use['group'] == ['ANT','B6','NON'][c]].groupby(group_by_unique_cell_session).groups.keys()\n",
    "            # mask_dropped = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in ambiguous_dropped_identifiers], axis=1).any(axis=1)\n",
    "            # mask_non_dropped = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in ambiguous_non_dropped_identifiers], axis=1).any(axis=1)\n",
    "            if c == 0:\n",
    "                quality_type_dict['ANT'] = len(quality_dropped_identifiers)\n",
    "                quality_inv_type_dict['ANT'] = len(quality_non_dropped_identifiers)\n",
    "                center_type_dict['ANT'] = len(center_dropped_identifiers)\n",
    "                center_inv_type_dict['ANT'] = len(center_non_dropped_identifiers)\n",
    "                ambiguous_type_dict['ANT'] = len(ambiguous_dropped_identifiers)\n",
    "                ambiguous_inv_type_dict['ANT'] = len(ambiguous_non_dropped_identifiers)\n",
    "            elif c == 1:\n",
    "                quality_type_dict['B6'] = len(quality_dropped_identifiers)\n",
    "                quality_inv_type_dict['B6'] = len(quality_non_dropped_identifiers)\n",
    "                center_type_dict['B6'] = len(center_dropped_identifiers)\n",
    "                center_inv_type_dict['B6'] = len(center_non_dropped_identifiers)\n",
    "                ambiguous_type_dict['B6'] = len(ambiguous_dropped_identifiers)\n",
    "                ambiguous_inv_type_dict['B6'] = len(ambiguous_non_dropped_identifiers)\n",
    "            elif c == 2:\n",
    "                quality_type_dict['NON'] = len(quality_dropped_identifiers)\n",
    "                quality_inv_type_dict['NON'] = len(quality_non_dropped_identifiers)\n",
    "                center_type_dict['NON'] = len(center_dropped_identifiers)\n",
    "                center_inv_type_dict['NON'] = len(center_non_dropped_identifiers)\n",
    "                ambiguous_type_dict['NON'] = len(ambiguous_dropped_identifiers)\n",
    "                ambiguous_inv_type_dict['NON'] = len(ambiguous_non_dropped_identifiers)\n",
    "\n",
    "            c += 1\n",
    "        ses_cut_dict[ses_cut]['quality_type_dict'] = copy.deepcopy(quality_type_dict)\n",
    "        ses_cut_dict[ses_cut]['quality_inv_type_dict'] = copy.deepcopy(quality_inv_type_dict)\n",
    "        ses_cut_dict[ses_cut]['center_type_dict'] = copy.deepcopy(center_type_dict)\n",
    "        ses_cut_dict[ses_cut]['center_inv_type_dict'] = copy.deepcopy(center_inv_type_dict)\n",
    "        ses_cut_dict[ses_cut]['ambiguous_type_dict'] = copy.deepcopy(ambiguous_type_dict)\n",
    "        ses_cut_dict[ses_cut]['ambiguous_inv_type_dict'] = copy.deepcopy(ambiguous_inv_type_dict)\n",
    "\n",
    "    c = 0\n",
    "    for df_current in [ANT_object_cell_df, B6_object_cell_df, NON_object_cell_df]:\n",
    "\n",
    "        param = 6.5\n",
    "        # filter out rows where iso_dist is < 5 - Quality control\n",
    "        quality_dropped_identifiers = df_current[df_current['iso_dist'] < param].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        quality_non_dropped_identifiers = df_current[df_current['iso_dist'] >= param].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        df_current = df_current[df_current['iso_dist'] >= param]\n",
    "\n",
    "        # # filter out rows where obj_q_NO is < obj_q - CLOSER to middle than a side\n",
    "        # center_dropped_identifiers = df_current[df_current['obj_q_NO'] < df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        # center_non_dropped_identifiers = df_current[df_current['obj_q_NO'] >= df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        # df_current = df_current[df_current['obj_q_NO'] >= df_current['obj_q']] \n",
    "        # \"\"\" HAVE TO RE RUN ALL U HAD THIS LINE AS /3, need ot check combos of ambiguous classic and mbiuous theshold 0.05 \"\"\"\n",
    "\n",
    "        # # filter out rows where obj_a for centroid is != obj_a for field - Ambiguous\n",
    "        # ambiguous_dropped_identifiers = df_all_unmatched_field.groupby(group_by_unique_cell_session).groups.keys()\n",
    "        # # ambiguous_dropped_identifiers = df_all_ambiguous_field.groupby(group_by_unique_cell_session).groups.keys()\n",
    "        # mask = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6 & (df_current['session_id'] == id7)) for id1, id2, id3, id4, id5, id6, id7 in ambiguous_dropped_identifiers], axis=1).any(axis=1)\n",
    "        # df_current = df_current[~mask]\n",
    "\n",
    "        # # filter out > 3 fields\n",
    "        # field_dropped_identifiers = df_current[df_current['field_id'] > 3].groupby(group_by_unique_cell).groups.keys()\n",
    "        # df_current = df_current[df_current['field_id'] <= 3]\n",
    "\n",
    "        # filter out less than 2 sessions\n",
    "        remaining_session_dropped_identifiers = df_current.groupby(group_by_unique_cell).filter(lambda x: len(x) < 2).groupby(group_by_unique_cell).groups.keys()\n",
    "        df_current = df_current.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2)\n",
    "\n",
    "        if c == 0:\n",
    "            ANT_object_cell_df = df_current\n",
    "        elif c == 1:\n",
    "            B6_object_cell_df = df_current\n",
    "        elif c == 2:\n",
    "            NON_object_cell_df = df_current\n",
    "\n",
    "        c += 1  \n",
    "        \n",
    "    ANT_cell_type_df = ANT_object_cell_df.copy()\n",
    "    B6_cell_type_df = B6_object_cell_df.copy()\n",
    "    NON_cell_type_df = NON_object_cell_df.copy()\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    identifier_dict = {}\n",
    "\n",
    "    identifier_dict['object'] = {}\n",
    "    identifier_dict['trace'] = {}\n",
    "    identifier_dict['unassigned'] = {}\n",
    "\n",
    "    identifier_dict['object']['ANT'] = list(ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "    identifier_dict['object']['B6'] = list(B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "    identifier_dict['object']['NON'] = list(NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "    mask = pd.concat([(ANT_cell_type_df['group'] == id1) & (ANT_cell_type_df['name'] == id2) & (ANT_cell_type_df['depth'] == id3) & (ANT_cell_type_df['date'] == id4) & (ANT_cell_type_df['tetrode'] == id5) & (ANT_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['ANT']], axis=1).any(axis=1)\n",
    "    ANT_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "    mask = pd.concat([(B6_cell_type_df['group'] == id1) & (B6_cell_type_df['name'] == id2) & (B6_cell_type_df['depth'] == id3) & (B6_cell_type_df['date'] == id4) & (B6_cell_type_df['tetrode'] == id5) & (B6_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['B6']], axis=1).any(axis=1)\n",
    "    B6_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "    mask = pd.concat([(NON_cell_type_df['group'] == id1) & (NON_cell_type_df['name'] == id2) & (NON_cell_type_df['depth'] == id3) & (NON_cell_type_df['date'] == id4) & (NON_cell_type_df['tetrode'] == id5) & (NON_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['NON']], axis=1).any(axis=1)\n",
    "    NON_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "\n",
    "        only_object_ANT = ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "        only_object_B6 = B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "        only_object_NON = NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "        only_object = pd.concat([only_object_ANT, only_object_B6, only_object_NON], axis=0)\n",
    "        only_object.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_object.xlsx')\n",
    "\n",
    "    filt = ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = ANT_cell_type_df[(ANT_cell_type_df['isObject'] == 0) & (ANT_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['ANT']]\n",
    "    identifier_dict['trace']['ANT'] = list(unique_set)\n",
    "    mask = pd.concat([(ANT_cell_type_df['group'] == id1) & (ANT_cell_type_df['name'] == id2) & (ANT_cell_type_df['depth'] == id3) & (ANT_cell_type_df['date'] == id4) & (ANT_cell_type_df['tetrode'] == id5) & (ANT_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['ANT']], axis=1).any(axis=1)\n",
    "    ANT_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "\n",
    "        only_trace_ANT_set1 = filt[filt['isTrace'] == 1]\n",
    "        only_trace_ANT_set2 = ANT_cell_type_df[(ANT_cell_type_df['isObject'] == 0) & (ANT_cell_type_df['isTrace'] == 1)]\n",
    "        only_trace_ANT = pd.concat([only_trace_ANT_set1, only_trace_ANT_set2], axis=0)\n",
    "\n",
    "    filt = B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = B6_cell_type_df[(B6_cell_type_df['isObject'] == 0) & (B6_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['B6']]\n",
    "    identifier_dict['trace']['B6'] = list(unique_set)\n",
    "    mask = pd.concat([(B6_cell_type_df['group'] == id1) & (B6_cell_type_df['name'] == id2) & (B6_cell_type_df['depth'] == id3) & (B6_cell_type_df['date'] == id4) & (B6_cell_type_df['tetrode'] == id5) & (B6_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['B6']], axis=1).any(axis=1)\n",
    "    B6_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "\n",
    "        only_trace_B6_set1 = filt[filt['isTrace'] == 1]\n",
    "        only_trace_B6_set2 = B6_cell_type_df[(B6_cell_type_df['isObject'] == 0) & (B6_cell_type_df['isTrace'] == 1)]\n",
    "        only_trace_B6 = pd.concat([only_trace_B6_set1, only_trace_B6_set2], axis=0)\n",
    "\n",
    "    filt = NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = NON_cell_type_df[(NON_cell_type_df['isObject'] == 0) & (NON_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # .groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['NON']]\n",
    "    identifier_dict['trace']['NON'] = list(unique_set)\n",
    "    mask = pd.concat([(NON_cell_type_df['group'] == id1) & (NON_cell_type_df['name'] == id2) & (NON_cell_type_df['depth'] == id3) & (NON_cell_type_df['date'] == id4) & (NON_cell_type_df['tetrode'] == id5) & (NON_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['NON']], axis=1).any(axis=1)\n",
    "    NON_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "    \n",
    "    if ses_limit == 'session_3':\n",
    "        only_trace_NON_set1 = filt[filt['isTrace'] == 1]\n",
    "        only_trace_NON_set2 = NON_cell_type_df[(NON_cell_type_df['isObject'] == 0) & (NON_cell_type_df['isTrace'] == 1)]\n",
    "        only_trace_NON = pd.concat([only_trace_NON_set1, only_trace_NON_set2], axis=0)\n",
    "\n",
    "        only_trace = pd.concat([only_trace_ANT, only_trace_B6, only_trace_NON], axis=0)\n",
    "        only_trace.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_trace.xlsx')\n",
    "\n",
    "    objectfiltout = ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = ANT_cell_type_df[(ANT_cell_type_df['isTrace'] == 0) & (ANT_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['trace']['ANT']]\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['ANT']]\n",
    "    identifier_dict['unassigned']['ANT'] = list(unique_set)\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "            \n",
    "        only_unassigned_ANT_set1 = objectfiltout[objectfiltout['isTrace'] == 0]\n",
    "        only_unassigned_ANT_set2 = ANT_cell_type_df[(ANT_cell_type_df['isTrace'] == 0) & (ANT_cell_type_df['isObject'] == 0)]\n",
    "        only_unassigned_ANT = pd.concat([only_unassigned_ANT_set1, only_unassigned_ANT_set2], axis=0)\n",
    "\n",
    "    objectfiltout = B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = B6_cell_type_df[(B6_cell_type_df['isTrace'] == 0) & (B6_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['trace']['B6']]\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['B6']]\n",
    "    identifier_dict['unassigned']['B6'] = list(unique_set)\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "            \n",
    "        only_unassigned_B6_set1 = objectfiltout[objectfiltout['isTrace'] == 0]\n",
    "        only_unassigned_B6_set2 = B6_cell_type_df[(B6_cell_type_df['isTrace'] == 0) & (B6_cell_type_df['isObject'] == 0)]\n",
    "        only_unassigned_B6 = pd.concat([only_unassigned_B6_set1, only_unassigned_B6_set2], axis=0)\n",
    "\n",
    "    objectfiltout = NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "    first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "    second_set = NON_cell_type_df[(NON_cell_type_df['isTrace'] == 0) & (NON_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "    # unq_set = set(first_set).difference(second_set)\n",
    "    unique_set = list(set(list(first_set) + list(second_set)))\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['trace']['NON']]\n",
    "    unique_set = [x for x in unique_set if x not in identifier_dict['object']['NON']]\n",
    "    identifier_dict['unassigned']['NON'] = list(unique_set)\n",
    "\n",
    "    if ses_limit == 'session_3':\n",
    "                \n",
    "        only_unassigned_NON_set1 = objectfiltout[objectfiltout['isTrace'] == 0]\n",
    "        only_unassigned_NON_set2 = NON_cell_type_df[(NON_cell_type_df['isTrace'] == 0) & (NON_cell_type_df['isObject'] == 0)]\n",
    "        only_unassigned_NON = pd.concat([only_unassigned_NON_set1, only_unassigned_NON_set2], axis=0)\n",
    "\n",
    "        only_unassigned = pd.concat([only_unassigned_ANT, only_unassigned_B6, only_unassigned_NON], axis=0)\n",
    "        only_unassigned.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_unassigned.xlsx')\n",
    "\n",
    "    dlist.append(identifier_dict)\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cell_type_df = pd.concat([ANT_cell_type_df, B6_cell_type_df, NON_cell_type_df], axis=0)\n",
    "# full_cell_type_df.to_excel('/Users/alexgonzalez/Google Drive/PostDoc/Data/ephys_summary/summary_cell_type_df.xlsx')\n",
    "full_cell_type_df.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_assigned.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Assuming 'full_cell_type' is the DataFrame\n",
    "groups = ['ANT', 'NON', 'B6']\n",
    "cell_types = ['object','trace','unassigned']\n",
    "cell_type_opp = ['non-object', 'non-trace', 'assigned']\n",
    "identifier_dict = dlist[0]\n",
    "#  'trace', 'trace_2', 'unassigned']\n",
    "\n",
    "\n",
    "type_dict = {'ANT': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'B6': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'NON': {'object': 0, 'trace': 0, 'unassigned': 0}}\n",
    "\n",
    "inv_type_dict = {'ANT': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'B6': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'NON': {'object': 0, 'trace': 0, 'unassigned': 0}}\n",
    "\n",
    "for ctype in identifier_dict:\n",
    "    for group in identifier_dict[ctype]:\n",
    "        type_dict[group][ctype] += len(identifier_dict[ctype][group])\n",
    "        for ctype2 in identifier_dict:\n",
    "            if ctype2 != ctype:\n",
    "                inv_type_dict[group][ctype] += len(identifier_dict[ctype2][group])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "comparisons = []\n",
    "odds_ratios = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i+1, len(groups)):\n",
    "        group1 = groups[i]\n",
    "        group2 = groups[j]\n",
    "        \n",
    "        ctype_count = 0\n",
    "        \n",
    "        for cell_type in cell_types:\n",
    "            \n",
    "            contingency_table_2x2 = pd.DataFrame({str(cell_type): [type_dict[group1][cell_type], type_dict[group2][cell_type]],\n",
    "                                    str(cell_type_opp[ctype_count]): [inv_type_dict[group1][cell_type], inv_type_dict[group2][cell_type]]},\n",
    "                                    index=[group1, group2])\n",
    "            print(contingency_table_2x2)\n",
    "            # Performing Fisher's Exact Test\n",
    "            odds_ratio, p_value = fisher_exact(contingency_table_2x2)\n",
    "\n",
    "            # Storing results\n",
    "            comparisons.append(f\"{group1} vs. {group2} - {cell_type.upper()}\")\n",
    "            odds_ratios.append(odds_ratio)\n",
    "            p_values.append(p_value)\n",
    "            ctype_count += 1\n",
    "\n",
    "# Adjust p-values for multiple comparisons using the Benjamini-Hochberg method\n",
    "rejected, adjusted_p_values, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05, is_sorted=False, returnsorted=False)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Accepted\": rejected,\n",
    "    \"Comparison\": comparisons,\n",
    "    \"Odds Ratio\": odds_ratios,\n",
    "    \"P-value\": p_values,\n",
    "    \"Adjusted P-value\": adjusted_p_values\n",
    "})\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist_to_use = [ses_cut_dict[3]['quality_type_dict'], ses_cut_dict[3]['center_type_dict'], ses_cut_dict[3]['ambiguous_type_dict']]\n",
    "dlist_inv = [ses_cut_dict[3]['quality_inv_type_dict'], ses_cut_dict[3]['center_inv_type_dict'], ses_cut_dict[3]['ambiguous_inv_type_dict']]\n",
    "dflist = []\n",
    "\n",
    "for ct in range(len(dlist_to_use)):\n",
    "    tp_dict = dlist_to_use[ct]\n",
    "    tp_inv_dict = dlist_inv[ct]\n",
    "    # Initialize empty lists to store results\n",
    "    comparisons = []\n",
    "    odds_ratios = []\n",
    "    p_values = []\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        for j in range(i+1, len(groups)):\n",
    "            group1 = groups[i]\n",
    "            group2 = groups[j]\n",
    "                        \n",
    "                \n",
    "            contingency_table_2x2 = pd.DataFrame({'excluded': [tp_dict[group1], tp_dict[group2]],\n",
    "                                    'included': [tp_inv_dict[group1], tp_inv_dict[group2]]},\n",
    "                                    index=[group1, group2])\n",
    "            print(contingency_table_2x2)\n",
    "            # Performing Fisher's Exact Test\n",
    "            odds_ratio, p_value = fisher_exact(contingency_table_2x2)\n",
    "            # Storing results\n",
    "            comparisons.append(f\"{group1} vs. {group2} - {cell_type.upper()}\")\n",
    "            odds_ratios.append(odds_ratio)\n",
    "            p_values.append(p_value)\n",
    "\n",
    "    # Adjust p-values for multiple comparisons using the Benjamini-Hochberg method\n",
    "    rejected, adjusted_p_values, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05, is_sorted=False, returnsorted=False)\n",
    "\n",
    "    # Display the results\n",
    "    res_df = pd.DataFrame({\n",
    "        \"Accepted\": rejected,\n",
    "        \"Comparison\": comparisons,\n",
    "        \"Odds Ratio\": odds_ratios,\n",
    "        \"P-value\": p_values,\n",
    "        \"Adjusted P-value\": adjusted_p_values\n",
    "    })\n",
    "\n",
    "    dflist.append(res_df)\n",
    "\n",
    "    print(res_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11517986/indicating-the-statistically-significant-difference-in-bar-graph\n",
    "\n",
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05\n",
    "        # ** is p < 0.005\n",
    "        # *** is p < 0.0005\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        if data <= 0.05:\n",
    "            text = '*'\n",
    "        if data <= 0.01:\n",
    "            text = '**'\n",
    "        if data <= 0.001:\n",
    "            text = '***'\n",
    "        if data <= 0.0001:\n",
    "            text = '****'\n",
    "\n",
    "        # while data < p:\n",
    "        #     text += '*'\n",
    "        #     p /= 10.\n",
    "\n",
    "        #     if maxasterix and len(text) == maxasterix:\n",
    "        #         break\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black')\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "# ses1234_identifier_dict, ses12345_identifier_dict, ses123456_identifier_dict]\n",
    "dtitles = ['Session 1 to 3', 'Session 1 to 4', 'Session 1 to 5', 'Session 1 to 6']\n",
    "# dpositions = [0.93, 0.68, 0.33, 0.03]\n",
    "# axids = [[[0,0],[0,1],[0,2]], [[1,0],[1,1],[1,2]], [[2,0],[2,1],[2,2]], [[3,0],[3,1],[3,2]]]\n",
    "lbls = ['N=3', 'N=4', 'N=5', 'N=6']\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "# gspec 3x3\n",
    "gspec = gridspec.GridSpec(2,3)\n",
    "\n",
    "b6notdone_1 = True\n",
    "b6notdone_2 = True\n",
    "b6notdone_3 = True\n",
    "nonnotdone_2 = True\n",
    "\n",
    "for i in range(len(dlist)):\n",
    "    dtouse = dlist[i]\n",
    "    dtitle = dtitles[i]\n",
    "    # dpos = dpositions[i]\n",
    "    # axid = axids[i]\n",
    "\n",
    "    ANT_obj = dtouse['object']['ANT']\n",
    "    ANT_trace = dtouse['trace']['ANT']\n",
    "    ANT_unassigned = dtouse['unassigned']['ANT']\n",
    "\n",
    "    prev_ANT_obj_per = len(ANT_obj) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    sum_ANT = ANT_cell_type_df[ANT_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_ANT = sum_ANT.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_ANT = len(sum_ANT)\n",
    "    ANT_obj_per = len(ANT_obj) / sum_ANT\n",
    "    print(ANT_obj_per, prev_ANT_obj_per)\n",
    "    ANT_trace_per = len(ANT_trace) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    ANT_unassigned_per = 1 - ANT_obj_per - ANT_trace_per\n",
    "\n",
    "    B6_obj = dtouse['object']['B6']\n",
    "    B6_trace = dtouse['trace']['B6']\n",
    "    B6_unassigned = dtouse['unassigned']['B6']\n",
    "\n",
    "    # B6_obj_per = len(B6_obj) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    sum_B6 = B6_cell_type_df[B6_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_B6 = sum_B6.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_B6 = len(sum_B6)\n",
    "    B6_obj_per = len(B6_obj) / sum_B6\n",
    "    B6_trace_per = len(B6_trace) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    # B6_unassigned_per = len(B6_unassigned) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    B6_unassigned_per = 1 - B6_obj_per - B6_trace_per\n",
    "\n",
    "    NON_obj = dtouse['object']['NON']\n",
    "    NON_trace = dtouse['trace']['NON']\n",
    "    NON_unassigned = dtouse['unassigned']['NON']\n",
    "\n",
    "    # NON_obj_per = len(NON_obj) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    sum_NON = NON_cell_type_df[NON_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_NON = sum_NON.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_NON = len(sum_NON)\n",
    "    NON_obj_per = len(NON_obj) / sum_NON\n",
    "    NON_trace_per = len(NON_trace) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    # NON_unassigned_per = len(NON_unassigned) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    NON_unassigned_per = 1 - NON_obj_per - NON_trace_per\n",
    "\n",
    "    # obj_per = np.array([ANT_obj_per, B6_obj_per, NON_obj_per]) * 100\n",
    "    obj_per = np.array([B6_obj_per, NON_obj_per, ANT_obj_per]) * 100\n",
    "    # trace_per = np.array([ANT_trace_per, B6_trace_per, NON_trace_per]) * 100\n",
    "    trace_per = np.array([B6_trace_per, NON_trace_per, ANT_trace_per]) * 100\n",
    "    # unassigned_per = np.array([ANT_unassigned_per, B6_unassigned_per, NON_unassigned_per]) * 100\n",
    "    unassigned_per = np.array([B6_unassigned_per, NON_unassigned_per, ANT_unassigned_per]) * 100\n",
    "\n",
    "    unassigned_per = 100 - unassigned_per\n",
    "\n",
    "\n",
    "    # top left\n",
    "    ax1 = plt.subplot(gspec[0,0])\n",
    "    ax1.bar(['B6', 'NON', 'ANT'], obj_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax1.bar(['B6', 'NON', 'ANT'], obj_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'OBJECT' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax1.plot(['B6', 'NON', 'ANT'], obj_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    # annotate\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_1:\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_1 = False\n",
    "\n",
    "\n",
    "    ax1.set_title('Object')\n",
    "    ax1.set_ylabel('% unique cells')\n",
    "\n",
    "    # top middle\n",
    "    ax2 = plt.subplot(gspec[0,1])\n",
    "    ax2.bar(['B6', 'NON', 'ANT'], trace_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax2.bar(['B6', 'NON', 'ANT'], trace_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'TRACE' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax2.plot(['B6', 'NON', 'ANT'], trace_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_2:\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        # elif lbl == 'NON' and nonnotdone_2 and i != 0:\n",
    "        #     ax2.annotate('N=4,5,6', xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,15), ha='center')\n",
    "        # elif lbl == 'ANT' or (lbl == 'NON' and i == 0):\n",
    "        elif lbl != 'B6':\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_2 = False\n",
    "        if lbl == 'NON' and i != 0:\n",
    "            nonnotdone_2 = False\n",
    "    ax2.set_title('Trace')\n",
    "\n",
    "    # top right\n",
    "    ax3 = plt.subplot(gspec[0,2])\n",
    "    ax3.bar(['B6', 'NON', 'ANT'], unassigned_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax3.bar(['B6', 'NON', 'ANT'], unassigned_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "    ax3.plot(['B6', 'NON', 'ANT'], unassigned_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax3.set_title('Assigned')\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_3:\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_3 = False\n",
    "    ax3.set_ylim([0,100])\n",
    "\n",
    "\n",
    "    ax6 = plt.subplot(gspec[1,2])\n",
    "    per1 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] +  ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['NON'])\n",
    "    ambiguous_pers = np.array( [per2, per3, per1])*100\n",
    "    ax6.plot(['B6', 'NON', 'ANT'], ambiguous_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax6.set_title('Between 2 angles')\n",
    "    ax6.bar(['B6', 'NON', 'ANT'], ambiguous_pers, color=['blue', 'green', 'red'], alpha=0.2)\n",
    "    if i == 0:\n",
    "        ax6.bar(['B6', 'NON', 'ANT'], ambiguous_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    print(ambiguous_pers)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = dflist[2]['Comparison']\n",
    "        accepted = dflist[2]['Accepted']\n",
    "        adjusted = dflist[2]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], ambiguous_pers, maxasterix=5)\n",
    "    # for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "    #     if lbl == 'B6' and i == 0:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "    #     elif lbl != 'B6' and i == 0 or lbl == 'B6' and i == 3:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "\n",
    "\n",
    "\n",
    "    ax5 = plt.subplot(gspec[1,1])\n",
    "    per1 = ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['center_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['center_type_dict']['B6'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['center_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['center_type_dict']['NON'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['NON'])\n",
    "    center_pers = np.array( [per2, per3, per1])*100\n",
    "    ax5.plot(['B6', 'NON', 'ANT'], center_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax5.set_title('Closer to center')\n",
    "    ax5.bar(['B6', 'NON', 'ANT'], center_pers, color=['blue', 'green', 'red'], alpha = .2)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        ax5.bar(['B6', 'NON', 'ANT'], center_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "\n",
    "    if i == len(dlist)-1:\n",
    "        comps = dflist[1]['Comparison']\n",
    "        accepted = dflist[1]['Accepted']\n",
    "        adjusted = dflist[1]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], center_pers, maxasterix=5)\n",
    "\n",
    "    ax4 = plt.subplot(gspec[1,0])\n",
    "    per1 = ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['NON'])\n",
    "    quality_pers = np.array( [per2, per3, per1])*100\n",
    "    ax4.plot(['B6', 'NON', 'ANT'], quality_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax4.set_title('Low quality')\n",
    "    ax4.bar(['B6', 'NON', 'ANT'], quality_pers, color=['blue', 'green', 'red'], alpha = .2)\n",
    "\n",
    "    if i == 0:\n",
    "        ax4.bar(['B6', 'NON', 'ANT'], quality_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    if i == len(dlist)-1:   \n",
    "        comps = dflist[0]['Comparison']\n",
    "        accepted = dflist[0]['Accepted']\n",
    "        adjusted = dflist[0]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], quality_pers, maxasterix=5)\n",
    "\n",
    "    ax4.set_ylabel('% of cell-session appearances')\n",
    "\n",
    "\n",
    "    fig.suptitle(\"Fisher's exact test, capped at first 3 session\")\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "# ses1234_identifier_dict, ses12345_identifier_dict, ses123456_identifier_dict]\n",
    "dtitles = ['Session 1 to 3']\n",
    "#  'Session 1 to 4', 'Session 1 to 5', 'Session 1 to 6']\n",
    "# dpositions = [0.93, 0.68, 0.33, 0.03]\n",
    "# axids = [[[0,0],[0,1],[0,2]], [[1,0],[1,1],[1,2]], [[2,0],[2,1],[2,2]], [[3,0],[3,1],[3,2]]]\n",
    "lbls = ['N=3']\n",
    "#  'N=4', 'N=5', 'N=6']\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "# gspec 3x3\n",
    "gspec = gridspec.GridSpec(2,3)\n",
    "\n",
    "b6notdone_1 = True\n",
    "b6notdone_2 = True\n",
    "b6notdone_3 = True\n",
    "nonnotdone_2 = True\n",
    "\n",
    "for i in range(1):\n",
    "    dtouse = dlist[i]\n",
    "    dtitle = dtitles[i]\n",
    "    # dpos = dpositions[i]\n",
    "    # axid = axids[i]\n",
    "\n",
    "    ANT_obj = dtouse['object']['ANT']\n",
    "    ANT_trace = dtouse['trace']['ANT']\n",
    "    ANT_unassigned = dtouse['unassigned']['ANT']\n",
    "\n",
    "    prev_ANT_obj_per = len(ANT_obj) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    sum_ANT = ANT_cell_type_df[ANT_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_ANT = sum_ANT.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_ANT = len(sum_ANT)\n",
    "    ANT_obj_per = len(ANT_obj) / sum_ANT\n",
    "    print(ANT_obj_per, prev_ANT_obj_per)\n",
    "    ANT_trace_per = len(ANT_trace) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    ANT_unassigned_per = 1 - ANT_obj_per - ANT_trace_per\n",
    "\n",
    "    B6_obj = dtouse['object']['B6']\n",
    "    B6_trace = dtouse['trace']['B6']\n",
    "    B6_unassigned = dtouse['unassigned']['B6']\n",
    "\n",
    "    # B6_obj_per = len(B6_obj) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    sum_B6 = B6_cell_type_df[B6_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_B6 = sum_B6.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_B6 = len(sum_B6)\n",
    "    B6_obj_per = len(B6_obj) / sum_B6\n",
    "    B6_trace_per = len(B6_trace) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    # B6_unassigned_per = len(B6_unassigned) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    B6_unassigned_per = 1 - B6_obj_per - B6_trace_per\n",
    "\n",
    "    NON_obj = dtouse['object']['NON']\n",
    "    NON_trace = dtouse['trace']['NON']\n",
    "    NON_unassigned = dtouse['unassigned']['NON']\n",
    "\n",
    "    # NON_obj_per = len(NON_obj) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    sum_NON = NON_cell_type_df[NON_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_NON = sum_NON.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_NON = len(sum_NON)\n",
    "    NON_obj_per = len(NON_obj) / sum_NON\n",
    "    NON_trace_per = len(NON_trace) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    # NON_unassigned_per = len(NON_unassigned) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    NON_unassigned_per = 1 - NON_obj_per - NON_trace_per\n",
    "\n",
    "    # obj_per = np.array([ANT_obj_per, B6_obj_per, NON_obj_per]) * 100\n",
    "    obj_per = np.array([B6_obj_per, NON_obj_per, ANT_obj_per]) * 100\n",
    "    # trace_per = np.array([ANT_trace_per, B6_trace_per, NON_trace_per]) * 100\n",
    "    trace_per = np.array([B6_trace_per, NON_trace_per, ANT_trace_per]) * 100\n",
    "    # unassigned_per = np.array([ANT_unassigned_per, B6_unassigned_per, NON_unassigned_per]) * 100\n",
    "    unassigned_per = np.array([B6_unassigned_per, NON_unassigned_per, ANT_unassigned_per]) * 100\n",
    "\n",
    "    # unassigned_per = 100 - unassigned_per\n",
    "    unassigned_per = unassigned_per\n",
    "\n",
    "\n",
    "    # top left\n",
    "    ax1 = plt.subplot(gspec[0,0])\n",
    "    ax1.bar(['B6', 'NON', 'ANT'], obj_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax1.bar(['B6', 'NON', 'ANT'], obj_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    # if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'OBJECT' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax1.plot(['B6', 'NON', 'ANT'], obj_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    # annotate\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_1:\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_1 = False\n",
    "\n",
    "\n",
    "    ax1.set_title('Object')\n",
    "    ax1.set_ylabel('% unique cells')\n",
    "\n",
    "    # top middle\n",
    "    ax2 = plt.subplot(gspec[0,1])\n",
    "    ax2.bar(['B6', 'NON', 'ANT'], trace_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax2.bar(['B6', 'NON', 'ANT'], trace_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    # if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'TRACE' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax2.plot(['B6', 'NON', 'ANT'], trace_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_2:\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        # elif lbl == 'NON' and nonnotdone_2 and i != 0:\n",
    "        #     ax2.annotate('N=4,5,6', xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,15), ha='center')\n",
    "        # elif lbl == 'ANT' or (lbl == 'NON' and i == 0):\n",
    "        elif lbl != 'B6':\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_2 = False\n",
    "        if lbl == 'NON' and i != 0:\n",
    "            nonnotdone_2 = False\n",
    "    ax2.set_title('Trace')\n",
    "\n",
    "    # top right\n",
    "    ax3 = plt.subplot(gspec[0,2])\n",
    "    ax3.bar(['B6', 'NON', 'ANT'], unassigned_per, color=['blue', 'green', 'red'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax3.bar(['B6', 'NON', 'ANT'], unassigned_per, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    # if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "    ax3.plot(['B6', 'NON', 'ANT'], unassigned_per, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax3.set_title('Unassigned')\n",
    "    for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "        if lbl == 'B6' and b6notdone_3:\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_3 = False\n",
    "    ax3.set_ylim([0,100])\n",
    "\n",
    "\n",
    "    ax6 = plt.subplot(gspec[1,2])\n",
    "    per1 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] +  ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['NON'])\n",
    "    ambiguous_pers = np.array( [per2, per3, per1])*100\n",
    "    ax6.plot(['B6', 'NON', 'ANT'], ambiguous_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax6.set_title('Between 2 angles')\n",
    "    ax6.bar(['B6', 'NON', 'ANT'], ambiguous_pers, color=['blue', 'green', 'red'], alpha=0.2)\n",
    "    if i == 0:\n",
    "        ax6.bar(['B6', 'NON', 'ANT'], ambiguous_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    # print(ambiguous_pers)\n",
    "    # if i == len(dlist)-1:\n",
    "        comps = dflist[2]['Comparison']\n",
    "        accepted = dflist[2]['Accepted']\n",
    "        adjusted = dflist[2]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], ambiguous_pers, maxasterix=5)\n",
    "    # for j, lbl in enumerate(['B6', 'NON', 'ANT']):\n",
    "    #     if lbl == 'B6' and i == 0:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "    #     elif lbl != 'B6' and i == 0 or lbl == 'B6' and i == 3:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "\n",
    "\n",
    "\n",
    "    ax5 = plt.subplot(gspec[1,1])\n",
    "    per1 = ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['center_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['center_type_dict']['B6'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['center_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['center_type_dict']['NON'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['NON'])\n",
    "    center_pers = np.array( [per2, per3, per1])*100\n",
    "    ax5.plot(['B6', 'NON', 'ANT'], center_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax5.set_title('Closer to center')\n",
    "    ax5.bar(['B6', 'NON', 'ANT'], center_pers, color=['blue', 'green', 'red'], alpha = .2)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        ax5.bar(['B6', 'NON', 'ANT'], center_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "\n",
    "    # if i == len(dlist)-1:\n",
    "        comps = dflist[1]['Comparison']\n",
    "        accepted = dflist[1]['Accepted']\n",
    "        adjusted = dflist[1]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], center_pers, maxasterix=5)\n",
    "\n",
    "    ax4 = plt.subplot(gspec[1,0])\n",
    "    per1 = ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['NON'])\n",
    "    quality_pers = np.array( [per2, per3, per1])*100\n",
    "    ax4.plot(['B6', 'NON', 'ANT'], quality_pers, 'k-', marker='o', alpha=0.5, lw=0)\n",
    "    ax4.set_title('Low quality')\n",
    "    ax4.bar(['B6', 'NON', 'ANT'], quality_pers, color=['blue', 'green', 'red'], alpha = .2)\n",
    "\n",
    "    if i == 0:\n",
    "        ax4.bar(['B6', 'NON', 'ANT'], quality_pers, color=['blue', 'green', 'red'], alpha = 1)\n",
    "    # if i == len(dlist)-1:   \n",
    "        comps = dflist[0]['Comparison']\n",
    "        accepted = dflist[0]['Accepted']\n",
    "        adjusted = dflist[0]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,1]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], quality_pers, maxasterix=5)\n",
    "\n",
    "    ax4.set_ylabel('% of cell-session appearances')\n",
    "    \n",
    "    ax1.set_ylim(0,100)\n",
    "    ax2.set_ylim(0,100)\n",
    "    ax3.set_ylim(0,100)\n",
    "    ax4.set_ylim(0,100)\n",
    "    ax5.set_ylim(0,100)\n",
    "    ax6.set_ylim(0,100)\n",
    "\n",
    "    fig.suptitle(\"Fisher's exact test, capped at first 3 session\")\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosave = dlist[0]\n",
    "import pickle\n",
    "# save the dictionary\n",
    "with open('ses123_identifier_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tosave, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = None\n",
    "for ky in tosave['trace']:\n",
    "    for idd in tosave['trace'][ky]:\n",
    "        id1, id2, id3, id4, id5, id6 = idd\n",
    "        # print(id1, id2, id3, id4, id5, id6)\n",
    "        # mask = pd.concat([(df2['group'] == id1) & (df2['name'] == id2)?\n",
    "        mask = pd.concat([(df2['group'] == id1) & (df2['name'] == id2) & (df2['depth'] == id3) & (df2['date'] == id4) & (df2['tetrode'] == id5) & (df2['unit_id'] == id6)], axis=1).any(axis=1)\n",
    "        if prev_df is None:\n",
    "            new_df = df2[mask]\n",
    "        else:\n",
    "            new_df = pd.concat([prev_df, df2[mask]])\n",
    "        prev_df = new_df\n",
    "\n",
    "prev_df.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_trace_cells.xlsx')\n",
    "\n",
    "prev_df = None\n",
    "for ky in tosave['object']:\n",
    "    for idd in tosave['object'][ky]:\n",
    "        id1, id2, id3, id4, id5, id6 = idd\n",
    "        # mask = pd.concat([(df2['group'] == id1) & (df2['name'] == id2)?\n",
    "        mask = pd.concat([(df2['group'] == id1) & (df2['name'] == id2) & (df2['depth'] == id3) & (df2['date'] == id4) & (df2['tetrode'] == id5) & (df2['unit_id'] == id6)], axis=1).any(axis=1)\n",
    "        if prev_df is None:\n",
    "            new_df = df2[mask]\n",
    "        else:\n",
    "            new_df = pd.concat([prev_df, df2[mask]])\n",
    "        prev_df = new_df\n",
    "\n",
    "prev_df.to_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\df_full_LEC_only_object_cells.xlsx')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tosave['unassigned']['ANT']) + len(tosave['unassigned']['B6']) + len(tosave['unassigned']['NON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "126+53+41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
