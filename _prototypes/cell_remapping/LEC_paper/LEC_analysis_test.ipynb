{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "\n",
    "PROJECT_PATH = os.getcwd()\n",
    "sys.path.append(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANT_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\ANT\"\n",
    "# NON_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\NON\"\n",
    "# B6_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\B6\"\n",
    "# dfs = []\n",
    "# for path in [ANT_path, NON_path, B6_path]:\n",
    "#     for animal_dir in os.listdir(path):\n",
    "#         for csv_file in os.listdir(path + r'\\\\' + animal_dir):\n",
    "#             if csv_file.endswith('.xlsx'):\n",
    "#                 file_df = pd.read_excel(path + r'\\\\' + animal_dir + r'\\\\' + csv_file)\n",
    "#                 # add column for genotype\n",
    "#                 if path == ANT_path:\n",
    "#                     file_df['group'] = 'ANT'\n",
    "#                 elif path == NON_path:\n",
    "#                     file_df['group'] = 'NON'\n",
    "#                 elif path == B6_path:\n",
    "#                     file_df['group'] = 'B6'\n",
    "#                 dfs.append(file_df)\n",
    "\n",
    "# df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = pd.read_excel(r'C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\LEC_full_merged_scores.xlsx')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" QUALITY CHECK DATA \"\"\" \n",
    "nan_idx = np.where(df['obj_q_0'].isna())[0]\n",
    "not_nan_idx = np.where(~df['obj_q_0'].isna())[0]\n",
    "nan_dates = (df['date'][nan_idx].unique())\n",
    "nan_names = (df['name'][nan_idx].unique())\n",
    "\n",
    "print('Number of NaN rows: ' + str(len(nan_idx)))\n",
    "print('Animals with NaN rows: ' + str(nan_names))\n",
    "print('Dates with NaN rows: ' + str(nan_dates))\n",
    "\n",
    "# remove rows with NaN values\n",
    "print('Removing nan rows')\n",
    "df = df.iloc[not_nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" OBJECT CELL DETECTION \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Definition:\n",
    "\n",
    "Cell with a field that is at the object location \n",
    "\n",
    "Criteria:\n",
    "    - Quantile < threshold\n",
    "    - Nmb of sessions detected\n",
    "    - Consecutive vs non consecutive\n",
    "    - Score to use (field or centroid)\n",
    "    - Main field or all fields\n",
    "\"\"\"\n",
    "consecutive_sessions_threshold = 2\n",
    "quantile_threshold = 0.2\n",
    "consecutive = False\n",
    "score = 'field'\n",
    "main_field_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FILTERING \"\"\"\n",
    "\n",
    "\"\"\" REMOVE FIELD WITH LOW COVERAGE % \"\"\"\n",
    "# remove rows with field_coverage < 0.1\n",
    "# df = df[df['field_coverage'] >= 0.1]\n",
    "\n",
    "\"\"\" ONLY KEEPING MAIN FIELD \"\"\"\n",
    "# remove rows where field_id is not 1 and score is not 'whole' or 'spike_density'\n",
    "# df = df[(df['field_id'] == 1) | (df['score'] == 'whole') | (df['score'] == 'spike_density')]\n",
    "\n",
    "\"\"\" CHOOSING ANGLE FOR EACH ROW \"\"\"\n",
    "# for each row, choose lowest quantile from ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "df['obj_q'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].min(axis=1)\n",
    "df['obj_a'] = df[['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']].idxmin(axis=1)\n",
    "# convert obj_a to degrees\n",
    "df['obj_a'] = df['obj_a'].apply(lambda x: int(x.split('_')[2]))\n",
    "# use obj_wass with angle of min quantile\n",
    "df['obj_w'] = df.apply(lambda x: x['obj_wass_' + str(x['obj_a'])], axis=1)\n",
    "\n",
    "\"\"\" ASSESSING SIG FOR EACH ROW AT EACH ANGLE \"\"\"\n",
    "obj_s_rows = ['obj_s_0', 'obj_s_90', 'obj_s_180', 'obj_s_270']\n",
    "obj_q_rows = ['obj_q_0', 'obj_q_90', 'obj_q_180', 'obj_q_270']\n",
    "for i in range(len(obj_s_rows)):\n",
    "    obj_q_x = obj_q_rows[i]\n",
    "    df[obj_s_rows[i]] = df[obj_q_x].apply(lambda x: 1 if x < quantile_threshold else 0)\n",
    "\n",
    "\n",
    "# df2 = df[df['score'] == 'whole'].copy()\n",
    "df2 = df.copy()\n",
    "# group_by_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "# df2 = df2.groupby(group_by_cell).mean().reset_index()\n",
    "cts = df2[df2['spike_count'] > 30000]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count upper of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(30000 , str(len(df2[df2['spike_count'] > 30000])), \n",
    "                       cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_count'] < 100]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike count lower of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(100 , str(len(df2[df2['spike_count'] < 100])),\n",
    "                          cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['information'] < 0.25]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spatial info of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.25 , str(len(df2[df2['information'] < 0.25])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['selectivity'] < 5]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Selectivity of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['selectivity'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['iso_dist'] < 5]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Isolation distance of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(5 , str(len(df2[df2['iso_dist'] < 5])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['firing_rate'] > 20]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Firing rate of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(20 , str(len(df2[df2['firing_rate'] > 20])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "cts = df2[df2['spike_width'] < 0.00005]['group'].value_counts()\n",
    "for nm in ['ANT', 'B6', 'NON']:\n",
    "    if nm not in cts:\n",
    "        cts[nm] = 0\n",
    "print('Spike width of {} would drop {} cells including {} ANT, {} B6 and {} NON'.format(0.00005 , str(len(df2[df2['spike_width'] < 0.00005])),\n",
    "                            cts['ANT'], cts['B6'], cts['NON']))\n",
    "\n",
    "# drop spike count column \n",
    "df2 = df2.drop(columns=['spike_count'])\n",
    "# rename spike_count.1 to spike_count\n",
    "df2 = df2.rename(columns={'spike_count.1': 'spike_count'})\n",
    "\n",
    "# df2 = df2[df2['spike_count'] < 30000]\n",
    "# df2 = df2[df2['spike_count'] > 100]\n",
    "# df2 = df2[df2['information'] > 0.25]\n",
    "# df2 = df2[df2['selectivity'] > 5]\n",
    "# df2 = df2[df2['iso_dist'] > 5]\n",
    "# df2 = df2[df2['firing_rate'] < 20]\n",
    "# df2 = df2[df2['spike_width'] > 0.00005]\n",
    "\n",
    "print('Remaining cells: ' + str(len(df2)) + ' of which ' + str(len(df2[df2['group'] == 'ANT'])) + ' ANT, ' + str(len(df2[df2['group'] == 'B6'])) + ' B6 and ' + str(len(df2[df2['group'] == 'NON'])) + ' NON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CHECK AGREEMENT OF CENTROID VS FIELD DISTANCE SCORES \"\"\"\n",
    "\n",
    "scores_to_compare = ['centroid', 'field']\n",
    "df_use = df[df['session_id'].isin(['session_1','session_2','session_3'])]\n",
    "# 'session_4','session_5','session_6'])]   \n",
    "main_centroid_df = df_use[(df_use['score'] == 'centroid') & (df_use['field_id'] == 1)]\n",
    "main_field_df = df_use[(df_use['score'] == 'field') & (df_use['field_id'] == 1)]\n",
    "all_field_df = df_use[df_use['score'] == 'field']\n",
    "all_centroid_df = df_use[df_use['score'] == 'centroid']\n",
    "\n",
    "\n",
    "assert len(main_centroid_df) == len(main_field_df)\n",
    "assert len(all_field_df) == len(all_centroid_df)\n",
    "\n",
    "matched = []\n",
    "unmatched = []\n",
    "all_matched = []\n",
    "all_unmatched = []\n",
    "all_ambig = []\n",
    "all_unambig = []\n",
    "all_diffs = []\n",
    "for i in range(len(all_field_df)):\n",
    "    all_field_obj_a = all_field_df.iloc[i]['obj_a']\n",
    "    all_centroid_obj_a = all_centroid_df.iloc[i]['obj_a']\n",
    "\n",
    "    obj_qs = [all_field_df.iloc[i]['obj_q_0'], all_field_df.iloc[i]['obj_q_90'], all_field_df.iloc[i]['obj_q_180'], all_field_df.iloc[i]['obj_q_270']]\n",
    "    sorted_obj_qs = np.sort(obj_qs)\n",
    "    min1 = sorted_obj_qs[0]\n",
    "    min2 = sorted_obj_qs[1]\n",
    "    all_diffs.append(abs(min1 - min2))\n",
    "\n",
    "    if abs(min1 - min2) < 0.05:\n",
    "        all_ambig.append(i)\n",
    "    else:\n",
    "        all_unambig.append(i)\n",
    "\n",
    "    if all_field_obj_a == all_centroid_obj_a:\n",
    "        all_matched.append(i)\n",
    "    else:\n",
    "        all_unmatched.append(i)\n",
    "for i in range(len(main_field_df)):\n",
    "    field_obj_a = main_field_df.iloc[i]['obj_a']\n",
    "    centroid_obj_a = main_centroid_df.iloc[i]['obj_a']\n",
    "    if field_obj_a == centroid_obj_a:\n",
    "        matched.append(i)\n",
    "    else:\n",
    "        unmatched.append(i)\n",
    "df_matched_field = main_field_df.iloc[matched]\n",
    "df_all_matched_field = all_field_df.iloc[all_matched]\n",
    "df_all_ambiguous_field = all_field_df.iloc[all_ambig]\n",
    "df_unmatched_field = main_field_df.iloc[unmatched]\n",
    "df_all_unmatched_field = all_field_df.iloc[all_unmatched]\n",
    "df_all_unambiguous_field = all_field_df.iloc[all_unambig]\n",
    "df_matched_centroid = main_centroid_df.iloc[matched]\n",
    "df_all_matched_centroid = all_centroid_df.iloc[all_matched]\n",
    "df_unmatched_centroid = main_centroid_df.iloc[unmatched]\n",
    "df_all_unmatched_centroid = all_centroid_df.iloc[all_unmatched]\n",
    "assert len(df_matched_field) == len(df_matched_centroid)\n",
    "assert len(df_all_matched_field) == len(df_all_matched_centroid)\n",
    "assert len(df_unmatched_field) == len(df_unmatched_centroid)\n",
    "assert len(df_all_unmatched_field) == len(df_all_unmatched_centroid)\n",
    "print('There are {} rows where main field distance and centroid distance are in the same direction'.format(len(df_matched_field)))\n",
    "print('There are {} rows where main field distance and centroid distance are in different directions'.format(len(df_unmatched_field)))\n",
    "print('There are {} rows where all field distance and all centroid distance are in the same direction'.format(len(df_all_matched_field)))\n",
    "print('There are {} rows where all field distance and all centroid distance are in different directions'.format(len(df_all_unmatched_field)))\n",
    "\n",
    "ANT_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'ANT']) / (len(df_matched_field[df_matched_field['group'] == 'ANT']) + len(df_unmatched_field[df_unmatched_field['group'] == 'ANT'])))\n",
    "ANT_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'ANT']) / (len(df_matched_field[df_matched_field['group'] == 'ANT']) + len(df_unmatched_field[df_unmatched_field['group'] == 'ANT'])))\n",
    "B6_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'B6']) / (len(df_matched_field[df_matched_field['group'] == 'B6']) + len(df_unmatched_field[df_unmatched_field['group'] == 'B6'])))\n",
    "B6_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'B6']) / (len(df_matched_field[df_matched_field['group'] == 'B6']) + len(df_unmatched_field[df_unmatched_field['group'] == 'B6'])))\n",
    "NON_same_dir= float(len(df_matched_field[df_matched_field['group'] == 'NON']) / (len(df_matched_field[df_matched_field['group'] == 'NON']) + len(df_unmatched_field[df_unmatched_field['group'] == 'NON'])))\n",
    "NON_diff_dir= float(len(df_unmatched_field[df_unmatched_field['group'] == 'NON']) / (len(df_matched_field[df_matched_field['group'] == 'NON']) + len(df_unmatched_field[df_unmatched_field['group'] == 'NON'])))\n",
    "\n",
    "print('There are {} rows where main field distance and centroid distance are in the same direction for ANT'.format(ANT_same_dir))\n",
    "print('There are {} rows where main field distance and centroid distance are in different directions for ANT'.format(ANT_diff_dir))\n",
    "print('There are {} rows where main field distance and centroid distance are in the same direction for B6'.format(B6_same_dir))\n",
    "print('There are {} rows where main field distance and centroid distance are in different directions for B6'.format(B6_diff_dir))\n",
    "print('There are {} rows where main field distance and centroid distance are in the same direction for NON'.format(NON_same_dir))\n",
    "print('There are {} rows where main field distance and centroid distance are in different directions for NON'.format(NON_diff_dir))\n",
    "\n",
    "ANT_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT'])))\n",
    "ANT_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'ANT']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT'])))\n",
    "B6_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6'])))\n",
    "B6_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'B6']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6'])))\n",
    "NON_same_dir_all= float(len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON'])))\n",
    "NON_diff_dir_all= float(len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON']) / (len(df_all_matched_field[df_all_matched_field['group'] == 'NON']) + len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON'])))\n",
    "\n",
    "print('There are {} rows where all field distance and all centroid distance are in the same direction for ANT'.format(ANT_same_dir_all))\n",
    "print('There are {} rows where all field distance and all centroid distance are in different directions for ANT'.format(ANT_diff_dir_all))\n",
    "print('There are {} rows where all field distance and all centroid distance are in the same direction for B6'.format(B6_same_dir_all))\n",
    "print('There are {} rows where all field distance and all centroid distance are in different directions for B6'.format(B6_diff_dir_all))\n",
    "print('There are {} rows where all field distance and all centroid distance are in the same direction for NON'.format(NON_same_dir_all))\n",
    "print('There are {} rows where all field distance and all centroid distance are in different directions for NON'.format(NON_diff_dir_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_c = 0 \n",
    "diff_c = 0\n",
    "for l in all_ambig:\n",
    "    isMatched = False\n",
    "    for l2 in all_unmatched:\n",
    "        if l == l2:\n",
    "            isMatched = True\n",
    "    if isMatched:\n",
    "        same_c += 1\n",
    "    else:\n",
    "        diff_c += 1\n",
    "\n",
    "print(same_c, diff_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_diffs, np.arange(0, 1, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop when field and centroid are in different directions\n",
    "# df = df_all_matched_field.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_unambig), len(all_ambig))\n",
    "print(len(all_matched), len(all_unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_unique_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "\n",
    "object_cell_df = df[df['score'] == score]\n",
    "object_cell_df = object_cell_df[object_cell_df['session_id'].isin(['session_1','session_2','session_3'])]   \n",
    "# 'session_4','session_5','session_6'])]\n",
    "\n",
    "group_by_unique_cell_field = ['group', 'name', 'depth', 'date','tetrode', 'unit_id', 'field_id']\n",
    "\n",
    "ANT_df = object_cell_df[object_cell_df['group'] == 'ANT'].reset_index(drop=True)\n",
    "B6_df = object_cell_df[object_cell_df['group'] == 'B6'].reset_index(drop=True)\n",
    "NON_df = object_cell_df[object_cell_df['group'] == 'NON'].reset_index(drop=True)\n",
    "\n",
    "ANT_object_cell_df = ANT_df.copy() \n",
    "B6_object_cell_df = B6_df.copy() \n",
    "NON_object_cell_df = NON_df.copy() \n",
    "ANT_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "B6_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "NON_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "ANT_object_cell_df['cell_type'] = 'unassigned'\n",
    "B6_object_cell_df['cell_type'] = 'unassigned'\n",
    "NON_object_cell_df['cell_type'] = 'unassigned'\n",
    "ANT_object_cell_df['isTrace'] = 0\n",
    "B6_object_cell_df['isTrace'] = 0\n",
    "NON_object_cell_df['isTrace'] = 0\n",
    "ANT_object_cell_df['isObject'] = 0\n",
    "B6_object_cell_df['isObject'] = 0\n",
    "NON_object_cell_df['isObject'] = 0\n",
    "ANT_object_cell_df['trace_a'] = None\n",
    "B6_object_cell_df['trace_a'] = None\n",
    "NON_object_cell_df['trace_a'] = None\n",
    "\n",
    "\n",
    "ANT_object_cell_df.loc[ANT_object_cell_df['obj_a'].astype(str)  == ANT_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "B6_object_cell_df.loc[B6_object_cell_df['obj_a'].astype(str)  == B6_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "NON_object_cell_df.loc[NON_object_cell_df['obj_a'].astype(str)  == NON_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "\n",
    "keep_trace_appearances = []\n",
    "for df_touse in [ANT_object_cell_df, B6_object_cell_df, NON_object_cell_df]:\n",
    "    prev_angles = []\n",
    "    prev_unit_id = None\n",
    "    prev_field_id = None\n",
    "    prev_tetrode = None\n",
    "    prev_name = None\n",
    "    prev_date = None\n",
    "    prev_depth = None\n",
    "    to_keep_trace_appearance = []\n",
    "    for i, row in df_touse.iterrows():\n",
    "        curr_unit_id = row['unit_id']\n",
    "        curr_tetrode = row['tetrode']\n",
    "        curr_field_id = row['field_id']\n",
    "        curr_angle = row['obj_a']\n",
    "        curr_name = row['name']\n",
    "        curr_date = row['date']\n",
    "        curr_depth = row['depth']\n",
    "\n",
    "        if curr_unit_id == prev_unit_id and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "            if str(curr_angle) in prev_angles:\n",
    "                to_keep_trace_appearance.append(i)\n",
    "        else:\n",
    "            prev_angles = []\n",
    "\n",
    "        prev_unit_id = curr_unit_id\n",
    "        prev_angle = curr_angle\n",
    "        prev_field_id = curr_field_id\n",
    "        prev_tetrode = curr_tetrode\n",
    "        prev_name = curr_name\n",
    "        prev_date = curr_date\n",
    "        prev_depth = curr_depth\n",
    "        prev_angles.append(row['object_location'])\n",
    "\n",
    "    keep_trace_appearances.append(to_keep_trace_appearance)\n",
    "\n",
    "ANT_object_cell_df.loc[keep_trace_appearances[0],'isTrace'] = 1\n",
    "B6_object_cell_df.loc[keep_trace_appearances[1],'isTrace'] = 1\n",
    "NON_object_cell_df.loc[keep_trace_appearances[2],'isTrace'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "group_by_unique_cell_session = ['group', 'name', 'depth', 'date', 'tetrode', 'unit_id','session_id'] # (joins field rowsfrom that ses)\n",
    "\n",
    "ses_cut_dict = {}\n",
    "\n",
    "for ses_cut in [3,4,5,6]:\n",
    "\n",
    "    center_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}\n",
    "\n",
    "    center_inv_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}\n",
    "\n",
    "    quality_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}   \n",
    "\n",
    "    quality_inv_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}           \n",
    "\n",
    "    ambiguous_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}       \n",
    "\n",
    "    ambiguous_inv_type_dict = {'ANT': 0,\n",
    "                    'B6':0,\n",
    "                    'NON': 0}\n",
    "\n",
    "    ses_cut_dict[ses_cut] = {}\n",
    "    ANT_object_cell_df_to_use = ANT_object_cell_df[ANT_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    B6_object_cell_df_to_use = B6_object_cell_df[B6_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    NON_object_cell_df_to_use = NON_object_cell_df[NON_object_cell_df['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    df_all_unmatched_field_to_use = df_all_unmatched_field[df_all_unmatched_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    df_all_matched_field_to_use = df_all_matched_field[df_all_matched_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    # df_all_unmatched_field_to_use = df_all_ambiguous_field[df_all_ambiguous_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    # df_all_matched_field_to_use = df_all_unambiguous_field[df_all_unambiguous_field['session_id'].isin(['session_'+str(i) for i in range(1,ses_cut+1)])]\n",
    "    c = 0\n",
    "    for df_current in [ANT_object_cell_df_to_use, B6_object_cell_df_to_use, NON_object_cell_df_to_use]:\n",
    "        quality_dropped_identifiers = df_current[df_current['iso_dist'] < 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        quality_non_dropped_identifiers = df_current[df_current['iso_dist'] >= 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        center_dropped_identifiers = df_current[df_current['obj_q_NO'] < df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        center_non_dropped_identifiers = df_current[df_current['obj_q_NO'] >= df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        ambiguous_dropped_identifiers = df_all_unmatched_field_to_use[df_all_unmatched_field_to_use['group'] == ['ANT','B6','NON'][c]].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        ambiguous_non_dropped_identifiers = df_all_matched_field_to_use[df_all_matched_field_to_use['group'] == ['ANT','B6','NON'][c]].groupby(group_by_unique_cell_session).groups.keys()\n",
    "        # mask_dropped = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in ambiguous_dropped_identifiers], axis=1).any(axis=1)\n",
    "        # mask_non_dropped = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in ambiguous_non_dropped_identifiers], axis=1).any(axis=1)\n",
    "        if c == 0:\n",
    "            quality_type_dict['ANT'] = len(quality_dropped_identifiers)\n",
    "            quality_inv_type_dict['ANT'] = len(quality_non_dropped_identifiers)\n",
    "            center_type_dict['ANT'] = len(center_dropped_identifiers)\n",
    "            center_inv_type_dict['ANT'] = len(center_non_dropped_identifiers)\n",
    "            ambiguous_type_dict['ANT'] = len(ambiguous_dropped_identifiers)\n",
    "            ambiguous_inv_type_dict['ANT'] = len(ambiguous_non_dropped_identifiers)\n",
    "        elif c == 1:\n",
    "            quality_type_dict['B6'] = len(quality_dropped_identifiers)\n",
    "            quality_inv_type_dict['B6'] = len(quality_non_dropped_identifiers)\n",
    "            center_type_dict['B6'] = len(center_dropped_identifiers)\n",
    "            center_inv_type_dict['B6'] = len(center_non_dropped_identifiers)\n",
    "            ambiguous_type_dict['B6'] = len(ambiguous_dropped_identifiers)\n",
    "            ambiguous_inv_type_dict['B6'] = len(ambiguous_non_dropped_identifiers)\n",
    "        elif c == 2:\n",
    "            quality_type_dict['NON'] = len(quality_dropped_identifiers)\n",
    "            quality_inv_type_dict['NON'] = len(quality_non_dropped_identifiers)\n",
    "            center_type_dict['NON'] = len(center_dropped_identifiers)\n",
    "            center_inv_type_dict['NON'] = len(center_non_dropped_identifiers)\n",
    "            ambiguous_type_dict['NON'] = len(ambiguous_dropped_identifiers)\n",
    "            ambiguous_inv_type_dict['NON'] = len(ambiguous_non_dropped_identifiers)\n",
    "\n",
    "        c += 1\n",
    "    ses_cut_dict[ses_cut]['quality_type_dict'] = copy.deepcopy(quality_type_dict)\n",
    "    ses_cut_dict[ses_cut]['quality_inv_type_dict'] = copy.deepcopy(quality_inv_type_dict)\n",
    "    ses_cut_dict[ses_cut]['center_type_dict'] = copy.deepcopy(center_type_dict)\n",
    "    ses_cut_dict[ses_cut]['center_inv_type_dict'] = copy.deepcopy(center_inv_type_dict)\n",
    "    ses_cut_dict[ses_cut]['ambiguous_type_dict'] = copy.deepcopy(ambiguous_type_dict)\n",
    "    ses_cut_dict[ses_cut]['ambiguous_inv_type_dict'] = copy.deepcopy(ambiguous_inv_type_dict)\n",
    "\n",
    "c = 0\n",
    "for df_current in [ANT_object_cell_df, B6_object_cell_df, NON_object_cell_df]:\n",
    "\n",
    "\n",
    "    # filter out rows where iso_dist is < 5 - Quality control\n",
    "    quality_dropped_identifiers = df_current[df_current['iso_dist'] < 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "    quality_non_dropped_identifiers = df_current[df_current['iso_dist'] >= 5].groupby(group_by_unique_cell_session).groups.keys()\n",
    "    df_current = df_current[df_current['iso_dist'] >= 5]\n",
    "\n",
    "    # filter out rows where obj_q_NO is < obj_q - CLOSER to middle than a side\n",
    "    center_dropped_identifiers = df_current[df_current['obj_q_NO'] < df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "    center_non_dropped_identifiers = df_current[df_current['obj_q_NO'] >= df_current['obj_q']].groupby(group_by_unique_cell_session).groups.keys()\n",
    "    df_current = df_current[df_current['obj_q_NO'] >= df_current['obj_q']] \n",
    "    \"\"\" HAVE TO RE RUN ALL U HAD THIS LINE AS /3, need ot check combos of ambiguous classic and mbiuous theshold 0.05 \"\"\"\n",
    "\n",
    "    # filter out rows where obj_a for centroid is != obj_a for field - Ambiguous\n",
    "    ambiguous_dropped_identifiers = df_all_unmatched_field.groupby(group_by_unique_cell_session).groups.keys()\n",
    "    # ambiguous_dropped_identifiers = df_all_ambiguous_field.groupby(group_by_unique_cell_session).groups.keys()\n",
    "    mask = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6 & (df_current['session_id'] == id7)) for id1, id2, id3, id4, id5, id6, id7 in ambiguous_dropped_identifiers], axis=1).any(axis=1)\n",
    "    df_current = df_current[~mask]\n",
    "\n",
    "    # # filter out > 3 fields\n",
    "    # field_dropped_identifiers = df_current[df_current['field_id'] > 3].groupby(group_by_unique_cell).groups.keys()\n",
    "    # df_current = df_current[df_current['field_id'] <= 3]\n",
    "\n",
    "    # filter out less than 2 sessions\n",
    "    remaining_session_dropped_identifiers = df_current.groupby(group_by_unique_cell).filter(lambda x: len(x) < 2).groupby(group_by_unique_cell).groups.keys()\n",
    "    df_current = df_current.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2)\n",
    "\n",
    "    if c == 0:\n",
    "        ANT_object_cell_df = df_current\n",
    "    elif c == 1:\n",
    "        B6_object_cell_df = df_current\n",
    "    elif c == 2:\n",
    "        NON_object_cell_df = df_current\n",
    "\n",
    "    c += 1  \n",
    "    \n",
    "ANT_cell_type_df = ANT_object_cell_df.copy()\n",
    "B6_cell_type_df = B6_object_cell_df.copy()\n",
    "NON_cell_type_df = NON_object_cell_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_cut = 3\n",
    "print('Cell-session appearances are low quality from ANT: {}, B6: {}, NON: {}'.format(ses_cut_dict[ses_cut]['quality_type_dict']['ANT'], ses_cut_dict[ses_cut]['quality_type_dict']['B6'], ses_cut_dict[ses_cut]['quality_type_dict']['NON']))\n",
    "print('Cell-session appearances are closer to the center from ANT: {}, B6: {}, NON: {}'.format(ses_cut_dict[ses_cut]['center_type_dict']['ANT'], ses_cut_dict[ses_cut]['center_type_dict']['B6'], ses_cut_dict[ses_cut]['center_type_dict']['NON']))\n",
    "print('Cell-session appearances are ambiguous to angle from ANT: {}, B6: {}, NON: {}'.format(ses_cut_dict[ses_cut]['ambiguous_type_dict']['ANT'], ses_cut_dict[ses_cut]['ambiguous_type_dict']['B6'], ses_cut_dict[ses_cut]['ambiguous_type_dict']['NON']))\n",
    "# print('Cells dropped due to field from ANT: {}, B6: {}, NON: {}'.format(ses_cut_dict[ses_cut]['field_type_dict']['ANT'], ses_cut_dict[ses_cut]['field_type_dict']['B6'], ses_cut_dict[ses_cut]['field_type_dict']['NON']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(len(ANT_cell_type_df.groupby(group_by_unique_cell_session).groups.keys()), ' remaining ANT cell-session appearances that are part of a cell with >=2 sessions')\n",
    "print(len(B6_cell_type_df.groupby(group_by_unique_cell_session).groups.keys()), ' remaining B6 cell-session appearances that are part of a cell with >=2 sessions')\n",
    "print(len(NON_cell_type_df.groupby(group_by_unique_cell_session).groups.keys()), ' remaining NON cell-session appearances that are part of a cell with >=2 sessions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_cell_df = df[df['score'] == score]\n",
    "# object_cell_df = object_cell_df[object_cell_df['session_id'].isin(['session_1','session_2','session_3'])]\n",
    "# centroid_cell_df = df[df['score'] == 'centroid']\n",
    "\n",
    "# object_cell_df = object_cell_df[object_cell_df['object_location'].astype(str) != 'NO']\n",
    "# object_cell_df = object_cell_df.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# if main_field_only:\n",
    "#     object_cell_df = object_cell_df[object_cell_df['field_id'] == 1]\n",
    "#     centroid_cell_df = centroid_cell_df[centroid_cell_df['field_id'] == 1]\n",
    "\n",
    "# # Split by group\n",
    "# group_by_unique_cell_field = ['group', 'name', 'depth', 'date','tetrode', 'unit_id', 'field_id']\n",
    "\n",
    "# ANT_df = object_cell_df[object_cell_df['group'] == 'ANT'].reset_index(drop=True)\n",
    "# B6_df = object_cell_df[object_cell_df['group'] == 'B6'].reset_index(drop=True)\n",
    "# NON_df = object_cell_df[object_cell_df['group'] == 'NON'].reset_index(drop=True)\n",
    "# centroid_ANT_df = centroid_cell_df[centroid_cell_df['group'] == 'ANT'].reset_index(drop=True)\n",
    "# centroid_B6_df = centroid_cell_df[centroid_cell_df['group'] == 'B6'].reset_index(drop=True)\n",
    "# centroid_NON_df = centroid_cell_df[centroid_cell_df['group'] == 'NON'].reset_index(drop=True)\n",
    "\n",
    "# # ANT_object_cell_df = ANT_df[ANT_df['obj_q'] <= ANT_df['field_coverage']].reset_index(drop=True)\n",
    "# # B6_object_cell_df = B6_df[B6_df['obj_q'] <= B6_df['field_coverage']].reset_index(drop=True)\n",
    "# # NON_object_cell_df = NON_df[NON_df['obj_q'] <= NON_df['field_coverage']].reset_index(drop=True)\n",
    "# ANT_object_cell_df = ANT_df.copy() \n",
    "# B6_object_cell_df = B6_df.copy() \n",
    "# NON_object_cell_df = NON_df.copy() \n",
    "# ANT_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "# B6_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "# NON_object_cell_df.sort_values(by=group_by_unique_cell_field, inplace=True)\n",
    "# ANT_object_cell_df['cell_type'] = 'unassigned'\n",
    "# B6_object_cell_df['cell_type'] = 'unassigned'\n",
    "# NON_object_cell_df['cell_type'] = 'unassigned'\n",
    "# ANT_object_cell_df['isTrace'] = 0\n",
    "# B6_object_cell_df['isTrace'] = 0\n",
    "# NON_object_cell_df['isTrace'] = 0\n",
    "# ANT_object_cell_df['isObject'] = 0\n",
    "# B6_object_cell_df['isObject'] = 0\n",
    "# NON_object_cell_df['isObject'] = 0\n",
    "# ANT_object_cell_df['trace_a'] = None\n",
    "# B6_object_cell_df['trace_a'] = None\n",
    "# NON_object_cell_df['trace_a'] = None\n",
    "\n",
    "# ANT_object_cell_df.loc[ANT_object_cell_df['obj_a'].astype(str)  == ANT_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "# B6_object_cell_df.loc[B6_object_cell_df['obj_a'].astype(str)  == B6_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "# NON_object_cell_df.loc[NON_object_cell_df['obj_a'].astype(str)  == NON_object_cell_df['object_location'].astype(str),'isObject'] = 1\n",
    "# print(len(ANT_object_cell_df[ANT_object_cell_df['cell_type'] == 'object']))\n",
    "\n",
    "# # B6_object_cell_df = B6_object_cell_df[B6_object_cell_df['obj_a'].astype(str) == B6_object_cell_df['object_location'].astype(str) ]\n",
    "# # NON_object_cell_df = NON_object_cell_df[NON_object_cell_df['obj_a'].astype(str)  == NON_object_cell_df['object_location'].astype(str) ]\n",
    "# # ANT_object_cell_df = ANT_df \n",
    "# # B6_object_cell_df = B6_df\n",
    "# # NON_object_cell_df = NON_df\n",
    "\n",
    "# keep_ids = []\n",
    "# keep_ids2 = []\n",
    "# keep_ids3 = []\n",
    "# keep_ids4 = []\n",
    "# keep_prev_locs = []\n",
    "# keep_trace_appearances = []\n",
    "# # keep_ids5 = []\n",
    "# for df_touse in [ANT_object_cell_df, B6_object_cell_df, NON_object_cell_df]:\n",
    "#     prev_unit_id = None\n",
    "#     prev_angle = None\n",
    "#     prev_loc = None\n",
    "#     prev_loc_2 = None\n",
    "#     pair_count = 0\n",
    "#     cell_passed = False\n",
    "#     to_keep = []\n",
    "#     to_keep2 = []\n",
    "#     to_keep3 = []\n",
    "#     to_keep4 = []\n",
    "#     to_keep_prev_locs = []\n",
    "#     to_keep_trace_appearance = []\n",
    "#     prev_angles = []\n",
    "#     # to_keep5 = []\n",
    "#     for i, row in df_touse.iterrows():\n",
    "#         curr_unit_id = row['unit_id']\n",
    "#         curr_tetrode = row['tetrode']\n",
    "#         curr_field_id = row['field_id']\n",
    "#         curr_angle = row['obj_a']\n",
    "#         curr_name = row['name']\n",
    "#         curr_date = row['date']\n",
    "#         curr_depth = row['depth']\n",
    "\n",
    "#         if curr_unit_id == prev_unit_id and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "#             if str(curr_angle) in prev_angles:\n",
    "#                 to_keep_trace_appearance.append(i)\n",
    "        \n",
    "#         # if curr_unit_id == prev_unit_id and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "#         #     if curr_angle == prev_angle:\n",
    "#         #         # pass # has remapped, is ok \n",
    "#         #         to_keep3.append(i)\n",
    "\n",
    "#         #     if str(curr_angle) == str(prev_loc) and str(prev_loc) != str(row['object_location']):\n",
    "#         #         to_keep.append(i)  \n",
    "#         #         to_keep_prev_locs.append(str(prev_loc))\n",
    "\n",
    "#         # if curr_unit_id == prev_unit_id and prev_loc_2 is not None and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "#         #     if str(curr_angle) == str(prev_loc_2) and str(prev_loc) != str(prev_loc_2) and str(curr_angle) != str(prev_loc) and str(curr_angle) != str(prev_angle) and str(curr_angle) != str(row['object_location']):\n",
    "#         #         to_keep2.append(i) \n",
    "\n",
    "\n",
    "#         # if curr_unit_id == prev_unit_id and curr_field_id == prev_field_id and curr_tetrode == prev_tetrode and curr_name == prev_name and curr_date == prev_date and curr_depth == prev_depth:\n",
    "#         #     if str(curr_angle) != str(prev_loc_2) and str(curr_angle) != str(prev_loc) and str(curr_angle) != str(prev_angle) and str(curr_angle) != str(row['object_location']):\n",
    "#         #         assert i not in to_keep3 \n",
    "#         #         assert i not in to_keep\n",
    "#         #         assert i not in to_keep2\n",
    "#         #         to_keep4.append(i)  \n",
    "       \n",
    "\n",
    "#         if curr_unit_id != prev_unit_id or curr_field_id != prev_field_id or curr_tetrode != prev_tetrode or curr_name != prev_name or curr_date != prev_date or curr_depth != prev_depth:\n",
    "#             prev_angles = []\n",
    "#             if i != 0:\n",
    "#                 prev_loc_2 = None\n",
    "\n",
    "#         if prev_loc is not None: \n",
    "#             prev_loc_2 = prev_loc\n",
    "\n",
    "#         prev_loc = row['object_location']\n",
    "#         prev_unit_id = curr_unit_id\n",
    "#         prev_angle = curr_angle\n",
    "#         prev_field_id = curr_field_id\n",
    "#         prev_tetrode = curr_tetrode\n",
    "#         prev_name = curr_name\n",
    "#         prev_date = curr_date\n",
    "#         prev_depth = curr_depth\n",
    "#         prev_angles.append(str(prev_loc))\n",
    "\n",
    "#     keep_ids.append(np.unique(to_keep))\n",
    "#     keep_ids2.append(np.unique(to_keep2))\n",
    "#     keep_ids3.append(np.unique(to_keep3))\n",
    "#     keep_ids4.append(np.unique(to_keep4))\n",
    "#     keep_prev_locs.append(to_keep_prev_locs)\n",
    "#     keep_trace_appearances.append(to_keep_trace_appearance)\n",
    "#     # keep_ids5.append(np.unique(to_keep5))\n",
    "\n",
    "# # # very important unassigned set first\n",
    "# # ANT_object_cell_df.loc[keep_ids4[0],'cell_type'] = 'unassigned'\n",
    "# # B6_object_cell_df.loc[keep_ids4[1],'cell_type'] = 'unassigned'\n",
    "# # NON_object_cell_df.loc[keep_ids4[2],'cell_type'] = 'unassigned'\n",
    "\n",
    "# # do not uncomment, will override object cells\n",
    "\n",
    "# # ANT_object_cell_df.loc[keep_ids3[0],'cell_type'] = 'stable'\n",
    "# # B6_object_cell_df.loc[keep_ids3[1],'cell_type'] = 'stable'\n",
    "# # NON_object_cell_df.loc[keep_ids3[2],'cell_type'] = 'stable'\n",
    "\n",
    "# # ANT_object_cell_df.loc[keep_ids[0],'cell_type'] = 'trace'\n",
    "# # B6_object_cell_df.loc[keep_ids[1],'cell_type'] = 'trace'\n",
    "# # NON_object_cell_df.loc[keep_ids[2],'cell_type'] = 'trace'\n",
    "# ANT_object_cell_df.loc[keep_ids[0],'trace_a'] = keep_prev_locs[0]\n",
    "# B6_object_cell_df.loc[keep_ids[1],'trace_a'] = keep_prev_locs[1]\n",
    "# NON_object_cell_df.loc[keep_ids[2],'trace_a'] = keep_prev_locs[2]\n",
    "\n",
    "# ANT_object_cell_df.loc[keep_trace_appearances[0],'isTrace'] = 1\n",
    "# B6_object_cell_df.loc[keep_trace_appearances[1],'isTrace'] = 1\n",
    "# NON_object_cell_df.loc[keep_trace_appearances[2],'isTrace'] = 1\n",
    "\n",
    "# # print(len(ANT_object_cell_df[ANT_object_cell_df['cell_type'] == 'object']))\n",
    "\n",
    "# # ANT_object_cell_df.loc[keep_ids2[0],'cell_type'] = 'trace'\n",
    "# # B6_object_cell_df.loc[keep_ids2[1],'cell_type'] = 'trace'\n",
    "# # NON_object_cell_df.loc[keep_ids2[2],'cell_type'] = 'trace'\n",
    "\n",
    "# # print(len(ANT_object_cell_df[ANT_object_cell_df['cell_type'] == 'object']))\n",
    "\n",
    "# # ANT_object_cell_df.loc[keep_ids5[0],'cell_type'] = 'object'\n",
    "# # B6_object_cell_df.loc[keep_ids5[1],'cell_type'] = 'object'\n",
    "# # NON_object_cell_df.loc[keep_ids5[2],'cell_type'] = 'object'\n",
    "\n",
    "\n",
    "# ANT_cell_type_df = ANT_object_cell_df.copy()\n",
    "# B6_cell_type_df = B6_object_cell_df.copy()\n",
    "# NON_cell_type_df = NON_object_cell_df.copy()\n",
    "\n",
    "# # ANT_object_cell_df = ANT_object_cell_df.iloc[keep_ids[0]]\n",
    "# # B6_object_cell_df = B6_object_cell_df.iloc[keep_ids[1]]\n",
    "# # NON_object_cell_df = NON_object_cell_df.iloc[keep_ids[2]]\n",
    "# # ANT_object_cell_df = ANT_object_cell_df[ANT_object_cell_df['cell_type'] == 'object']\n",
    "# # B6_object_cell_df = B6_object_cell_df[B6_object_cell_df['cell_type'] == 'object']\n",
    "# # NON_object_cell_df = NON_object_cell_df[NON_object_cell_df['cell_type'] == 'object']\n",
    "\n",
    "# ANT_object_cell_df = ANT_object_cell_df[ANT_object_cell_df['isObject'] == 1]\n",
    "# B6_object_cell_df = B6_object_cell_df[B6_object_cell_df['isObject'] == 1]\n",
    "# NON_object_cell_df = NON_object_cell_df[NON_object_cell_df['isObject'] == 1]\n",
    "\n",
    "# # print(len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'trace']))\n",
    "# # print(len(ANT_object_cell_df[ANT_object_cell_df['cell_type'] == 'trace']))\n",
    "\n",
    "\n",
    "\n",
    "# # get idx of cells with object width < 15\n",
    "# # idx = centroid_ANT_df[centroid_ANT_df['obj_w'] <= 4].index\n",
    "# # ANT_object_cell_df = ANT_df.iloc[idx]\n",
    "# # idx = centroid_B6_df[centroid_B6_df['obj_w'] <= 4].index\n",
    "# # B6_object_cell_df = B6_df.iloc[idx]\n",
    "# # idx = centroid_NON_df[centroid_NON_df['obj_w'] <= 4].index\n",
    "# # NON_object_cell_df = NON_df.iloc[idx]\n",
    "\n",
    "# # ANT_object_cell_df = ANT_object_cell_df[ANT_object_cell_df['obj_q'] < 0.1]\n",
    "# # B6_object_cell_df = B6_object_cell_df[B6_object_cell_df['obj_q'] < 0.1]\n",
    "# # NON_object_cell_df = NON_object_cell_df[NON_object_cell_df['obj_q'] < 0.1]\n",
    "\n",
    "# # ANT_object_cell_df = ANT_df[ANT_df['field_coverage'] < 0.2]\n",
    "# # B6_object_cell_df = B6_df[B6_df['field_coverage'] < 0.2]\n",
    "# # NON_object_cell_df = NON_df[NON_df['field_coverage'] < 0.2]\n",
    "\n",
    "# # ANT_object_cell_df = ANT_object_cell_df[ANT_object_cell_df['obj_q'] < ANT_object_cell_df['field_coverage']]\n",
    "# # B6_object_cell_df = B6_object_cell_df[B6_object_cell_df['obj_q'] < B6_object_cell_df['field_coverage']]\n",
    "# # NON_object_cell_df = NON_object_cell_df[NON_object_cell_df['obj_q'] < NON_object_cell_df['field_coverage']]\n",
    "\n",
    "# # Must be over X consecutive sessions\n",
    "# # group_by = ['name','tetrode','unit_id','date', 'field_id']\n",
    "# group_by = None\n",
    "# group_by_unique_cell_field = ['group', 'name', 'depth', 'date','tetrode', 'unit_id', 'field_id']\n",
    "# group_by_unique_cell = ['group', 'name', 'depth', 'date','tetrode', 'unit_id']\n",
    "# from itertools import count\n",
    "\n",
    "# def filter_consecutive_sessions(group):\n",
    "#     session_ids = group['session_id'].str.split('_').str[-1]\n",
    "#     prev_session_id = None\n",
    "#     count = 0\n",
    "    \n",
    "#     for session_id in session_ids:\n",
    "#         if prev_session_id is None:\n",
    "#             count = 1\n",
    "#         elif int(session_id) - int(prev_session_id) == 1:\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             count = 1\n",
    "#         prev_session_id = session_id\n",
    "        \n",
    "#         if count >= consecutive_sessions_threshold:\n",
    "#             return True\n",
    "    \n",
    "#     return False\n",
    "\n",
    "# if not consecutive:\n",
    "#     sporadic_ANT_object_cell_df = ANT_object_cell_df.groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "#     sporadic_B6_object_cell_df = B6_object_cell_df.groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "#     sporadic_NON_object_cell_df = NON_object_cell_df.groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# ANT_object_cell_df = ANT_object_cell_df.groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions)\n",
    "# B6_object_cell_df = B6_object_cell_df.groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions)\n",
    "# NON_object_cell_df = NON_object_cell_df.groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions)\n",
    "# # ANT_object_cell_df  = sporadic_ANT_object_cell_df\n",
    "# # B6_object_cell_df  = sporadic_B6_object_cell_df\n",
    "# # NON_object_cell_df  = sporadic_NON_object_cell_df\n",
    "\n",
    "# print('There are {} object cells in ANT'.format(len(ANT_object_cell_df.groupby(group_by_unique_cell))))\n",
    "# print('There are {} object cells in B6'.format(len(B6_object_cell_df.groupby(group_by_unique_cell))))\n",
    "# print('There are {} object cells in NON'.format(len(NON_object_cell_df.groupby(group_by_unique_cell))))\n",
    "\n",
    "# sum_ANT = ANT_df[ANT_df['object_location'].astype(str) != 'NO']\n",
    "# sum_ANT = sum_ANT.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "# sum_ANT = len(sum_ANT)\n",
    "# sum_B6 = B6_df[B6_df['object_location'].astype(str) != 'NO']\n",
    "# sum_B6 = sum_B6.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "# sum_B6 = len(sum_B6)\n",
    "# sum_NON = NON_df[NON_df['object_location'].astype(str) != 'NO']\n",
    "# sum_NON = sum_NON.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "# sum_NON = len(sum_NON)\n",
    "# # sum_ANT = len(ANT_df.groupby(group_by))\n",
    "# # sum_B6 = len(B6_df.groupby(group_by))\n",
    "# # sum_NON = len(NON_df.groupby(group_by))\n",
    "\n",
    "# antP = len(ANT_object_cell_df.groupby(group_by_unique_cell)) / sum_ANT * 100\n",
    "# b6P = len(B6_object_cell_df.groupby(group_by_unique_cell)) / sum_B6 * 100\n",
    "# nonP = len(NON_object_cell_df.groupby(group_by_unique_cell)) / sum_NON * 100\n",
    "# print('{}% of ANT cells are object cells out of {} available cells'.format(antP,sum_ANT))\n",
    "# print('{}% of B6 cells are object cells out of {} available cells'.format(b6P, sum_B6))\n",
    "# print('{}% of NON cells are object cells out of {} available cells'.format(nonP, sum_NON))\n",
    "\n",
    "# # ANT vs B6 chi squared test\n",
    "# from scipy.stats import chi2_contingency\n",
    "# from scipy.stats import chi2\n",
    "\n",
    "# expected_freq = (antP + nonP) / 100 * (sum_ANT + sum_NON)\n",
    "# assert expected_freq > 5, 'Expected frequency is less than 5, it is {}'.format(expected_freq)\n",
    "\n",
    "# observed_freq = len(ANT_object_cell_df.groupby(group_by_unique_cell)) + len(NON_object_cell_df.groupby(group_by_unique_cell))\n",
    "# chi, p, dof, expected = chi2_contingency([[len(ANT_object_cell_df.groupby(group_by_unique_cell)), len(NON_object_cell_df.groupby(group_by_unique_cell))],\n",
    "#                                             [sum_ANT - len(ANT_object_cell_df.groupby(group_by_unique_cell)), sum_NON - len(NON_object_cell_df.groupby(group_by_unique_cell))]])\n",
    "# print('ANT vs NON chi squared test')\n",
    "# print('chi = {}, p = {}, dof = {}'.format(chi, p, dof))\n",
    "# print('expected = {}'.format(expected))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 5))\n",
    "# groups = ['ANT', 'NON', 'B6']\n",
    "# bar_width = 0.35\n",
    "# # Main bars\n",
    "# percentages_main = [len(ANT_object_cell_df.groupby(group_by_unique_cell)) / sum_ANT * 100,\n",
    "#                len(NON_object_cell_df.groupby(group_by_unique_cell)) / sum_NON * 100,\n",
    "#                len(B6_object_cell_df.groupby(group_by_unique_cell)) / sum_B6 * 100]\n",
    "# # main_bars = ax.bar(np.arange(len(groups)), percentages_main, bar_width, color='k', alpha=1)\n",
    "# in_ANT = len(ANT_object_cell_df[ANT_object_cell_df['obj_q'] <= ANT_object_cell_df['field_coverage']]) / len(ANT_object_cell_df.groupby(group_by_unique_cell)) * percentages_main[0]\n",
    "# in_NON = len(NON_object_cell_df[NON_object_cell_df['obj_q'] <= NON_object_cell_df['field_coverage']]) / len(NON_object_cell_df.groupby(group_by_unique_cell)) * percentages_main[1]\n",
    "# in_B6 = len(B6_object_cell_df[B6_object_cell_df['obj_q'] <= B6_object_cell_df['field_coverage']]) / len(B6_object_cell_df.groupby(group_by_unique_cell)) * percentages_main[2]\n",
    "# in_field_prop = [ in_ANT / sum_ANT * 100,\n",
    "#                     in_NON / sum_NON * 100,\n",
    "#                     in_B6 / sum_B6 * 100]\n",
    "# out_field_prop = [percentages_main[0] - in_field_prop[0],\n",
    "#                     percentages_main[1] - in_field_prop[1],\n",
    "#                     percentages_main[2] - in_field_prop[2]]\n",
    "# b1 = ax.bar(np.arange(len(groups)), in_field_prop, bar_width, color='k', alpha=1, label='In-field')\n",
    "# b2 = ax.bar(np.arange(len(groups)), out_field_prop, bar_width, color='k', alpha=0.75, \n",
    "#             label='Out-field', bottom=in_field_prop)\n",
    "# # Sporadic bars\n",
    "# percentages_sporadic = [len(sporadic_ANT_object_cell_df.groupby(group_by_unique_cell)) / sum_ANT * 100,\n",
    "#                         len(sporadic_NON_object_cell_df.groupby(group_by_unique_cell)) / sum_NON * 100,\n",
    "#                         len(sporadic_B6_object_cell_df.groupby(group_by_unique_cell)) / sum_B6 * 100]\n",
    "# in_sporadic_ANT = len(sporadic_ANT_object_cell_df[sporadic_ANT_object_cell_df['obj_q'] <= sporadic_ANT_object_cell_df['field_coverage']]) / len(sporadic_ANT_object_cell_df.groupby(group_by_unique_cell)) * percentages_sporadic[0]\n",
    "# in_sporadic_NON = len(sporadic_NON_object_cell_df[sporadic_NON_object_cell_df['obj_q'] <= sporadic_NON_object_cell_df['field_coverage']]) / len(sporadic_NON_object_cell_df.groupby(group_by_unique_cell)) * percentages_sporadic[1]\n",
    "# in_sporadic_B6 = len(sporadic_B6_object_cell_df[sporadic_B6_object_cell_df['obj_q'] <= sporadic_B6_object_cell_df['field_coverage']]) / len(sporadic_B6_object_cell_df.groupby(group_by_unique_cell)) * percentages_sporadic[2]\n",
    "# in_field_prop_sporadic = [ in_sporadic_ANT / sum_ANT * 100,\n",
    "#                     in_sporadic_NON / sum_NON * 100,\n",
    "#                     in_sporadic_B6 / sum_B6 * 100]\n",
    "# out_field_prop_sporadic = [percentages_sporadic[0] - in_field_prop_sporadic[0],\n",
    "#                     percentages_sporadic[1] - in_field_prop_sporadic[1],\n",
    "#                     percentages_sporadic[2] - in_field_prop_sporadic[2]]\n",
    "\n",
    "# b3 = ax.bar(np.arange(len(groups)) + bar_width, in_field_prop_sporadic, bar_width, color='k', alpha=0.5)\n",
    "# b4 = ax.bar(np.arange(len(groups)) + bar_width, out_field_prop_sporadic, bar_width, color='k', \n",
    "#             alpha=0.25, bottom=in_field_prop_sporadic)\n",
    "# # sporadic_bars = ax.bar(np.arange(len(groups)) + bar_width, percentages_sporadic, bar_width, color='grey', alpha=1)\n",
    "# # Connect the bars with a line\n",
    "# ax.plot(np.arange(len(groups)), percentages_main, 'k-', marker='o', alpha=0.5)\n",
    "# ax.plot(np.arange(len(groups)) + bar_width, percentages_sporadic, 'k-', marker='o', alpha=0.5)\n",
    "# # Set the x-axis tick positions and labels\n",
    "# ax.set_xticks(np.arange(len(groups)) + bar_width / 2)\n",
    "# ax.set_xticklabels(groups)\n",
    "# # Add legend\n",
    "# ax.legend([b1, b2, b3, b4], ['Consecutive In-field', 'Consecutive Out-field', 'Sporadic In-field', 'Sporadic Out-field'])\n",
    "# ax.set_ylabel('% of object cells')\n",
    "# title = 'Object cells using {} distance'.format(score)\n",
    "# ax.set_title(title)\n",
    "# plt.show()\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 5))\n",
    "# ax = fig.add_subplot(111)\n",
    "# colors = ['r', 'g', 'b']\n",
    "# angle_groups = [0, 90, 180, 270]\n",
    "# bar_width = 0.1\n",
    "# spacing = 0.1\n",
    "# for i, obj_a in enumerate(angle_groups):\n",
    "#     # obj_a = str(obj_a)\n",
    "#     ANT_obj_a_df = ANT_object_cell_df[ANT_object_cell_df['obj_a'] == obj_a]\n",
    "#     B6_obj_a_df = B6_object_cell_df[B6_object_cell_df['obj_a'] == obj_a]\n",
    "#     NON_obj_a_df = NON_object_cell_df[NON_object_cell_df['obj_a'] == obj_a]\n",
    "#     # ANT_obj_a_df = ANT_object_cell_df[ANT_object_cell_df['trace_a'] == obj_a]\n",
    "#     # B6_obj_a_df = B6_object_cell_df[B6_object_cell_df['trace_a'] == obj_a]\n",
    "#     # NON_obj_a_df = NON_object_cell_df[NON_object_cell_df['trace_a'] == obj_a]\n",
    "#     # sum_ANT = len(ANT_df[ANT_df['obj_a'] == obj_a].groupby(group_by))\n",
    "#     # sum_B6 = len(B6_df[B6_df['obj_a'] == obj_a].groupby(group_by))\n",
    "#     # sum_NON = len(NON_df[NON_df['obj_a'] == obj_a].groupby(group_by))\n",
    "\n",
    "#     if not consecutive:\n",
    "#         sporadic_ANT_obj_a_df = ANT_obj_a_df.groupby(group_by_unique_cell).filter(lambda x: len(x) >= consecutive_sessions_threshold)\n",
    "#         sporadic_B6_obj_a_df = B6_obj_a_df.groupby(group_by_unique_cell).filter(lambda x: len(x) >= consecutive_sessions_threshold)\n",
    "#         sporadic_NON_obj_a_df = NON_obj_a_df.groupby(group_by_unique_cell).filter(lambda x: len(x) >= consecutive_sessions_threshold)\n",
    "#     ANT_obj_a_df = ANT_obj_a_df.groupby(group_by_unique_cell).filter(filter_consecutive_sessions)\n",
    "#     B6_obj_a_df = B6_obj_a_df.groupby(group_by_unique_cell).filter(filter_consecutive_sessions)\n",
    "#     NON_obj_a_df = NON_obj_a_df.groupby(group_by_unique_cell).filter(filter_consecutive_sessions)\n",
    "#     percentages = [len(ANT_obj_a_df.groupby(group_by_unique_cell)) / sum_ANT * 100,\n",
    "#                    len(NON_obj_a_df.groupby(group_by_unique_cell)) / sum_NON * 100,\n",
    "#                    len(B6_obj_a_df.groupby(group_by_unique_cell)) / sum_B6 * 100]\n",
    "#     if not consecutive:\n",
    "#         percentages_sporadic = [len(sporadic_ANT_obj_a_df.groupby(group_by_unique_cell)) / sum_ANT * 100,\n",
    "#                                 len(sporadic_NON_obj_a_df.groupby(group_by_unique_cell)) / sum_NON * 100,\n",
    "#                                 len(sporadic_B6_obj_a_df.groupby(group_by_unique_cell)) / sum_B6 * 100]\n",
    "#     # Main bars\n",
    "#     main_bars = ax.bar(np.arange(len(groups)) + i * (bar_width + spacing), percentages, bar_width, color=colors, alpha=1)\n",
    "\n",
    "#     # Sporadic bars\n",
    "#     if not consecutive:\n",
    "#         sporadic_bars = ax.bar(np.arange(len(groups)) + i * (bar_width + spacing) + bar_width, percentages_sporadic, bar_width, color=colors, alpha=0.5)\n",
    "# ax.legend([main_bars, sporadic_bars], ['Consecutive', 'Sporadic'])\n",
    "# ax.set_xticks(np.arange(len(groups)) + (len(angle_groups) - 1) * (bar_width + spacing) / 2)\n",
    "# ax.set_xticklabels(groups)\n",
    "\n",
    "# ax.set_ylabel('% of object cells')\n",
    "# title = 'Object cells using {} distance for angle order {}'.format(score, angle_groups)\n",
    "# ax.set_title(title)\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# groups = ['ANT', 'NON', 'B6']\n",
    "# data = []\n",
    "# metric = 'obj_w'\n",
    "\n",
    "# # Extract values from DataFrames\n",
    "# for subdf in [ANT_object_cell_df, NON_object_cell_df, B6_object_cell_df]:\n",
    "#     data.append(subdf[metric].values)  # Replace 'column_name' with the actual column name in your data\n",
    "\n",
    "# # Create boxplots\n",
    "# boxprops = dict(linewidth=1.5, color='k')\n",
    "# flierprops = dict(marker='o', markersize=5, markerfacecolor='k', linestyle='none')\n",
    "# whiskerprops = dict(linewidth=1.5, color='k')\n",
    "# medianprops = dict(linewidth=2.5, color='r')\n",
    "# ax.boxplot(data, positions=np.arange(len(groups)), patch_artist=True, boxprops=boxprops,\n",
    "#            flierprops=flierprops, whiskerprops=whiskerprops, medianprops=medianprops)\n",
    "\n",
    "# # Set the x-axis tick positions and labels\n",
    "# ax.set_xticks(np.arange(len(groups)))\n",
    "# ax.set_xticklabels(groups)\n",
    "# # Set the y-axis label and title\n",
    "# ax.set_ylabel(metric)\n",
    "# title = 'Object cells using {} distance'.format(score)\n",
    "# ax.set_title(title)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # # Split by session_id\n",
    "# # for group in ['ANT', 'B6', 'NON']:\n",
    "# #     for session_id in df[df['group'] == group]['session_id'].unique():\n",
    "# #         session_df = df[(df['group'] == group) & (df['session_id'] == session_id)]\n",
    "# #         object_cell_df = session_df[(session_df['score'] == 'field') & (session_df['field_id'] == 1) & (session_df['obj_q'] < 0.05)]\n",
    "\n",
    "# #         print('There are {} object cells in {} {}'.format(len(object_cell_df), group, session_id))\n",
    "# #         print('{}% of {} cells are object cells in {}'.format(len(object_cell_df) / len(session_df) * 100, group, session_id))\n",
    "# #         print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_dict = {}\n",
    "\n",
    "identifier_dict['object'] = {}\n",
    "identifier_dict['trace'] = {}\n",
    "identifier_dict['unassigned'] = {}\n",
    "\n",
    "# identifier_dict['object']['ANT'] = list(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell).groups.keys())\n",
    "\n",
    "identifier_dict['object']['ANT'] = list(ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "identifier_dict['object']['B6'] = list(B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "identifier_dict['object']['NON'] = list(NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell).groups.keys())\n",
    "mask = pd.concat([(ANT_cell_type_df['group'] == id1) & (ANT_cell_type_df['name'] == id2) & (ANT_cell_type_df['depth'] == id3) & (ANT_cell_type_df['date'] == id4) & (ANT_cell_type_df['tetrode'] == id5) & (ANT_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['ANT']], axis=1).any(axis=1)\n",
    "ANT_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "mask = pd.concat([(B6_cell_type_df['group'] == id1) & (B6_cell_type_df['name'] == id2) & (B6_cell_type_df['depth'] == id3) & (B6_cell_type_df['date'] == id4) & (B6_cell_type_df['tetrode'] == id5) & (B6_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['B6']], axis=1).any(axis=1)\n",
    "B6_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "mask = pd.concat([(NON_cell_type_df['group'] == id1) & (NON_cell_type_df['name'] == id2) & (NON_cell_type_df['depth'] == id3) & (NON_cell_type_df['date'] == id4) & (NON_cell_type_df['tetrode'] == id5) & (NON_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['object']['NON']], axis=1).any(axis=1)\n",
    "NON_cell_type_df.loc[mask,'cell_type'] = 'object'\n",
    "\n",
    "filt = ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = ANT_cell_type_df[(ANT_cell_type_df['isObject'] == 0) & (ANT_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['ANT']]\n",
    "identifier_dict['trace']['ANT'] = list(unique_set)\n",
    "mask = pd.concat([(ANT_cell_type_df['group'] == id1) & (ANT_cell_type_df['name'] == id2) & (ANT_cell_type_df['depth'] == id3) & (ANT_cell_type_df['date'] == id4) & (ANT_cell_type_df['tetrode'] == id5) & (ANT_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['ANT']], axis=1).any(axis=1)\n",
    "ANT_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "\n",
    "filt = B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = B6_cell_type_df[(B6_cell_type_df['isObject'] == 0) & (B6_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['B6']]\n",
    "identifier_dict['trace']['B6'] = list(unique_set)\n",
    "mask = pd.concat([(B6_cell_type_df['group'] == id1) & (B6_cell_type_df['name'] == id2) & (B6_cell_type_df['depth'] == id3) & (B6_cell_type_df['date'] == id4) & (B6_cell_type_df['tetrode'] == id5) & (B6_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['B6']], axis=1).any(axis=1)\n",
    "B6_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "\n",
    "filt = NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = filt[filt['isTrace'] == 1].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = NON_cell_type_df[(NON_cell_type_df['isObject'] == 0) & (NON_cell_type_df['isTrace'] == 1)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['NON']]\n",
    "identifier_dict['trace']['NON'] = list(unique_set)\n",
    "mask = pd.concat([(NON_cell_type_df['group'] == id1) & (NON_cell_type_df['name'] == id2) & (NON_cell_type_df['depth'] == id3) & (NON_cell_type_df['date'] == id4) & (NON_cell_type_df['tetrode'] == id5) & (NON_cell_type_df['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in identifier_dict['trace']['NON']], axis=1).any(axis=1)\n",
    "NON_cell_type_df.loc[mask,'cell_type'] = 'trace'\n",
    "\n",
    "objectfiltout = ANT_cell_type_df[ANT_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = ANT_cell_type_df[(ANT_cell_type_df['isTrace'] == 0) & (ANT_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['trace']['ANT']]\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['ANT']]\n",
    "identifier_dict['unassigned']['ANT'] = list(unique_set)\n",
    "\n",
    "objectfiltout = B6_cell_type_df[B6_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = B6_cell_type_df[(B6_cell_type_df['isTrace'] == 0) & (B6_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['trace']['B6']]\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['B6']]\n",
    "identifier_dict['unassigned']['B6'] = list(unique_set)\n",
    "objectfiltout = NON_cell_type_df[NON_cell_type_df['isObject'] == 1].groupby(group_by_unique_cell_field).filter(lambda x: len(x) < 2)\n",
    "first_set = objectfiltout[objectfiltout['isTrace'] == 0].groupby(group_by_unique_cell).groups.keys()\n",
    "second_set = NON_cell_type_df[(NON_cell_type_df['isTrace'] == 0) & (NON_cell_type_df['isObject'] == 0)].groupby(group_by_unique_cell).groups.keys()\n",
    "# unq_set = set(first_set).difference(second_set)\n",
    "unique_set = list(set(list(first_set) + list(second_set)))\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['trace']['NON']]\n",
    "unique_set = [x for x in unique_set if x not in identifier_dict['object']['NON']]\n",
    "identifier_dict['unassigned']['NON'] = list(unique_set)\n",
    "# identifier_dict['unassigned']['ANT'] = list(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell).groups.keys())\n",
    "# identifier_dict['unassigned']['B6'] = list(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell).groups.keys())\n",
    "# identifier_dict['unassigned']['NON'] = list(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell).groups.keys())\n",
    "\n",
    "# save dict \n",
    "# import pickle\n",
    "# with open('LEC_cell_types.pkl', 'wb') as f:\n",
    "#     pickle.dump(identifier_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('group','ANT', 'B6', 'NON')\n",
    "print('object',len(identifier_dict['object']['ANT']), len(identifier_dict['object']['B6']), len(identifier_dict['object']['NON']))\n",
    "print('trace',len(identifier_dict['trace']['ANT']), len(identifier_dict['trace']['B6']), len(identifier_dict['trace']['NON']))\n",
    "print('neither',len(identifier_dict['unassigned']['ANT']), len(identifier_dict['unassigned']['B6']), len(identifier_dict['unassigned']['NON']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ANT_cell_type_df.groupby(group_by_unique_cell).agg({'cell_type': 'first'}).reset_index()['cell_type'].value_counts())\n",
    "print(B6_cell_type_df.groupby(group_by_unique_cell).agg({'cell_type': 'first'}).reset_index()['cell_type'].value_counts())\n",
    "print(NON_cell_type_df.groupby(group_by_unique_cell).agg({'cell_type': 'first'}).reset_index()['cell_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "ses123_identifier_dict = copy.deepcopy(identifier_dict)\n",
    "# ses1234_identifier_dict = copy.deepcopy(identifier_dict)\n",
    "# ses12345_identifier_dict = copy.deepcopy(identifier_dict)\n",
    "# ses123456_identifier_dict = copy.deepcopy(identifier_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ses123_identifier_dict['object']['ANT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "dlist = [ses123_identifier_dict]\n",
    "# ses1234_identifier_dict, ses12345_identifier_dict, ses123456_identifier_dict]\n",
    "dtitles = ['Session 1 to 3', 'Session 1 to 4', 'Session 1 to 5', 'Session 1 to 6']\n",
    "# dpositions = [0.93, 0.68, 0.33, 0.03]\n",
    "# axids = [[[0,0],[0,1],[0,2]], [[1,0],[1,1],[1,2]], [[2,0],[2,1],[2,2]], [[3,0],[3,1],[3,2]]]\n",
    "lbls = ['N=3', 'N=4', 'N=5', 'N=6']\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "# gspec 3x3\n",
    "gspec = gridspec.GridSpec(2,3)\n",
    "\n",
    "b6notdone_1 = True\n",
    "b6notdone_2 = True\n",
    "b6notdone_3 = True\n",
    "nonnotdone_2 = True\n",
    "\n",
    "for i in range(len(dlist)):\n",
    "    dtouse = dlist[i]\n",
    "    dtitle = dtitles[i]\n",
    "    # dpos = dpositions[i]\n",
    "    # axid = axids[i]\n",
    "\n",
    "    ANT_obj = dtouse['object']['ANT']\n",
    "    ANT_trace = dtouse['trace']['ANT']\n",
    "    ANT_unassigned = dtouse['unassigned']['ANT']\n",
    "\n",
    "    prev_ANT_obj_per = len(ANT_obj) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    sum_ANT = ANT_cell_type_df[ANT_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_ANT = sum_ANT.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_ANT = len(sum_ANT)\n",
    "    ANT_obj_per = len(ANT_obj) / sum_ANT\n",
    "    print(ANT_obj_per, prev_ANT_obj_per)\n",
    "    ANT_trace_per = len(ANT_trace) / (len(ANT_obj) + len(ANT_trace) + len(ANT_unassigned))\n",
    "    ANT_unassigned_per = 1 - ANT_obj_per - ANT_trace_per\n",
    "\n",
    "    B6_obj = dtouse['object']['B6']\n",
    "    B6_trace = dtouse['trace']['B6']\n",
    "    B6_unassigned = dtouse['unassigned']['B6']\n",
    "\n",
    "    # B6_obj_per = len(B6_obj) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    sum_B6 = B6_cell_type_df[B6_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_B6 = sum_B6.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_B6 = len(sum_B6)\n",
    "    B6_obj_per = len(B6_obj) / sum_B6\n",
    "    B6_trace_per = len(B6_trace) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    # B6_unassigned_per = len(B6_unassigned) / (len(B6_obj) + len(B6_trace) + len(B6_unassigned))\n",
    "    B6_unassigned_per = 1 - B6_obj_per - B6_trace_per\n",
    "\n",
    "    NON_obj = dtouse['object']['NON']\n",
    "    NON_trace = dtouse['trace']['NON']\n",
    "    NON_unassigned = dtouse['unassigned']['NON']\n",
    "\n",
    "    # NON_obj_per = len(NON_obj) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    sum_NON = NON_cell_type_df[NON_cell_type_df['object_location'].astype(str) != 'NO']\n",
    "    sum_NON = sum_NON.groupby(group_by_unique_cell).filter(lambda x: len(x) >= 2).groupby(group_by_unique_cell)\n",
    "    sum_NON = len(sum_NON)\n",
    "    NON_obj_per = len(NON_obj) / sum_NON\n",
    "    NON_trace_per = len(NON_trace) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    # NON_unassigned_per = len(NON_unassigned) / (len(NON_obj) + len(NON_trace) + len(NON_unassigned))\n",
    "    NON_unassigned_per = 1 - NON_obj_per - NON_trace_per\n",
    "\n",
    "    obj_per = np.array([ANT_obj_per, B6_obj_per, NON_obj_per]) * 100\n",
    "    trace_per = np.array([ANT_trace_per, B6_trace_per, NON_trace_per]) * 100\n",
    "    unassigned_per = np.array([ANT_unassigned_per, B6_unassigned_per, NON_unassigned_per]) * 100\n",
    "\n",
    "    unassigned_per = 100 - unassigned_per\n",
    "\n",
    "\n",
    "    # top left\n",
    "    ax1 = plt.subplot(gspec[0,0])\n",
    "    ax1.bar(['ANT', 'B6', 'NON'], obj_per, color=['red', 'blue', 'green'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax1.bar(['ANT', 'B6', 'NON'], obj_per, color=['red', 'blue', 'green'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'OBJECT' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax1.plot(['ANT', 'B6', 'NON'], obj_per, 'k-', marker='o', alpha=0.5)\n",
    "    # annotate\n",
    "    for j, lbl in enumerate(['ANT', 'B6', 'NON']):\n",
    "        if lbl == 'B6' and b6notdone_1:\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax1.annotate(lbls[i], xy=(lbl, obj_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_1 = False\n",
    "\n",
    "\n",
    "    ax1.set_title('Object')\n",
    "    ax1.set_ylabel('% unique cells')\n",
    "\n",
    "    # top middle\n",
    "    ax2 = plt.subplot(gspec[0,1])\n",
    "    ax2.bar(['ANT', 'B6', 'NON'], trace_per, color=['red', 'blue', 'green'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax2.bar(['ANT', 'B6', 'NON'], trace_per, color=['red', 'blue', 'green'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'TRACE' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "    ax2.plot(['ANT', 'B6', 'NON'], trace_per, 'k-', marker='o', alpha=0.5)\n",
    "    for j, lbl in enumerate(['ANT', 'B6', 'NON']):\n",
    "        if lbl == 'B6' and b6notdone_2:\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        # elif lbl == 'NON' and nonnotdone_2 and i != 0:\n",
    "        #     ax2.annotate('N=4,5,6', xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,15), ha='center')\n",
    "        # elif lbl == 'ANT' or (lbl == 'NON' and i == 0):\n",
    "        elif lbl != 'B6':\n",
    "            ax2.annotate(lbls[i], xy=(lbl, trace_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_2 = False\n",
    "        if lbl == 'NON' and i != 0:\n",
    "            nonnotdone_2 = False\n",
    "    ax2.set_title('Trace')\n",
    "\n",
    "    # top right\n",
    "    ax3 = plt.subplot(gspec[0,2])\n",
    "    ax3.bar(['ANT', 'B6', 'NON'], unassigned_per, color=['red', 'blue', 'green'], alpha = 0.2)\n",
    "    if i == 0:\n",
    "        ax3.bar(['ANT', 'B6', 'NON'], unassigned_per, color=['red', 'blue', 'green'], alpha = 1)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = results_df['Comparison']\n",
    "        accepted = results_df['Accepted']\n",
    "        adjusted = results_df['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], unassigned_per, maxasterix=5)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "    ax3.plot(['ANT', 'B6', 'NON'], unassigned_per, 'k-', marker='o', alpha=0.5)\n",
    "    ax3.set_title('Assigned')\n",
    "    for j, lbl in enumerate(['ANT', 'B6', 'NON']):\n",
    "        if lbl == 'B6' and b6notdone_3:\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "        elif lbl != 'B6':\n",
    "            ax3.annotate(lbls[i], xy=(lbl, unassigned_per[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "        if lbl == 'B6':\n",
    "            b6notdone_3 = False\n",
    "    ax3.set_ylim([0,100])\n",
    "\n",
    "\n",
    "    ax6 = plt.subplot(gspec[1,2])\n",
    "    per1 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['ANT'] +  ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['B6'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['ambiguous_type_dict']['NON'] + ses_cut_dict[int(3+i)]['ambiguous_inv_type_dict']['NON'])\n",
    "    ambiguous_pers = np.array( [per1, per2, per3])*100\n",
    "    ax6.plot(['ANT', 'B6', 'NON'], ambiguous_pers, 'k-', marker='o', alpha=0.5)\n",
    "    ax6.set_title('Ambiguous')\n",
    "    ax6.bar(['ANT', 'B6', 'NON'], ambiguous_pers, color=['red', 'blue', 'green'], alpha=0.2)\n",
    "    if i == 0:\n",
    "        ax6.bar(['ANT', 'B6', 'NON'], ambiguous_pers, color=['red', 'blue', 'green'], alpha = 1)\n",
    "    print(ambiguous_pers)\n",
    "    if i == len(dlist)-1:\n",
    "        comps = dflist[2]['Comparison']\n",
    "        accepted = dflist[2]['Accepted']\n",
    "        adjusted = dflist[2]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], ambiguous_pers, maxasterix=5)\n",
    "    # for j, lbl in enumerate(['ANT', 'B6', 'NON']):\n",
    "    #     if lbl == 'B6' and i == 0:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,10), ha='center')\n",
    "    #     elif lbl != 'B6' and i == 0 or lbl == 'B6' and i == 3:\n",
    "    #         ax6.annotate(lbls[i], xy=(lbl, ambiguous_pers[j]),textcoords='offset points', xytext=(0,3), ha='center')\n",
    "\n",
    "\n",
    "\n",
    "    ax5 = plt.subplot(gspec[1,1])\n",
    "    per1 = ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['center_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['center_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['center_type_dict']['B6'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['center_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['center_type_dict']['NON'] + ses_cut_dict[int(3+i)]['center_inv_type_dict']['NON'])\n",
    "    center_pers = np.array( [per1, per2, per3])*100\n",
    "    ax5.plot(['ANT', 'B6', 'NON'], center_pers, 'k-', marker='o', alpha=0.5)\n",
    "    ax5.set_title('Closer to center')\n",
    "    ax5.bar(['ANT', 'B6', 'NON'], center_pers, color=['red', 'blue', 'green'], alpha = .2)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        ax5.bar(['ANT', 'B6', 'NON'], center_pers, color=['red', 'blue', 'green'], alpha = 1)\n",
    "\n",
    "    if i == len(dlist)-1:\n",
    "        comps = dflist[1]['Comparison']\n",
    "        accepted = dflist[1]['Accepted']\n",
    "        adjusted = dflist[1]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], center_pers, maxasterix=5)\n",
    "\n",
    "    ax4 = plt.subplot(gspec[1,0])\n",
    "    per1 = ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['ANT'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['ANT'])\n",
    "    per2 = ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['B6'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['B6'])\n",
    "    per3 = ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] / (ses_cut_dict[int(3+i)]['quality_type_dict']['NON'] + ses_cut_dict[int(3+i)]['quality_inv_type_dict']['NON'])\n",
    "    quality_pers = np.array( [per1, per2, per3])*100\n",
    "    ax4.plot(['ANT', 'B6', 'NON'], quality_pers, 'k-', marker='o', alpha=0.5)\n",
    "    ax4.set_title('Low quality')\n",
    "    ax4.bar(['ANT', 'B6', 'NON'], quality_pers, color=['red', 'blue', 'green'], alpha = .2)\n",
    "\n",
    "    if i == 0:\n",
    "        ax4.bar(['ANT', 'B6', 'NON'], quality_pers, color=['red', 'blue', 'green'], alpha = 1)\n",
    "    if i == len(dlist)-1:   \n",
    "        comps = dflist[0]['Comparison']\n",
    "        accepted = dflist[0]['Accepted']\n",
    "        adjusted = dflist[0]['Adjusted P-value']\n",
    "        for k in range(len(comps)):\n",
    "            comparison = comps[k]\n",
    "            if 'UNASSIGNED' in str(comparison):\n",
    "                if 'ANT' in comparison and 'B6' in comparison:\n",
    "                    nme = [0,1]\n",
    "                elif 'ANT' in comparison and 'NON' in comparison:\n",
    "                    nme = [0,2]\n",
    "                elif 'B6' in comparison and 'NON' in comparison:\n",
    "                    nme = [1,2]\n",
    "                \n",
    "                # if accepted[k]:\n",
    "                barplot_annotate_brackets(nme[0],nme[1],adjusted[k],[0,1,2], quality_pers, maxasterix=5)\n",
    "\n",
    "    ax4.set_ylabel('% of cell-session appearances')\n",
    "\n",
    "\n",
    "    fig.suptitle(\"Fisher's exact test, capped at first 3 session\")\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type_dict['ANT'] = len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'ANT'].groupby(group_by_unique_cell).groups.keys())\n",
    "type_dict['B6'] = len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'B6'].groupby(group_by_unique_cell).groups.keys())\n",
    "type_dict['NON'] = len(df_all_unmatched_field[df_all_unmatched_field['group'] == 'NON'].groupby(group_by_unique_cell).groups.keys())\n",
    "inv_type_dict['ANT'] = len(df_all_matched_field[df_all_matched_field['group'] == 'ANT'].groupby(group_by_unique_cell).groups.keys())\n",
    "inv_type_dict['B6'] = len(df_all_matched_field[df_all_matched_field['group'] == 'B6'].groupby(group_by_unique_cell).groups.keys())\n",
    "inv_type_dict['NON'] = len(df_all_matched_field[df_all_matched_field['group'] == 'NON'].groupby(group_by_unique_cell).groups.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11517986/indicating-the-statistically-significant-difference-in-bar-graph\n",
    "\n",
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05\n",
    "        # ** is p < 0.005\n",
    "        # *** is p < 0.0005\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        while data < p:\n",
    "            text += '*'\n",
    "            p /= 10.\n",
    "\n",
    "            if maxasterix and len(text) == maxasterix:\n",
    "                break\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black')\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3,2) graph of spike count, information , selectivity, firing rate, spike width, iso dist\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(12, 26))\n",
    "gs = gridspec.GridSpec(8, 2)\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "ax3 = plt.subplot(gs[1, 0])\n",
    "ax4 = plt.subplot(gs[1, 1])\n",
    "ax5 = plt.subplot(gs[2, 0])\n",
    "ax6 = plt.subplot(gs[2, 1])\n",
    "ax7 = plt.subplot(gs[3, 0])\n",
    "ax8 = plt.subplot(gs[3, 1])\n",
    "ax9 = plt.subplot(gs[4, 0])\n",
    "ax10 = plt.subplot(gs[4, 1])\n",
    "ax11 = plt.subplot(gs[5, 0])\n",
    "ax12 = plt.subplot(gs[5, 1])\n",
    "ax13 = plt.subplot(gs[6, 0])\n",
    "ax14 = plt.subplot(gs[6, 1])\n",
    "ax15 = plt.subplot(gs[7, 0])\n",
    "ax16 = plt.subplot(gs[7, 1])\n",
    "\n",
    "groups = ['ANT', 'B6', 'NON']\n",
    "clrs = ['r', 'b', 'g']\n",
    "axs = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16]\n",
    "metrics = ['spike_count', 'firing_rate', 'information', 'selectivity', 'spike_width', 'iso_dist', 'coherence', 'sparsity',\n",
    "           'field_coverage', 'field_area', 'Avg. Spikes/Burst', 'bursting', 'obj_q', 'obj_w', 'depth', 'obj_a']\n",
    "data = [ANT_cell_type_df, B6_cell_type_df, NON_cell_type_df]\n",
    "\n",
    "for metric, ax in zip(metrics, axs):\n",
    "    for i, group in enumerate(groups):\n",
    "        touse = data[i]\n",
    "        touse = touse[touse['cell_type'] == 'unassigned']\n",
    "        sns.kdeplot(touse[metric], label=group, color=clrs[i], ax=ax)\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend()\n",
    "\n",
    "        if metric == 'selectivity':\n",
    "            ax.set_xlim(0, 300)\n",
    "        else:\n",
    "            ax.set_xlim(0)\n",
    "\n",
    "# ttl = 'Properties for {} ANT, {} B6, {} NON object cells'.format(len(ANT_object_cell_df), len(B6_object_cell_df), len(NON_object_cell_df))\n",
    "# fig.suptitle(ttl)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cell_type = pd.concat([ANT_cell_type_df, B6_cell_type_df, NON_cell_type_df])\n",
    "# full_cell_type.loc[full_cell_type['cell_type'] == 'stable', 'cell_type'] = 'unassigned'\n",
    "\n",
    "type_dict = {'ANT': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'B6': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'NON': {'object': 0, 'trace': 0, 'unassigned': 0}}\n",
    "\n",
    "type_dict['ANT']['object'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['ANT']['non-object'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] != 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "ANT_object = type_dict['ANT']['object']\n",
    "type_dict['ANT']['trace'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['ANT']['non-trace'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] != 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "ANT_trace = type_dict['ANT']['trace']\n",
    "type_dict['ANT']['unassigned'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['ANT']['assigned'] = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] != 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "ANT_unassigned = type_dict['ANT']['unassigned']\n",
    "\n",
    "type_dict['B6']['object'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['B6']['non-object'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] != 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "B6_object = type_dict['B6']['object']\n",
    "type_dict['B6']['trace'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['B6']['non-trace'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] != 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "B6_trace = type_dict['B6']['trace']\n",
    "type_dict['B6']['unassigned'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['B6']['assigned'] = len(B6_cell_type_df[B6_cell_type_df['cell_type'] != 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "B6_unassigned = type_dict['B6']['unassigned']\n",
    "\n",
    "type_dict['NON']['object'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['NON']['non-object'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] != 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell)) \n",
    "NON_object = type_dict['NON']['object']\n",
    "type_dict['NON']['trace'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['NON']['non-trace'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] != 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "NON_trace = type_dict['NON']['trace']\n",
    "type_dict['NON']['unassigned'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "type_dict['NON']['assigned'] = len(NON_cell_type_df[NON_cell_type_df['cell_type'] != 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "NON_unassigned = type_dict['NON']['unassigned']\n",
    "\n",
    "# B6_object = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "# NON_object = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'object'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "\n",
    "# ANT_trace = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "# B6_trace = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "# NON_trace = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'trace'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "\n",
    "# ANT_unassigned = len(ANT_cell_type_df[ANT_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "# B6_unassigned = len(B6_cell_type_df[B6_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "# NON_unassigned = len(NON_cell_type_df[NON_cell_type_df['cell_type'] == 'unassigned'].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell))\n",
    "\n",
    "# Define the function to get unique cells for each category\n",
    "def get_unique_cells_in_category(df, category):\n",
    "    unique_cells = df[df['cell_type'] == category].groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions).groupby(group_by_unique_cell)\n",
    "    return unique_cells.groups.keys()\n",
    "\n",
    "# Get the unique cells for each category and group\n",
    "ANT_object_cells = set(get_unique_cells_in_category(ANT_cell_type_df, 'object'))\n",
    "ANT_trace_cells = set(get_unique_cells_in_category(ANT_cell_type_df, 'trace'))\n",
    "B6_object_cells = set(get_unique_cells_in_category(B6_cell_type_df, 'object'))\n",
    "B6_trace_cells = set(get_unique_cells_in_category(B6_cell_type_df, 'trace'))\n",
    "NON_object_cells = set(get_unique_cells_in_category(NON_cell_type_df, 'object'))\n",
    "NON_trace_cells = set(get_unique_cells_in_category(NON_cell_type_df, 'trace'))\n",
    "\n",
    "# Find unique cells not in both \"trace\" and \"object\" for each group\n",
    "ANT_not_in_both = len(ANT_object_cells.symmetric_difference(ANT_trace_cells))\n",
    "B6_not_in_both = len(B6_object_cells.symmetric_difference(B6_trace_cells))\n",
    "NON_not_in_both = len(NON_object_cells.symmetric_difference(NON_trace_cells))\n",
    "\n",
    "# Find unique cells in both \"trace\" and \"object\" for each group\n",
    "ANT_in_both = len(ANT_object_cells.intersection(ANT_trace_cells))\n",
    "B6_in_both = len(B6_object_cells.intersection(B6_trace_cells))\n",
    "NON_in_both = len(NON_object_cells.intersection(NON_trace_cells))\n",
    "\n",
    "# remove the cells that are in both \"trace\" and \"object\" from the total number of cells\n",
    "ANT_object = ANT_object - ANT_in_both\n",
    "B6_object = B6_object - B6_in_both\n",
    "NON_object = NON_object - NON_in_both\n",
    "type_dict['ANT']['object'] = ANT_object\n",
    "type_dict['B6']['object'] = B6_object\n",
    "type_dict['NON']['object'] = NON_object\n",
    "\n",
    "\n",
    "contingency_table = pd.DataFrame({'object': [ANT_object, B6_object, NON_object],\n",
    "                                    'trace': [ANT_trace, B6_trace, NON_trace],\n",
    "                                    'unassigned': [ANT_unassigned, B6_unassigned, NON_unassigned],\n",
    "                                    'both': [ANT_in_both, B6_in_both, NON_in_both]},\n",
    "                                    index=['ANT', 'B6', 'NON'])\n",
    "# full_cell_type = full_cell_type.groupby(['group', 'name', 'depth', 'date', 'tetrode', 'unit_id']).agg({'cell_type': 'first'}).reset_index()\n",
    "\n",
    "os.environ['R_HOME'] = r'C:\\Program Files\\R\\R-4.3.1'\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "stats = importr('stats')\n",
    "\n",
    "# Assuming 'full_cell_type' is the DataFrame\n",
    "# contingency_table = pd.crosstab(full_cell_type['group'], full_cell_type['cell_type'])\n",
    "\n",
    "res = stats.fisher_test(contingency_table.to_numpy(), alternative='two-sided', hybrid=True, simulate_p_value=True)\n",
    "p_value = res[0][0]\n",
    "\n",
    "# Display the results\n",
    "print(contingency_table)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "\n",
    "# currently we have a csv with column for obj, trace, etc... But the filter method is based on having only one of obj, trace, etc.\n",
    "# our filter just checks that there are conscutive ssesions. SInce we have alr filtered out anything row but object, this works\n",
    "# but if we laeve all labels in, we need to count nmb of sessions that are obj, tract, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = ANT_cell_type_df.groupby(group_by_unique_cell_field).filter(filter_consecutive_sessions)\n",
    "# for ky in test.groupby(group_by_unique_cell).groups:\n",
    "#     ct = test.groupby(group_by_unique_cell).get_group(ky)['cell_type'].value_counts()\n",
    "#     print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cell_type['object'] = 'non-object'\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'object', 'object'] = 'object'\n",
    "\n",
    "full_cell_type['trace'] = 'non-trace'\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'trace', 'trace'] = 'trace'\n",
    "\n",
    "full_cell_type['trace_2'] = 'non-trace_2'\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'trace_2', 'trace_2'] = 'trace_2'\n",
    "\n",
    "full_cell_type['unassigned'] = 'assigned'\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'unassigned', 'unassigned'] = 'unassigned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Assuming 'full_cell_type' is the DataFrame\n",
    "groups = ['ANT', 'NON', 'B6']\n",
    "cell_types = ['object','trace','unassigned']\n",
    "cell_type_opp = ['non-object', 'non-trace', 'assigned']\n",
    "#  'trace', 'trace_2', 'unassigned']\n",
    "\n",
    "\n",
    "type_dict = {'ANT': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'B6': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'NON': {'object': 0, 'trace': 0, 'unassigned': 0}}\n",
    "\n",
    "inv_type_dict = {'ANT': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'B6': {'object': 0, 'trace': 0, 'unassigned': 0},\n",
    "                'NON': {'object': 0, 'trace': 0, 'unassigned': 0}}\n",
    "\n",
    "for ctype in identifier_dict:\n",
    "    for group in identifier_dict[ctype]:\n",
    "        type_dict[group][ctype] += len(identifier_dict[ctype][group])\n",
    "        for ctype2 in identifier_dict:\n",
    "            if ctype2 != ctype:\n",
    "                inv_type_dict[group][ctype] += len(identifier_dict[ctype2][group])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "comparisons = []\n",
    "odds_ratios = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i+1, len(groups)):\n",
    "        group1 = groups[i]\n",
    "        group2 = groups[j]\n",
    "        \n",
    "        ctype_count = 0\n",
    "        \n",
    "        for cell_type in cell_types:\n",
    "            \n",
    "            contingency_table_2x2 = pd.DataFrame({str(cell_type): [type_dict[group1][cell_type], type_dict[group2][cell_type]],\n",
    "                                    str(cell_type_opp[ctype_count]): [inv_type_dict[group1][cell_type], inv_type_dict[group2][cell_type]]},\n",
    "                                    index=[group1, group2])\n",
    "            print(contingency_table_2x2)\n",
    "            # Performing Fisher's Exact Test\n",
    "            odds_ratio, p_value = fisher_exact(contingency_table_2x2)\n",
    "\n",
    "            # Storing results\n",
    "            comparisons.append(f\"{group1} vs. {group2} - {cell_type.upper()}\")\n",
    "            odds_ratios.append(odds_ratio)\n",
    "            p_values.append(p_value)\n",
    "            ctype_count += 1\n",
    "\n",
    "# Adjust p-values for multiple comparisons using the Benjamini-Hochberg method\n",
    "rejected, adjusted_p_values, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05, is_sorted=False, returnsorted=False)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Accepted\": rejected,\n",
    "    \"Comparison\": comparisons,\n",
    "    \"Odds Ratio\": odds_ratios,\n",
    "    \"P-value\": p_values,\n",
    "    \"Adjusted P-value\": adjusted_p_values\n",
    "})\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist = [ses_cut_dict[3]['quality_type_dict'], ses_cut_dict[3]['center_type_dict'], ses_cut_dict[3]['ambiguous_type_dict']]\n",
    "dlist_inv = [quality_inv_type_dict, center_inv_type_dict, ambiguous_inv_type_dict]\n",
    "dflist = []\n",
    "\n",
    "ct = 0\n",
    "for ct in range(len(dlist)):\n",
    "    tp_dict = dlist[ct]\n",
    "    tp_inv_dict = dlist_inv[ct]\n",
    "    # Initialize empty lists to store results\n",
    "    comparisons = []\n",
    "    odds_ratios = []\n",
    "    p_values = []\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        for j in range(i+1, len(groups)):\n",
    "            group1 = groups[i]\n",
    "            group2 = groups[j]\n",
    "                        \n",
    "                \n",
    "            contingency_table_2x2 = pd.DataFrame({'excluded': [tp_dict[group1], tp_dict[group2]],\n",
    "                                    'included': [tp_inv_dict[group1], tp_inv_dict[group2]]},\n",
    "                                    index=[group1, group2])\n",
    "            print(contingency_table_2x2)\n",
    "            # Performing Fisher's Exact Test\n",
    "            odds_ratio, p_value = fisher_exact(contingency_table_2x2)\n",
    "            # Storing results\n",
    "            comparisons.append(f\"{group1} vs. {group2} - {cell_type.upper()}\")\n",
    "            odds_ratios.append(odds_ratio)\n",
    "            p_values.append(p_value)\n",
    "\n",
    "    # Adjust p-values for multiple comparisons using the Benjamini-Hochberg method\n",
    "    rejected, adjusted_p_values, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05, is_sorted=False, returnsorted=False)\n",
    "\n",
    "    # Display the results\n",
    "    res_df = pd.DataFrame({\n",
    "        \"Accepted\": rejected,\n",
    "        \"Comparison\": comparisons,\n",
    "        \"Odds Ratio\": odds_ratios,\n",
    "        \"P-value\": p_values,\n",
    "        \"Adjusted P-value\": adjusted_p_values\n",
    "    })\n",
    "\n",
    "    dflist.append(res_df)\n",
    "\n",
    "    print(res_df)\n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "comparisons = []\n",
    "odds_ratios = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i+1, len(groups)):\n",
    "        group1 = groups[i]\n",
    "        group2 = groups[j]\n",
    "        \n",
    "        ctype_count = 0\n",
    "        \n",
    "            \n",
    "        contingency_table_2x2 = pd.DataFrame({str(cell_type): [type_dict[group1], type_dict[group2]],\n",
    "                                str(cell_type_opp[ctype_count]): [inv_type_dict[group1], inv_type_dict[group2]]},\n",
    "                                index=[group1, group2])\n",
    "        print(contingency_table_2x2)\n",
    "        # Performing Fisher's Exact Test\n",
    "        odds_ratio, p_value = fisher_exact(contingency_table_2x2)\n",
    "        # Storing results\n",
    "        comparisons.append(f\"{group1} vs. {group2} - {cell_type.upper()}\")\n",
    "        odds_ratios.append(odds_ratio)\n",
    "        p_values.append(p_value)\n",
    "        ctype_count += 1\n",
    "\n",
    "# Adjust p-values for multiple comparisons using the Benjamini-Hochberg method\n",
    "rejected, adjusted_p_values, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05, is_sorted=False, returnsorted=False)\n",
    "\n",
    "# Display the results\n",
    "results_df_center = pd.DataFrame({\n",
    "    \"Accepted\": rejected,\n",
    "    \"Comparison\": comparisons,\n",
    "    \"Odds Ratio\": odds_ratios,\n",
    "    \"P-value\": p_values,\n",
    "    \"Adjusted P-value\": adjusted_p_values\n",
    "})\n",
    "\n",
    "print(results_df_ambiguous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # filter out rows where iso_dist is < 5 - Quality control\n",
    "    quality_dropped_identifiers = df_current[df_current['iso_dist'] < 5].groupby(group_by_unique_cell).groups.keys()\n",
    "    df_current = df_current[df_current['iso_dist'] >= 5]\n",
    "\n",
    "    # filter out rows where obj_q_NO is < obj_q - CLOSER to middle than a side\n",
    "    center_dropped_identifiers = df_current[df_current['obj_q_NO'] < df_current['obj_q']].groupby(group_by_unique_cell).groups.keys()\n",
    "    df_current = df_current[df_current['obj_q_NO'] >= df_current['obj_q']]\n",
    "\n",
    "    # filter out rows where obj_a for centroid is != obj_a for field - Ambiguous\n",
    "    ambiguous_dropped_identifiers = df_all_unmatched_field.groupby(group_by_unique_cell).groups.keys()\n",
    "    mask = pd.concat([(df_current['group'] == id1) & (df_current['name'] == id2) & (df_current['depth'] == id3) & (df_current['date'] == id4) & (df_current['tetrode'] == id5) & (df_current['unit_id'] == id6) for id1, id2, id3, id4, id5, id6 in ambiguous_dropped_identifiers], axis=1).any(axis=1)\n",
    "    df_current = df_current[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cell_type['object_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "full_cell_type = pd.concat([ANT_cell_type_df, B6_cell_type_df, NON_cell_type_df])\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'stable', 'cell_type'] = 'unassigned'\n",
    "full_cell_type.loc[full_cell_type['cell_type'] == 'trace_2', 'cell_type'] = 'trace'\n",
    "\n",
    "# full_cell_type.loc[full_cell_type['object_location'] == 'NO', 'object_location'] = int(360)\n",
    "#cast object location as int\n",
    "# full_cell_type['object_location'] = full_cell_type['object_location'].astype(int)\n",
    "\n",
    "# C(group, Treatment('B6')) +\n",
    "model = smf.mixedlm(\"field_area ~ C(group, Treatment('B6')) + C(cell_type, Treatment('unassigned'))\", data=full_cell_type, groups=\"name\")\n",
    "res = model.fit()\n",
    "print(res.summary())\n",
    "\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "import statsmodels.api as smf\n",
    "\n",
    "model2 = smf.MixedLM.from_formula(\"field_area ~ C(group, Treatment('B6')) + C(cell_type, Treatment('unassigned'))\", data=full_cell_type, groups=\"name\")\n",
    "res = RLM(model2.endog, model2.exog, M=sm.robust.norms.HuberT()).fit()\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plots\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(res.fittedvalues, res.resid)\n",
    "ax.set_xlabel('Fitted values')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residuals vs Fitted')\n",
    "plt.show()\n",
    "\n",
    "# normality of residuals\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sm.qqplot(res.resid, ax=ax, line='q', c='k', markersize=2)\n",
    "ax.set_xlabel('Theoretical Quantiles')\n",
    "ax.set_title('Normal Q-Q')\n",
    "plt.show()\n",
    "\n",
    "# Durbin-Watson Test\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "durbin_watson(res.resid)\n",
    "\n",
    "# RSS compute\n",
    "RSS = np.sum(res.resid ** 2)\n",
    "# RMSE \n",
    "RMSE = np.sqrt(RSS / len(res.resid))\n",
    "# AIC compute\n",
    "AIC = len(res.resid) * np.log(RSS / len(res.resid)) + 2 * len(res.params)\n",
    "# BIC compute\n",
    "BIC = len(res.resid) * np.log(RSS / len(res.resid)) + len(res.params) * np.log(len(res.resid))\n",
    "\n",
    "print('RSS: {}'.format(RSS))\n",
    "print('RMSE: {}'.format(RMSE))\n",
    "print('AIC: {}'.format(AIC))\n",
    "print('BIC: {}'.format(BIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2[df2['spike_count'] > 100]\n",
    "# df2 = df2[df2['information'] > 0.25]\n",
    "# df2 = df2[df2['selectivity'] > 5]\n",
    "# df2 = df2[df2['iso_dist'] > 5]\n",
    "# df2 = df2[df2['firing_rate'] < 20]\n",
    "# df2 = df2[df2['spike_width'] > 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ANT_object_cell_df[ANT_object_cell_df['obj_q'] <= ANT_object_cell_df['field_coverage']])\n",
    "\n",
    "# len(ANT_object_cell_df[ANT_object_cell_df['obj_q'].astype(float) <= ANT_object_cell_df['field_coverage'].astype(float)].groupby(group_by)) / sum_ANT * 100,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1154 + 676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ANT_object_cell_df[ANT_object_cell_df['obj_q'] > ANT_object_cell_df['field_coverage']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ANT_object_cell_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ANT = ANT_df[ANT_df['object_location'].astype(str) != 'NO']\n",
    "sum_ANT = sum_ANT.groupby(group_by).filter(lambda x: len(x) >= consecutive_sessions_threshold).groupby(group_by)\n",
    "len(sum_ANT)\n",
    "\n",
    "# sum_ANT = len(sum_ANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ANT_df.groupby(group_by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['object_location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(to_keep).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['score'] == 'field']\n",
    "\n",
    "# # where object_location is NO and session_id != session_1\n",
    "temp2 = temp[temp['object_location'].astype(str) == 'NO']\n",
    "# temp2[temp2['session_id'] == 'session_2']\n",
    "temp2['session_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,8):\n",
    "    print('session_'+str(i))\n",
    "    print(len(temp2[temp2['session_id'] == 'session_' + str(i)]))\n",
    "    use = temp2[temp2['session_id'] == 'session_' + str(i)]\n",
    "    print('ANT: ' + str(len(use[use['group'] == 'ANT'])) + ' NON: ' + str(len(use[use['group'] == 'NON'])) + ' B6: ' + str(len(use[use['group'] == 'B6'])))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use['group'] == 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANT_object_cell_df = ANT_df[ANT_df['obj_q'] <= ANT_df['field_coverage']]\n",
    "ANT_object_cell_df[['object_location','obj_a','field_coverage','obj_q_0','obj_q_90','obj_q_180','obj_q_270']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cell_df = df[df['score'] == score]\n",
    "object_cell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} object cells in ANT'.format(len(ANT_object_cell_df.groupby(group_by))))\n",
    "print('There are {} object cells in B6'.format(len(B6_object_cell_df.groupby(group_by))))\n",
    "print('There are {} object cells in NON'.format(len(NON_object_cell_df.groupby(group_by))))\n",
    "\n",
    "ANT_unique = ANT_object_cell_df[group_by].drop_duplicates()\n",
    "print('The object cells in ANT are {}'.format(ANT_unique))\n",
    "B6_unique = B6_object_cell_df[group_by].drop_duplicates()\n",
    "print('The object cells in B6 are {}'.format(B6_unique))\n",
    "NON_unique = NON_object_cell_df[group_by].drop_duplicates()\n",
    "print('The object cells in NON are {}'.format(NON_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['obj_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['object_location']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
