{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from math import ceil\n",
    "import cv2\n",
    "import ot\n",
    "import itertools\n",
    "\n",
    "PROJECT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "# PROJECT_PATH = os.getcwd()\n",
    "sys.path.append(os.path.dirname(PROJECT_PATH))\n",
    "\n",
    "from _prototypes.cell_remapping.src.remapping import pot_sliced_wasserstein\n",
    "from _prototypes.cell_remapping.src.wasserstein_distance import _get_ratemap_bucket_midpoints\n",
    "from library.map_utils import _temp_occupancy_map, _temp_spike_map, _speed2D, _speed_bins, _interpolate_matrix\n",
    "from core.spatial import Position2D\n",
    "\n",
    "unit_matcher_path = os.getcwd()\n",
    "prototype_path = os.path.abspath(os.path.join(unit_matcher_path, os.pardir))\n",
    "project_path = os.path.abspath(os.path.join(prototype_path, os.pardir))\n",
    "lab_path = os.path.abspath(os.path.join(project_path, os.pardir))\n",
    "sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_io.rw.axona.batch_read import make_study\n",
    "from _prototypes.cell_remapping.src.settings import settings_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEC path\n",
    "\n",
    "project_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\MEC_GAR2\\Frontiers_examples2_GAR\"\n",
    "\n",
    "studies = []\n",
    "study_folders = []\n",
    "\n",
    "settings_dict['smoothing_factor'] = 2\n",
    "settings_dict['ppm'] = 665\n",
    "settings_dict['useMatchedCut'] = False\n",
    "study = make_study(project_path,settings_dict)\n",
    "study.make_animals()\n",
    "studies.append(study)\n",
    "# study_folders.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies[0].animals[0].sessions['session_1'].get_spike_data()['spike_cluster'].get_unique_cluster_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _prototypes.cell_remapping.src.masks import make_object_ratemap, check_disk_arena, flat_disk_mask, generate_grid, _sample_grid\n",
    "from library.hafting_spatial_maps import SpatialSpikeTrain2D\n",
    "from library.maps import map_blobs\n",
    "from library.scores import hd_score, grid_score, border_score\n",
    "from library.scores import rate_map_stats, rate_map_coherence, speed_score\n",
    "import scipy as sio\n",
    "\n",
    "def scale_points(pts):\n",
    "    # Separate the x and y coordinates\n",
    "    curr_spike_pos_x = pts[:, 0]\n",
    "    curr_spike_pos_y = pts[:, 1]\n",
    "\n",
    "    # Compute the minimum and maximum values for x and y coordinates\n",
    "    min_x = np.min(curr_spike_pos_x)\n",
    "    max_x = np.max(curr_spike_pos_x)\n",
    "    min_y = np.min(curr_spike_pos_y)\n",
    "    max_y = np.max(curr_spike_pos_y)\n",
    "\n",
    "    # Perform Min-Max scaling separately for x and y coordinates\n",
    "    scaled_x = (curr_spike_pos_x - min_x) / (max_x - min_x)\n",
    "    scaled_y = (curr_spike_pos_y - min_y) / (max_y - min_y)\n",
    "\n",
    "    # Combine the scaled x and y coordinates\n",
    "    scaled_pts = np.column_stack((scaled_x, scaled_y))\n",
    "\n",
    "    return scaled_pts\n",
    "\n",
    "gar_cell_dict = {}\n",
    "blobs_cell_dict = {}\n",
    "scores_cell_dict = {}\n",
    "func_scores_dict = {}\n",
    "\n",
    "for study in studies:\n",
    "    for animal in study.animals:\n",
    "        try:\n",
    "            max_matched_cell_count = max(list(map(lambda x: max(animal.sessions[x].get_cell_data()['cell_ensemble'].get_label_ids()), animal.sessions)))\n",
    "        except:\n",
    "            max_matched_cell_count = 0\n",
    "            for x in animal.sessions:\n",
    "                cell_label_ids = animal.sessions[x].get_cell_data()['cell_ensemble'].get_label_ids() \n",
    "                nmb_matched = len(cell_label_ids)\n",
    "                if nmb_matched > max_matched_cell_count:\n",
    "                    max_matched_cell_count = nmb_matched\n",
    "        for k in range(int(max_matched_cell_count)):\n",
    "            cell_label = k + 1\n",
    "            prev = None\n",
    "            for i in range(len(list(animal.sessions.keys()))):\n",
    "                seskey = 'session_' + str(i+1)\n",
    "                ses = animal.sessions[seskey]\n",
    "                path = ses.session_metadata.file_paths['tet']\n",
    "                fname = path.split('/')[-1].split('.')[0]\n",
    "                ensemble = animal.sessions[seskey].get_cell_data()['cell_ensemble']\n",
    "\n",
    "                if cell_label in ensemble.get_cell_label_dict():\n",
    "                    cell = ensemble.get_cell_by_id(cell_label)\n",
    "                    \n",
    "                    pos_obj = ses.get_position_data()['position']\n",
    "                    spatial_spike_train = ses.make_class(SpatialSpikeTrain2D, {'cell': cell, 'position': pos_obj})\n",
    "\n",
    "                    rate_map_obj = spatial_spike_train.get_map('rate')\n",
    "\n",
    "                    rate_map, _ = rate_map_obj.get_rate_map(new_size = 32)\n",
    "\n",
    "                    if 'cylinder' in fname.lower():\n",
    "                        cylinder = True\n",
    "                    else:\n",
    "                        cylinder = False\n",
    "\n",
    "                    image, n_labels, labels, centroids, field_sizes = map_blobs(spatial_spike_train, ratemap_size=32, cylinder=cylinder, \n",
    "                                                                                downsample=False, downsample_factor=None)\n",
    "                    \n",
    "                    # Disk mask ratemap\n",
    "                    if cylinder:\n",
    "                        curr = flat_disk_mask(rate_map)\n",
    "                        curr_ratemap = curr\n",
    "                        row, col = np.where(~np.isnan(curr_ratemap))\n",
    "                        disk_ids = np.array([row, col]).T\n",
    "                    else:\n",
    "                        curr = rate_map\n",
    "                        curr_ratemap = curr\n",
    "                        disk_ids = None\n",
    "                    gscore = grid_score(spatial_spike_train)\n",
    "                    bscore = border_score(spatial_spike_train)\n",
    "                    ratemap_stats_dict = rate_map_stats(spatial_spike_train)\n",
    "                    si_score = ratemap_stats_dict['spatial_information_content']\n",
    "                    if prev is not None:\n",
    "                        prev_ratemap = prev\n",
    "                        curr_spatial_spike_train = spatial_spike_train\n",
    "                        y, x = prev_ratemap.shape\n",
    "                        # prev_spike_pos_x, prev_spike_pos_y, prev_spike_pos_t = prev_spatial_spike_train.spike_x, prev_spatial_spike_train.spike_y, prev_spatial_spike_train.new_spike_times\n",
    "                        # prev_pts = np.array([prev_spike_pos_x, prev_spike_pos_y])\n",
    "                        # curr_spike_pos_x, curr_spike_pos_y, curr_spike_pos_t = curr_spatial_spike_train.spike_x, curr_spatial_spike_train.spike_y, curr_spatial_spike_train.new_spike_times\n",
    "                        # curr_pts = np.array([curr_spike_pos_x, curr_spike_pos_y]).T\n",
    "                        # find indices of not nan \n",
    "                        row_prev, col_prev = np.where(~np.isnan(prev_ratemap))\n",
    "                        row_curr, col_curr = np.where(~np.isnan(curr_ratemap))\n",
    "\n",
    "                        prev_height_bucket_midpoints, prev_width_bucket_midpoints = _get_ratemap_bucket_midpoints(prev_spatial_spike_train.arena_size, y, x)\n",
    "                        curr_height_bucket_midpoints, curr_width_bucket_midpoints = _get_ratemap_bucket_midpoints(curr_spatial_spike_train.arena_size, y, x)\n",
    "                                    \n",
    "                        prev_height_bucket_midpoints = prev_height_bucket_midpoints[row_prev]\n",
    "                        prev_width_bucket_midpoints = prev_width_bucket_midpoints[col_prev]\n",
    "                        curr_height_bucket_midpoints = curr_height_bucket_midpoints[row_curr]\n",
    "                        curr_width_bucket_midpoints = curr_width_bucket_midpoints[col_curr]\n",
    "                        source_weights = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_prev, col_prev)))\n",
    "                        target_weights = np.array(list(map(lambda x, y: curr_ratemap[x,y], row_curr, col_curr)))\n",
    "                        source_weights = source_weights / np.sum(source_weights)\n",
    "                        target_weights = target_weights / np.sum(target_weights)\n",
    "                        coord_buckets_curr = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr)))\n",
    "                        coord_buckets_prev = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_prev, col_prev)))\n",
    "\n",
    "                        # curr_pts = scale_points(curr_pts)\n",
    "                        # prev_pts = scale_points(prev_pts)\n",
    "                        # spike_dens_wass = pot_sliced_wasserstein(prev_pts, curr_pts, n_projections=10**0)\n",
    "                        wass = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr, source_weights, target_weights, n_projections=10**0)\n",
    "                        corr = pearsonr(source_weights, target_weights)[0]\n",
    "\n",
    "                        wass_shifts = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_center = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_center = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_center_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_center_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        print('looping')\n",
    "                        for i in range(prev_ratemap.shape[0]):\n",
    "                            for j in range(prev_ratemap.shape[1]):\n",
    "                                shift = (i, j)\n",
    "                                # # shift with nan\n",
    "                                \n",
    "                                wrap_map = np.roll(curr_ratemap, shift=shift, axis=(0, 1))\n",
    "                                shift_center = (i - int(np.floor(prev_ratemap.shape[0] / 2)), j - int(np.floor(prev_ratemap.shape[1] / 2)))\n",
    "                                wrap_map_center = np.roll(curr_ratemap, shift=shift_center, axis=(0, 1))\n",
    "                                # midp = int(np.floor(prev_ratemap.shape[0] / 2))\n",
    "                                # shift = (midp - i, midp - j)\n",
    "                                shifted_map = sio.ndimage.shift(curr_ratemap, shift, mode='constant', cval=0)\n",
    "                                shifted_map_center = sio.ndimage.shift(curr_ratemap, shift_center, mode='constant', cval=0)\n",
    "                                row_curr, col_curr = np.where(~np.isnan(shifted_map))\n",
    "                                curr_height_bucket_midpoints, curr_width_bucket_midpoints = _get_ratemap_bucket_midpoints(curr_spatial_spike_train.arena_size, y, x)\n",
    "                                curr_height_bucket_midpoints = curr_height_bucket_midpoints[row_curr]\n",
    "                                curr_width_bucket_midpoints = curr_width_bucket_midpoints[col_curr]\n",
    "                                target_weights = np.array(list(map(lambda x, y: shifted_map[x,y], row_curr, col_curr)))\n",
    "                                target_weights_wrap = np.array(list(map(lambda x, y: wrap_map[x,y], row_curr, col_curr)))\n",
    "                                target_weights_center = np.array(list(map(lambda x, y: shifted_map_center[x,y], row_curr, col_curr)))\n",
    "                                target_weights_wrap_center = np.array(list(map(lambda x, y: wrap_map_center[x,y], row_curr, col_curr)))\n",
    "                                source_weights_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_curr, col_curr)))\n",
    "                                target_weights = target_weights / np.sum(target_weights)\n",
    "                                target_weights_wrap = target_weights_wrap / np.sum(target_weights_wrap)\n",
    "                                target_weights_center = target_weights_center / np.sum(target_weights_center)\n",
    "                                target_weights_wrap_center = target_weights_wrap_center / np.sum(target_weights_wrap_center)\n",
    "                                source_weights_pearson = source_weights_pearson / np.sum(source_weights_pearson)\n",
    "\n",
    "                                wass_shift = pot_sliced_wasserstein(coord_buckets_prev, np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr))), source_weights, target_weights, n_projections=10**0)\n",
    "                                corr_shift = pearsonr(source_weights_pearson, target_weights)[0]\n",
    "                                wass_wrap = pot_sliced_wasserstein(coord_buckets_prev, np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr))), source_weights, target_weights_wrap, n_projections=10**0)\n",
    "                                corr_wrap = pearsonr(source_weights_pearson, target_weights_wrap)[0]\n",
    "                                wass_shift_center = pot_sliced_wasserstein(coord_buckets_prev, np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr))), source_weights, target_weights_center, n_projections=10**0)\n",
    "                                corr_shift_center = pearsonr(source_weights_pearson, target_weights_center)[0]\n",
    "                                wass_wrap_center = pot_sliced_wasserstein(coord_buckets_prev, np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr))), source_weights, target_weights_wrap_center, n_projections=10**0)\n",
    "                                corr_wrap_center = pearsonr(source_weights_pearson, target_weights_wrap_center)[0]\n",
    "                        \n",
    "                                wass_shifts[i, j] = wass_shift\n",
    "                                corr_shifts[i, j] = corr_shift\n",
    "                                wass_shifts_wrap[i, j] = wass_wrap\n",
    "                                corr_shifts_wrap[i, j] = corr_wrap\n",
    "                                wass_shifts_center[i, j] = wass_shift_center\n",
    "                                corr_shifts_center[i, j] = corr_shift_center\n",
    "                                wass_shifts_center_wrap[i, j] = wass_wrap_center\n",
    "                                corr_shifts_center_wrap[i, j] = corr_wrap_center\n",
    "  \n",
    "                                # corr_shifts_center_wrap[shift_center[0], shift_center[1]] = corr_wrap_center\n",
    "                                \n",
    "                    title = animal.animal_id + '_' + seskey.replace('_', '') + '_' + str(cell_label)\n",
    "\n",
    "                    gar_cell_dict[title] = curr_ratemap\n",
    "                    blobs_cell_dict[title] = [labels, centroids]\n",
    "                    if prev is not None:\n",
    "                        scores_cell_dict[title] = [wass, corr, wass_shifts, corr_shifts, wass_shifts_wrap, corr_shifts_wrap,\n",
    "                                                    wass_shifts_center, corr_shifts_center, wass_shifts_center_wrap, corr_shifts_center_wrap]\n",
    "\n",
    "                    func_scores_dict[title] = [gscore, bscore, si_score]\n",
    "\n",
    "                    prev = curr_ratemap\n",
    "                    prev_spatial_spike_train = spatial_spike_train\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar_cell_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal.animal_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c1u1ky = ['MEC_GAR2_tet1_session1_1', 'MEC_GAR2_tet1_session2_1']\n",
    "# c1u2ky = ['MEC_GAR2_tet1_session1_2', 'MEC_GAR2_tet1_session2_2']\n",
    "# c1u3ky = ['MEC_GAR2_tet1_session1_3', 'MEC_GAR2_tet1_session2_3']\n",
    "# c1u4ky = ['MEC_GAR2_tet1_session1_4', 'MEC_GAR2_tet1_session2_4']\n",
    "\n",
    "# c2u1ky = ['MEC_GAR2_tet2_session1_1', 'MEC_GAR2_tet2_session2_1']\n",
    "# c2u2ky = ['MEC_GAR2_tet2_session1_2', 'MEC_GAR2_tet2_session2_2']\n",
    "# c2u3ky = ['MEC_GAR2_tet2_session1_3', 'MEC_GAR2_tet2_session2_3']\n",
    "# c2u4ky = ['MEC_GAR2_tet2_session1_4', 'MEC_GAR2_tet2_session2_4']\n",
    "# c2u5ky = ['MEC_GAR2_tet2_session1_5', 'MEC_GAR2_tet2_session2_5']\n",
    "# c2u6ky = ['MEC_GAR2_tet2_session1_6', 'MEC_GAR2_tet2_session2_6']\n",
    "# c2u7ky = ['MEC_GAR2_tet2_session1_7', 'MEC_GAR2_tet2_session2_7']\n",
    "\n",
    "# c1u1 = [gar_cell_dict[x] for x in c1u1ky]\n",
    "# c1u2 = [gar_cell_dict[x] for x in c1u2ky]\n",
    "# c1u3 = [gar_cell_dict[x] for x in c1u3ky]\n",
    "# c1u4 = [gar_cell_dict[x] for x in c1u4ky]\n",
    "\n",
    "# c2u1 = [gar_cell_dict[x] for x in c2u1ky]\n",
    "# c2u2 = [gar_cell_dict[x] for x in c2u2ky]\n",
    "# c2u3 = [gar_cell_dict[x] for x in c2u3ky]\n",
    "# c2u4 = [gar_cell_dict[x] for x in c2u4ky]\n",
    "# c2u5 = [gar_cell_dict[x] for x in c2u5ky]\n",
    "# c2u6 = [gar_cell_dict[x] for x in c2u6ky]\n",
    "# c2u7 = [gar_cell_dict[x] for x in c2u7ky]\n",
    "\n",
    "# units = [c1u1, c1u2, c1u3, c1u4, c2u1, c2u2, c2u3, c2u4, c2u5, c2u6, c2u7]\n",
    "# unit_keys = [c1u1ky, c1u2ky, c1u3ky, c1u4ky, c2u1ky, c2u2ky, c2u3ky, c2u4ky, c2u5ky, c2u6ky, c2u7ky]\n",
    "# unit_titles = ['MEC Tet 1 Unit 1', 'MEC Tet 1 Unit 2', 'MEC Tet 1 Unit 3', 'MEC Tet 1 Unit 4', 'MEC Tet 2 Unit 1', 'MEC Tet 2 Unit 2', 'MEC Tet 2 Unit 3', 'MEC Tet 2 Unit 4', 'MEC Tet 2 Unit 5', 'MEC Tet 2 Unit 6', 'MEC Tet 2 Unit 7']\n",
    "\n",
    "cky = ['Frontiers_examples2_GAR_tet1_session1_1', 'Frontiers_examples2_GAR_tet1_session2_1']\n",
    "cu = [gar_cell_dict[x] for x in cky]\n",
    "unit_keys = [cky]\n",
    "unit_titles = ['MEC Tet 1 Unit 1']\n",
    "units = [cu]\n",
    "\n",
    "for i in range(len(units)):\n",
    "    unit = units[i]\n",
    "    unit_title = unit_titles[i]\n",
    "    wass, corr, wass_shifts, corr_shifts, wass_shifts_wrap, corr_shifts_wrap, wass_shifts_center, corr_shifts_center, wass_shifts_center_wrap, corr_shifts_center_wrap = scores_cell_dict[unit_keys[i][1]]\n",
    "    prev_gscore, prev_bscore, prev_si_score = func_scores_dict[unit_keys[i][0]]\n",
    "    curr_gscore, curr_bscore, curr_si_score = func_scores_dict[unit_keys[i][1]]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    toplot = unit[0]\n",
    "\n",
    "    ax = fig.add_subplot(2,4,1)\n",
    "    img = ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 1 (Reference)')\n",
    "\n",
    "    toplot = unit[1]\n",
    "    toplot_copy = sio.ndimage.shift(toplot, (8, 8), mode='constant', cval=np.nan)\n",
    "    toplot = sio.ndimage.shift(toplot, (8, 8), mode='constant', cval=np.nan)\n",
    "    ax = fig.add_subplot(2,4,5)\n",
    "    img = ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (No wrap)')\n",
    "    not_nan = ~np.isnan(toplot_copy)\n",
    "    is_nan = np.isnan(toplot_copy)\n",
    "    toplot_copy[is_nan] = 0\n",
    "    toplot_copy[not_nan] = np.nan\n",
    "    ax.imshow(toplot_copy, cmap='jet', aspect='auto')\n",
    "\n",
    "\n",
    "    toplot = unit[1]\n",
    "    toplot = np.roll(toplot, shift=(8, 8), axis=(0, 1))\n",
    "    ax = fig.add_subplot(2,4,6)\n",
    "    img = ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (Wrap)')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,2)\n",
    "    img = ax.imshow(unit[1], cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (Shifting)')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,5)\n",
    "    # img = ax.imshow(wass_shifts, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('')\n",
    "    # ax.set_title('EMD corner shift, no wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,3)\n",
    "    img = ax.imshow(wass_shifts_center, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('')\n",
    "    ax.set_title('EMD center shift, no wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,6)\n",
    "    # img = ax.imshow(corr_shifts, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('Correlation')\n",
    "    # ax.set_title('Pearson-R corner shift, no wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,4)\n",
    "    img = ax.imshow(corr_shifts_center, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Pearson-R center shift, no wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,11)\n",
    "    # img = ax.imshow(wass_shifts_wrap, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('')\n",
    "    # ax.set_title('EMD corner shift & wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,7)\n",
    "    img = ax.imshow(wass_shifts_center_wrap, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('')\n",
    "    ax.set_title('EMD center shift & wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,12)\n",
    "    # img = ax.imshow(corr_shifts_wrap, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('Correlation')\n",
    "    # ax.set_title('Pearson-R corner shift & wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,8)\n",
    "    img = ax.imshow(corr_shifts_center_wrap, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Pearson-R center shift & wrap')\n",
    "\n",
    "    fig.suptitle(unit_title + ' | Grid: (' + str(round(prev_gscore, 3)) + ', ' + str(round(curr_gscore, 3)) + \n",
    "                 ') | Border: (' + str(round(max(prev_bscore), 3)) + ', ' + str(round(max(curr_bscore), 3)) + \n",
    "                 ') | SI: (' + str(round(prev_si_score, 3)) + ', ' + str(round(curr_si_score, 3)) + ')',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # fig.suptitle(unit_title + ' | Grid: ' + str(round(gscore, 3)) + ' | Border: ' + str(round(max(bscore), 3)) + ' | SI: ' + str(round(si_score, 3)))\n",
    "    #' | Wass: ' + str(round(wass, 3)) + ' | Corr: ' + str(round(corr, 3)) + \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cell_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
