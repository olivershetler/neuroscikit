{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from math import ceil\n",
    "import cv2\n",
    "import ot\n",
    "import itertools\n",
    "\n",
    "PROJECT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "# PROJECT_PATH = os.getcwd()\n",
    "sys.path.append(os.path.dirname(PROJECT_PATH))\n",
    "\n",
    "from _prototypes.cell_remapping.src.remapping import pot_sliced_wasserstein\n",
    "from _prototypes.cell_remapping.src.wasserstein_distance import _get_ratemap_bucket_midpoints\n",
    "from library.map_utils import _temp_occupancy_map, _temp_spike_map, _speed2D, _speed_bins, _interpolate_matrix\n",
    "from core.spatial import Position2D\n",
    "\n",
    "unit_matcher_path = os.getcwd()\n",
    "prototype_path = os.path.abspath(os.path.join(unit_matcher_path, os.pardir))\n",
    "project_path = os.path.abspath(os.path.join(prototype_path, os.pardir))\n",
    "lab_path = os.path.abspath(os.path.join(project_path, os.pardir))\n",
    "sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_io.rw.axona.batch_read import make_study\n",
    "from _prototypes.cell_remapping.src.settings import settings_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEC path\n",
    "\n",
    "project_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\LC_RAD\"\n",
    "\n",
    "studies = []\n",
    "study_folders = []\n",
    "\n",
    "for folder in os.listdir(project_path):\n",
    "    settings_dict['smoothing_factor'] = 3\n",
    "    settings_dict['ppm'] = 511\n",
    "    settings_dict['useMatchedCut'] = True\n",
    "\n",
    "    project_path_use = os.path.join(project_path,folder)\n",
    "    study = make_study(project_path_use,settings_dict)\n",
    "    study.make_animals()\n",
    "    studies.append(study)\n",
    "    study_folders.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studies[0].animals[0].sessions['session_1'].get_spike_data()['spike_cluster'].get_unique_cluster_labels()\n",
    "studies[0].animals[0].sessions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _prototypes.cell_remapping.src.masks import make_object_ratemap, check_disk_arena, flat_disk_mask, generate_grid, _sample_grid\n",
    "from library.hafting_spatial_maps import SpatialSpikeTrain2D\n",
    "from library.maps import map_blobs\n",
    "from library.scores import hd_score, grid_score, border_score\n",
    "from library.scores import rate_map_stats, rate_map_coherence, speed_score\n",
    "import scipy as sio\n",
    "\n",
    "def scale_points(pts):\n",
    "    # Separate the x and y coordinates\n",
    "    curr_spike_pos_x = pts[:, 0]\n",
    "    curr_spike_pos_y = pts[:, 1]\n",
    "\n",
    "    # Compute the minimum and maximum values for x and y coordinates\n",
    "    min_x = np.min(curr_spike_pos_x)\n",
    "    max_x = np.max(curr_spike_pos_x)\n",
    "    min_y = np.min(curr_spike_pos_y)\n",
    "    max_y = np.max(curr_spike_pos_y)\n",
    "\n",
    "    # Perform Min-Max scaling separately for x and y coordinates\n",
    "    scaled_x = (curr_spike_pos_x - min_x) / (max_x - min_x)\n",
    "    scaled_y = (curr_spike_pos_y - min_y) / (max_y - min_y)\n",
    "\n",
    "    # Combine the scaled x and y coordinates\n",
    "    scaled_pts = np.column_stack((scaled_x, scaled_y))\n",
    "\n",
    "    return scaled_pts\n",
    "\n",
    "cell_dict = {}\n",
    "blobs_cell_dict = {}\n",
    "scores_cell_dict = {}\n",
    "func_scores_dict = {}\n",
    "\n",
    "for study in studies:\n",
    "    for animal in study.animals:\n",
    "        try:\n",
    "            max_matched_cell_count = max(list(map(lambda x: max(animal.sessions[x].get_cell_data()['cell_ensemble'].get_label_ids()), animal.sessions)))\n",
    "        except:\n",
    "            max_matched_cell_count = 0\n",
    "            for x in animal.sessions:\n",
    "                cell_label_ids = animal.sessions[x].get_cell_data()['cell_ensemble'].get_label_ids() \n",
    "                nmb_matched = len(cell_label_ids)\n",
    "                if nmb_matched > max_matched_cell_count:\n",
    "                    max_matched_cell_count = nmb_matched\n",
    "        for k in range(int(max_matched_cell_count)):\n",
    "            cell_label = k + 1\n",
    "            prev = None\n",
    "            for i in range(len(list(animal.sessions.keys()))):\n",
    "                seskey = 'session_' + str(i+1)\n",
    "                ses = animal.sessions[seskey]\n",
    "                path = ses.session_metadata.file_paths['tet']\n",
    "                fname = path.split('/')[-1].split('.')[0]\n",
    "                ensemble = animal.sessions[seskey].get_cell_data()['cell_ensemble']\n",
    "\n",
    "                if cell_label in ensemble.get_cell_label_dict():\n",
    "                    cell = ensemble.get_cell_by_id(cell_label)\n",
    "                    \n",
    "                    pos_obj = ses.get_position_data()['position']\n",
    "                    spatial_spike_train = ses.make_class(SpatialSpikeTrain2D, {'cell': cell, 'position': pos_obj})\n",
    "\n",
    "                    rate_map_obj = spatial_spike_train.get_map('rate')\n",
    "\n",
    "                    rate_map, _ = rate_map_obj.get_rate_map(new_size = 32)\n",
    "\n",
    "                    if 'cylinder' in fname.lower():\n",
    "                        cylinder = True\n",
    "                    else:\n",
    "                        cylinder = False\n",
    "\n",
    "                    image, n_labels, labels, centroids, field_sizes = map_blobs(spatial_spike_train, ratemap_size=32, cylinder=cylinder, \n",
    "                                                                                downsample=False, downsample_factor=None)\n",
    "                    \n",
    "                    # Disk mask ratemap\n",
    "                    if cylinder:\n",
    "                        non_flat_curr = np.copy(rate_map)\n",
    "                        curr = flat_disk_mask(rate_map)\n",
    "                        # curr[curr != curr] = 0\n",
    "                        curr_ratemap = curr\n",
    "                        row, col = np.where(~np.isnan(curr_ratemap))\n",
    "                        disk_ids = np.array([row, col]).T\n",
    "                    else:\n",
    "                        curr = rate_map\n",
    "                        curr_ratemap = curr\n",
    "                        disk_ids = None\n",
    "\n",
    "                    # plt.imshow(curr_ratemap, cmap='jet')\n",
    "                    # plt.title(fname + ' ' + str(cell_label) + ' ' + seskey)\n",
    "                    # plt.show()\n",
    "                    # stop()\n",
    "                    gscore = grid_score(spatial_spike_train)\n",
    "                    bscore = border_score(spatial_spike_train)\n",
    "                    ratemap_stats_dict = rate_map_stats(spatial_spike_train)\n",
    "                    si_score = ratemap_stats_dict['spatial_information_content']\n",
    "                    if prev is not None:\n",
    "                        prev_ratemap = prev\n",
    "                        curr_spatial_spike_train = spatial_spike_train\n",
    "                        y, x = prev_ratemap.shape\n",
    "                        # prev_spike_pos_x, prev_spike_pos_y, prev_spike_pos_t = prev_spatial_spike_train.spike_x, prev_spatial_spike_train.spike_y, prev_spatial_spike_train.new_spike_times\n",
    "                        # prev_pts = np.array([prev_spike_pos_x, prev_spike_pos_y])\n",
    "                        # curr_spike_pos_x, curr_spike_pos_y, curr_spike_pos_t = curr_spatial_spike_train.spike_x, curr_spatial_spike_train.spike_y, curr_spatial_spike_train.new_spike_times\n",
    "                        # curr_pts = np.array([curr_spike_pos_x, curr_spike_pos_y]).T\n",
    "                        # find indices of not nan \n",
    "                        row_prev, col_prev = np.where(~np.isnan(prev_ratemap))\n",
    "                        row_curr, col_curr = np.where(~np.isnan(curr_ratemap))\n",
    "\n",
    "                        prev_height_bucket_midpoints, prev_width_bucket_midpoints = _get_ratemap_bucket_midpoints(prev_spatial_spike_train.arena_size, y, x)\n",
    "                        curr_height_bucket_midpoints, curr_width_bucket_midpoints = _get_ratemap_bucket_midpoints(curr_spatial_spike_train.arena_size, y, x)\n",
    "                                    \n",
    "                        prev_height_bucket_midpoints = prev_height_bucket_midpoints[row_prev]\n",
    "                        prev_width_bucket_midpoints = prev_width_bucket_midpoints[col_prev]\n",
    "                        curr_height_bucket_midpoints = curr_height_bucket_midpoints[row_curr]\n",
    "                        curr_width_bucket_midpoints = curr_width_bucket_midpoints[col_curr]\n",
    "                        source_weights = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_prev, col_prev)))\n",
    "                        target_weights = np.array(list(map(lambda x, y: curr_ratemap[x,y], row_curr, col_curr)))\n",
    "                        if np.sum(source_weights) != 0:\n",
    "                            source_weights = source_weights / np.sum(source_weights)\n",
    "                        if np.sum(target_weights) != 0:\n",
    "                            target_weights = target_weights / np.sum(target_weights)\n",
    "                        coord_buckets_curr = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr)))\n",
    "                        coord_buckets_prev = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_prev, col_prev)))\n",
    "\n",
    "                        # curr_pts = scale_points(curr_pts)\n",
    "                        # prev_pts = scale_points(prev_pts)\n",
    "                        # spike_dens_wass = pot_sliced_wasserstein(prev_pts, curr_pts, n_projections=10**3)\n",
    "                        wass = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr, source_weights, target_weights, n_projections=10**3)\n",
    "                        \n",
    "                        # if len(source_weights) != len(target_weights):\n",
    "                            # row_use = np.argmin([len(source_weights), len(target_weights)])\n",
    "                            # row_use = [row_prev, row_curr][row_use]\n",
    "                            # col_use = np.argmin([len(source_weights), len(target_weights)])\n",
    "                            # col_use = [col_prev, col_curr][col_use]\n",
    "                        row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "                        row_not_nan_target = np.where(~np.isnan(curr_ratemap))[0]\n",
    "                        col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "                        col_not_nan_target = np.where(~np.isnan(curr_ratemap))[1]\n",
    "                        row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "                        row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "                        row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "                        row_use = [x[0] for x in row_col_use]\n",
    "                        col_use = [x[1] for x in row_col_use]                        \n",
    "                        source_weights = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_use, col_use)))\n",
    "                        target_weights = np.array(list(map(lambda x, y: curr_ratemap[x,y], row_use, col_use)))\n",
    "                        if np.sum(source_weights) != 0:\n",
    "                            source_weights = source_weights / np.sum(source_weights)\n",
    "                        if np.sum(target_weights) != 0:\n",
    "                            target_weights = target_weights / np.sum(target_weights)\n",
    "\n",
    "                        corr = pearsonr(source_weights, target_weights)[0]\n",
    "\n",
    "                        wass_shifts = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts[:] = np.nan\n",
    "                        corr_shifts = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts[:] = np.nan\n",
    "                        wass_shifts_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_wrap[:] = np.nan\n",
    "                        corr_shifts_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_wrap[:] = np.nan\n",
    "                        wass_shifts_center = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_center[:] = np.nan\n",
    "                        corr_shifts_center = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_center[:] = np.nan\n",
    "                        wass_shifts_center_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        wass_shifts_center_wrap[:] = np.nan\n",
    "                        corr_shifts_center_wrap = np.zeros((prev_ratemap.shape[0], prev_ratemap.shape[1]))\n",
    "                        corr_shifts_center_wrap[:] = np.nan\n",
    "                        print('looping')\n",
    "                        for i in range(prev_ratemap.shape[0]):\n",
    "                            for j in range(prev_ratemap.shape[1]):\n",
    "                                # if disk_ids is not None and [i,j] in disk_ids.tolist():\n",
    "                                if True:\n",
    "                                    \n",
    "                                    shift = (i, j)\n",
    "                                    shift_center = (i - int(np.floor(prev_ratemap.shape[0] / 2)), j - int(np.floor(prev_ratemap.shape[1] / 2)))\n",
    "                                    \n",
    "                                    if cylinder:\n",
    "                                        wrap_map = np.roll(non_flat_curr, shift=shift, axis=(0, 1))\n",
    "                                        wrap_map = flat_disk_mask(wrap_map)\n",
    "                                        # wrap_map[wrap_map != wrap_map] = 0\n",
    "                                        wrap_map_center = np.roll(non_flat_curr, shift=shift_center, axis=(0, 1)) \n",
    "                                        wrap_map_center = flat_disk_mask(wrap_map_center)\n",
    "\n",
    "                                        # wrap_map_center[wrap_map_center != wrap_map_center] = 0\n",
    "                                        shifted_map = flat_disk_mask(non_flat_curr)\n",
    "                                        shifted_map[shifted_map != shifted_map] = 0\n",
    "                                        shifted_map = sio.ndimage.shift(shifted_map, shift, mode='constant', cval=0)\n",
    "                                        # shifted_map[shifted_map < 0.01] = np.nan\n",
    "                                        shifted_map = flat_disk_mask(shifted_map)\n",
    "\n",
    "                                        # shifted_map[shifted_map != shifted_map] = 0\n",
    "                                        shifted_map_center = flat_disk_mask(non_flat_curr)\n",
    "                                        shifted_map_center[shifted_map_center != shifted_map_center] = 0\n",
    "                                        shifted_map_center = sio.ndimage.shift(shifted_map_center, shift_center, mode='constant', cval=0)\n",
    "                                        # shifted_map_center[shifted_map_center < 0.01] = np.nan\n",
    "                                        # shifted_map_center[shifted_map_center != shifted_map_center] = 0\n",
    "                                        shifted_map_center = flat_disk_mask(shifted_map_center)\n",
    "\n",
    "                                    else:\n",
    "                                        wrap_map = np.roll(curr_ratemap, shift=shift, axis=(0, 1))\n",
    "                                        wrap_map_center = np.roll(curr_ratemap, shift=shift_center, axis=(0, 1))\n",
    "                                        shifted_map = sio.ndimage.shift(curr_ratemap, shift, mode='constant', cval=0)\n",
    "                                        shifted_map_center = sio.ndimage.shift(curr_ratemap, shift_center, mode='constant', cval=0)\n",
    "\n",
    "                                    row_curr, col_curr = np.where(~np.isnan(shifted_map))\n",
    "                                    row_prev, col_prev = np.where(~np.isnan(prev_ratemap))\n",
    "                                    row_curr_center, col_curr_center = np.where(~np.isnan(shifted_map_center))\n",
    "                                    row_curr_wrap, col_curr_wrap = np.where(~np.isnan(wrap_map))\n",
    "                                    row_curr_center_wrap, col_curr_center_wrap = np.where(~np.isnan(wrap_map_center))\n",
    "                                    prev_height_bucket_midpoints, prev_width_bucket_midpoints = _get_ratemap_bucket_midpoints(prev_spatial_spike_train.arena_size, y, x)\n",
    "                                    curr_height_bucket_midpoints, curr_width_bucket_midpoints = _get_ratemap_bucket_midpoints(curr_spatial_spike_train.arena_size, y, x)\n",
    "                                    # curr_height_bucket_midpoints = curr_height_bucket_midpoints[row_curr]\n",
    "                                    # curr_width_bucket_midpoints = curr_width_bucket_midpoints[col_curr]\n",
    "                                    target_weights = np.array(list(map(lambda x, y: shifted_map[x,y], row_curr, col_curr)))\n",
    "                                    source_weights = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_prev, col_prev)))\n",
    "                                    target_weights_wrap = np.array(list(map(lambda x, y: wrap_map[x,y], row_curr_wrap, col_curr_wrap)))\n",
    "                                    target_weights_center = np.array(list(map(lambda x, y: shifted_map_center[x,y], row_curr_center, col_curr_center)))\n",
    "                                    target_weights_wrap_center = np.array(list(map(lambda x, y: wrap_map_center[x,y], row_curr_center_wrap, col_curr_wrap)))\n",
    "                                    # source_weights_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_prev, col_prev)))\n",
    "                                    if np.sum(source_weights) != 0:\n",
    "                                        source_weights = source_weights / np.sum(source_weights)\n",
    "                                    if np.sum(target_weights) != 0:\n",
    "                                        target_weights = target_weights / np.sum(target_weights)\n",
    "                                    if np.sum(target_weights_wrap) != 0:\n",
    "                                        target_weights_wrap = target_weights_wrap / np.sum(target_weights_wrap)\n",
    "                                    if np.sum(target_weights_center) != 0:\n",
    "                                        target_weights_center = target_weights_center / np.sum(target_weights_center)\n",
    "                                    if np.sum(target_weights_wrap_center) != 0:\n",
    "                                        target_weights_wrap_center = target_weights_wrap_center / np.sum(target_weights_wrap_center)\n",
    "                                    # source_weights_pearson = source_weights_pearson / np.sum(source_weights_pearson)\n",
    "\n",
    "                                    coord_buckets_prev = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_prev, col_prev)))\n",
    "                                    coord_buckets_curr = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr, col_curr)))\n",
    "                                    coord_buckets_curr_wrap = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr_wrap, col_curr_wrap)))\n",
    "                                    coord_buckets_curr_center = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr_center, col_curr_center)))\n",
    "                                    coord_buckets_curr_center_wrap = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_curr_center_wrap, col_curr_center_wrap)))\n",
    "\n",
    "                                    wass_shift = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr, source_weights, target_weights, n_projections=10**3)\n",
    "                                    wass_wrap = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr_wrap, source_weights, target_weights_wrap, n_projections=10**3)\n",
    "                                    wass_shift_center = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr_center, source_weights, target_weights_center, n_projections=10**3)\n",
    "                                    wass_wrap_center = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr_center_wrap, source_weights, target_weights_wrap_center, n_projections=10**3)\n",
    "\n",
    "                                    # if len(source_weights) != len(target_weights):\n",
    "\n",
    "                                    prev_ratemap = flat_disk_mask(prev_ratemap)\n",
    "                                    shifted_map = flat_disk_mask(shifted_map)   \n",
    "                                    row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "                                    row_not_nan_target = np.where(~np.isnan(shifted_map))[0]\n",
    "                                    col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "                                    col_not_nan_target = np.where(~np.isnan(shifted_map))[1]\n",
    "                                    row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "                                    row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "                                    row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "                                    row_use = [x[0] for x in row_col_use]\n",
    "                                    col_use = [x[1] for x in row_col_use]                                   \n",
    "                                    # row_use = np.argmin([len(source_weights), len(target_weights)])\n",
    "                                    # row_use = [row_prev, row_curr][row_use]\n",
    "                                    # col_use = np.argmin([len(source_weights), len(target_weights)])\n",
    "                                    # col_use = [col_prev, col_curr][col_use]\n",
    "                                    source_weights_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_use, col_use)))\n",
    "                                    target_weights_pearson = np.array(list(map(lambda x, y: shifted_map[x,y], row_use, col_use)))\n",
    "                                    assert len(source_weights_pearson) == len(target_weights_pearson), 'source and target weights are not the same length'\n",
    "                                    if np.sum(source_weights_pearson) != 0:\n",
    "                                        source_weights_pearson = source_weights_pearson / np.sum(source_weights_pearson)\n",
    "                                    if np.sum(target_weights_pearson) != 0: \n",
    "                                        target_weights_pearson = target_weights_pearson / np.sum(target_weights_pearson)\n",
    "                                    coord_buckets_curr = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    coord_buckets_prev = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_use, col_use)))                               \n",
    "                                    assert len(source_weights_pearson[source_weights_pearson != source_weights_pearson]) == 0, 'source weights pearson contains {} nan values'.format(len(source_weights_pearson[source_weights_pearson != source_weights_pearson]))\n",
    "\n",
    "                                    wrap_map = flat_disk_mask(wrap_map)\n",
    "                                    row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "                                    row_not_nan_target = np.where(~np.isnan(wrap_map))[0]\n",
    "                                    col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "                                    col_not_nan_target = np.where(~np.isnan(wrap_map))[1]\n",
    "                                    row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "                                    row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "                                    row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "                                    row_use = [x[0] for x in row_col_use]\n",
    "                                    col_use = [x[1] for x in row_col_use]\n",
    "\n",
    "                                    target_weights_wrap_pearson = np.array(list(map(lambda x, y: wrap_map[x,y], row_use, col_use)))\n",
    "                                    if np.sum(target_weights_wrap_pearson) != 0: \n",
    "                                        target_weights_wrap_pearson = target_weights_wrap_pearson / np.sum(target_weights_wrap_pearson)\n",
    "                                    coord_buckets_curr_wrap = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    coord_buckets_prev_wrap = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_use, col_use)))                               \n",
    "                                    source_weights_wrap_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_use, col_use)))\n",
    "                                    assert len(source_weights_wrap_pearson[source_weights_wrap_pearson != source_weights_wrap_pearson]) == 0, 'source weights pearson contains {} nan values'.format(len(source_weights_pearson[source_weights_pearson != source_weights_pearson]))\n",
    "                                    if np.sum(source_weights_wrap_pearson) != 0:\n",
    "                                        source_weights_wrap_pearson = source_weights_wrap_pearson / np.sum(source_weights_wrap_pearson)\n",
    "                                    \n",
    "                                    shifted_map_center = flat_disk_mask(shifted_map_center)     \n",
    "                                    row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "                                    row_not_nan_target = np.where(~np.isnan(shifted_map_center))[0]\n",
    "                                    col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "                                    col_not_nan_target = np.where(~np.isnan(shifted_map_center))[1]\n",
    "                                    row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "                                    row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "                                    row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "                                    row_use = [x[0] for x in row_col_use]\n",
    "                                    col_use = [x[1] for x in row_col_use]\n",
    "                                    \n",
    "                                    target_weights_center_pearson = np.array(list(map(lambda x, y: shifted_map_center[x,y], row_use, col_use)))\n",
    "                                    if np.sum(target_weights_center_pearson) != 0:\n",
    "                                        target_weights_center_pearson = target_weights_center_pearson / np.sum(target_weights_center_pearson)\n",
    "                                    coord_buckets_curr_center = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    coord_buckets_prev_center = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    source_weights_center_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_use, col_use)))\n",
    "                                    assert len(source_weights_center_pearson[source_weights_center_pearson != source_weights_center_pearson]) == 0, 'source weights pearson contains {} nan values'.format(len(source_weights_pearson[source_weights_pearson != source_weights_pearson]))\n",
    "                                    if np.sum(source_weights_center_pearson) != 0:\n",
    "                                        source_weights_center_pearson = source_weights_center_pearson / np.sum(source_weights_center_pearson)\n",
    "\n",
    "                                    wrap_map_center = flat_disk_mask(wrap_map_center)\n",
    "                                    row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "                                    row_not_nan_target = np.where(~np.isnan(wrap_map_center))[0]\n",
    "                                    col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "                                    col_not_nan_target = np.where(~np.isnan(wrap_map_center))[1]\n",
    "                                    row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "                                    row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "                                    row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "                                    row_use = [x[0] for x in row_col_use]\n",
    "                                    col_use = [x[1] for x in row_col_use]\n",
    "\n",
    "                                    target_weights_wrap_center_pearson = np.array(list(map(lambda x, y: wrap_map_center[x,y], row_use, col_use)))\n",
    "                                    if np.sum(target_weights_wrap_center_pearson) != 0:\n",
    "                                        target_weights_wrap_center_pearson = target_weights_wrap_center_pearson / np.sum(target_weights_wrap_center_pearson)\n",
    "                                    coord_buckets_curr_center_wrap = np.array(list(map(lambda x, y: [curr_height_bucket_midpoints[x],curr_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    coord_buckets_prev_center_wrap = np.array(list(map(lambda x, y: [prev_height_bucket_midpoints[x],prev_width_bucket_midpoints[y]], row_use, col_use)))\n",
    "                                    source_weights_wrap_center_pearson = np.array(list(map(lambda x, y: prev_ratemap[x,y], row_use, col_use)))\n",
    "                                    assert len(source_weights_wrap_center_pearson[source_weights_wrap_center_pearson != source_weights_wrap_center_pearson]) == 0,'source weights pearson contains {} nan values'.format(len(source_weights_pearson[source_weights_pearson != source_weights_pearson]))\n",
    "                                    if np.sum(source_weights_wrap_center_pearson) != 0:\n",
    "                                        source_weights_wrap_center_pearson = source_weights_wrap_center_pearson / np.sum(source_weights_wrap_center_pearson)\n",
    "\n",
    "                                    try:\n",
    "                                        corr_shift = pearsonr(source_weights_pearson, target_weights_pearson)[0]\n",
    "                                    except:\n",
    "                                        corr_shift = np.nan\n",
    "                                    try:\n",
    "                                        corr_wrap = pearsonr(source_weights_wrap_pearson, target_weights_wrap_pearson)[0]\n",
    "                                    except:\n",
    "                                        corr_wrap = np.nan\n",
    "                                    try:\n",
    "                                        corr_shift_center = pearsonr(source_weights_center_pearson, target_weights_center_pearson)[0]\n",
    "                                    except:\n",
    "                                        corr_shift_center = np.nan\n",
    "                                    try:\n",
    "                                        corr_wrap_center = pearsonr(source_weights_wrap_center_pearson, target_weights_wrap_center_pearson)[0]\n",
    "                                    except:\n",
    "                                        corr_wrap_center = np.nan\n",
    "                                        \n",
    "                                    wass_shifts[i, j] = wass_shift\n",
    "                                    corr_shifts[i, j] = corr_shift\n",
    "                                    wass_shifts_wrap[i, j] = wass_wrap\n",
    "                                    corr_shifts_wrap[i, j] = corr_wrap\n",
    "                                    wass_shifts_center[i, j] = wass_shift_center\n",
    "                                    corr_shifts_center[i, j] = corr_shift_center\n",
    "                                    wass_shifts_center_wrap[i, j] = wass_wrap_center\n",
    "                                    corr_shifts_center_wrap[i, j] = corr_wrap_center\n",
    "    \n",
    "                                    # corr_shifts_center_wrap[shift_center[0], shift_center[1]] = corr_wrap_center\n",
    "                                \n",
    "                    title = animal.animal_id + '_' + seskey.replace('_', '') + '_' + str(cell_label)\n",
    "\n",
    "                    cell_dict[title] = curr_ratemap\n",
    "                    blobs_cell_dict[title] = [labels, centroids]\n",
    "                    if prev is not None:\n",
    "                        scores_cell_dict[title] = [wass, corr, wass_shifts, corr_shifts, wass_shifts_wrap, corr_shifts_wrap,\n",
    "                                                    wass_shifts_center, corr_shifts_center, wass_shifts_center_wrap, corr_shifts_center_wrap]\n",
    "\n",
    "                    func_scores_dict[title] = [gscore, bscore, si_score]\n",
    "\n",
    "                    prev = curr_ratemap\n",
    "                    prev_spatial_spike_train = spatial_spike_train\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wass_wrap_center = pot_sliced_wasserstein(coord_buckets_prev, coord_buckets_curr_center_wrap, source_weights, target_weights_wrap_center, n_projections=10**3)\n",
    "plt.imshow(flat_disk_mask(shifted_map), cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_not_nan_source = np.where(~np.isnan(prev_ratemap))[0]\n",
    "row_not_nan_target = np.where(~np.isnan(curr_ratemap))[0]\n",
    "col_not_nan_source = np.where(~np.isnan(prev_ratemap))[1]\n",
    "col_not_nan_target = np.where(~np.isnan(curr_ratemap))[1]\n",
    "row_col_not_nan_source = np.array(list(zip(row_not_nan_source, col_not_nan_source)))\n",
    "row_col_not_nan_target = np.array(list(zip(row_not_nan_target, col_not_nan_target)))\n",
    "row_col_use = list(set(map(tuple, row_col_not_nan_source)).intersection(map(tuple, row_col_not_nan_target)))\n",
    "row_use = [x[0] for x in row_col_use]\n",
    "col_use = [x[1] for x in row_col_use]\n",
    "print(len(row_not_nan_source), len(row_not_nan_target), len(row_use))\n",
    "print(len(col_not_nan_source), len(col_not_nan_target), len(col_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prev_ratemap[row_use[0]:row_use[-1], col_use[0]:col_use[-1]], cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        # shifted_map = flat_disk_mask(non_flat_curr)\n",
    "                                        # shifted_map = sio.ndimage.shift(shifted_map, shift, mode='constant', cval=0)\n",
    "                                        # shifted_map[shifted_map < 0.01] = np.nan\n",
    "copied = np.copy(prev_ratemap)\n",
    "copied2 = np.copy(prev_ratemap)\n",
    "copied2[copied2 != copied2] = 0\n",
    "copied[copied != copied] = 0\n",
    "copied = sio.ndimage.shift(copied, (15,15), mode='constant', cval=0)\n",
    "# copied[copied < 0.01] = np.nan\n",
    "plt.imshow(copied, cmap='jet')\n",
    "plt.show()\n",
    "plt.imshow(copied2, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(target_weights_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_weights = np.array(list(map(lambda x, y: curr_ratemap[x,y], row_curr, col_curr)))\n",
    "target_weights = target_weights / np.sum(target_weights)\n",
    "print(target_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(shifted_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sessions = {}\n",
    "for key in cell_dict.keys():\n",
    "    parts = key.split('_')\n",
    "    tetrode = parts[1].split('tet')[-1]    \n",
    "    unit = parts[-1]  \n",
    "    unit_tetrode_key = f'sub_tet{tetrode}_unit{unit}' \n",
    "    if unit_tetrode_key not in grouped_sessions:\n",
    "        grouped_sessions[unit_tetrode_key] = []\n",
    "    grouped_sessions[unit_tetrode_key].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sio\n",
    "\n",
    "units = []\n",
    "unit_keys = []\n",
    "unit_titles = []\n",
    "for ky in grouped_sessions.keys(): \n",
    "    uns = [cell_dict[x] for x in grouped_sessions[ky]]\n",
    "    uns_keys = grouped_sessions[ky]\n",
    "    pairs = []\n",
    "    pair_keys = []\n",
    "    pair_titles = []\n",
    "    prev = None\n",
    "    prev_key = None\n",
    "    for i in range(len(uns)):\n",
    "        if prev is not None:\n",
    "            pairs.append([prev, uns[i]])\n",
    "            pair_keys.append([prev_key, uns_keys[i]])\n",
    "            pair_titles.append(prev_key + ' - ' + uns_keys[i])\n",
    "        prev = uns[i]\n",
    "        prev_key = uns_keys[i]\n",
    "\n",
    "    units.append(pairs)\n",
    "    unit_keys.append(pair_keys)\n",
    "    unit_titles.append(pair_titles)\n",
    "\n",
    "\n",
    "# c1u1ky = ['cell1_tet8_session1_1', 'cell1_tet8_session2_1']\n",
    "# c1u2ky = ['cell1_tet8_session1_2', 'cell1_tet8_session2_2']\n",
    "# c1u3ky = ['cell1_tet8_session1_3', 'cell1_tet8_session2_3']\n",
    "\n",
    "# c2u1ky = ['cell2_tet4_session1_1', 'cell2_tet4_session2_1']\n",
    "# c2u2ky = ['cell2_tet4_session1_2', 'cell2_tet4_session2_2']\n",
    "\n",
    "# c1u1 = [cell_dict[x] for x in c1u1ky]\n",
    "# c1u2 = [cell_dict[x] for x in c1u2ky]\n",
    "# c1u3 = [cell_dict[x] for x in c1u3ky]\n",
    "\n",
    "# c2u1 = [cell_dict[x] for x in c2u1ky]\n",
    "# c2u2 = [cell_dict[x] for x in c2u2ky]\n",
    "\n",
    "# units = [c1u1, c1u2, c1u3, c2u1, c2u2]\n",
    "# unit_keys = [c1u1ky, c1u2ky, c1u3ky, c2u1ky, c2u2ky]\n",
    "# unit_titles = ['MEC Tet 8 Unit 1', 'MEC Tet 8 Unit 2', 'MEC Tet 8 Unit 3', 'MEC Tet 4 Unit 1', 'MEC Tet 4 Unit 2']\n",
    "\n",
    "for i in range(len(units)):\n",
    "    unit = units[i][0]\n",
    "    unit_title = unit_titles[i][0]\n",
    "    wass, corr, wass_shifts, corr_shifts, wass_shifts_wrap, corr_shifts_wrap, wass_shifts_center, corr_shifts_center, wass_shifts_center_wrap, corr_shifts_center_wrap = scores_cell_dict[unit_keys[i][0][1]]\n",
    "    prev_gscore, prev_bscore, prev_si_score = func_scores_dict[unit_keys[i][0][0]]\n",
    "    curr_gscore, curr_bscore, curr_si_score = func_scores_dict[unit_keys[i][0][1]]\n",
    "\n",
    "    # wass_shifts = flat_disk_mask(wass_shifts)\n",
    "    # wass_shifts_wrap = flat_disk_mask(wass_shifts_wrap)\n",
    "    # corr_shifts = flat_disk_mask(corr_shifts)\n",
    "    # corr_shifts_wrap = flat_disk_mask(corr_shifts_wrap)\n",
    "    # wass_shifts_center = flat_disk_mask(wass_shifts_center)\n",
    "    # wass_shifts_center_wrap = flat_disk_mask(wass_shifts_center_wrap)\n",
    "    # corr_shifts_center = flat_disk_mask(corr_shifts_center)\n",
    "    # corr_shifts_center_wrap = flat_disk_mask(corr_shifts_center_wrap)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    toplot = unit[0]\n",
    "    # toplot[toplot == 0] = np.nan\n",
    "\n",
    "    ax = fig.add_subplot(2,4,1)\n",
    "    img = ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 1 (Reference)')\n",
    "\n",
    "    # toplot = unit[1]\n",
    "    # # toplot = flat_disk_mask(toplot)\n",
    "    # toplot[toplot != toplot] = 0\n",
    "    # toplot = sio.ndimage.shift(toplot, (8, 8), mode='constant', cval=0)\n",
    "    # shifted_copy = np.copy(toplot)\n",
    "    # toplot[toplot < 0.01] = np.nan\n",
    "    \n",
    "    toplot = unit[1]\n",
    "    toplot_copy = np.copy(toplot)\n",
    "    toplot_copy[toplot_copy != toplot_copy] = 0\n",
    "    toplot_copy = sio.ndimage.shift(toplot_copy, (8, 8), mode='constant', cval=0)\n",
    "    toplot_copy[toplot_copy < 0.01] = np.nan\n",
    "    ax = fig.add_subplot(2,4,5)\n",
    "    img = ax.imshow(toplot_copy, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (No wrap)')\n",
    "    # shited_copy[shited_copy > 0.01] = np.nan\n",
    "    # shifted_copy[shifted_copy == 0] = 0\n",
    "    # ax.imshow(shifted_copy, cmap='jet', aspect='auto')\n",
    "    not_nan = ~np.isnan(toplot_copy)\n",
    "    is_nan = np.isnan(toplot_copy)\n",
    "    toplot_copy[is_nan] = 0\n",
    "    toplot_copy[not_nan] = np.nan\n",
    "    ax.imshow(toplot_copy, cmap='jet', aspect='auto')\n",
    "\n",
    "    toplot = unit[1]\n",
    "    toplot[toplot == 0] = np.nan\n",
    "    toplot = np.roll(toplot, shift=(8, 8), axis=(0, 1))\n",
    "    ax = fig.add_subplot(2,4,6)\n",
    "    img = ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (Wrap)')\n",
    "    not_nan = ~np.isnan(toplot)\n",
    "    is_nan = np.isnan(toplot)\n",
    "    toplot[is_nan] = 0\n",
    "    toplot[not_nan] = np.nan\n",
    "    ax.imshow(toplot, cmap='jet', aspect='auto')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,2)\n",
    "    to_plot = unit[1] \n",
    "    # to_plot[toplot == 0] = np.nan\n",
    "    img = ax.imshow(to_plot, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    cbar.ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Session 2 (Shifting)')\n",
    "    # stop()\n",
    "    # ax = fig.add_subplot(2,6,3)\n",
    "    # img = ax.imshow(wass_shifts, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('')\n",
    "    # ax.set_title('EMD corner shift, no wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,3)\n",
    "    img = ax.imshow(wass_shifts_center, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('')\n",
    "    ax.set_title('EMD center shift, no wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,4)\n",
    "    # img = ax.imshow(corr_shifts, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('Correlation')\n",
    "    # ax.set_title('Pearson-R corner shift, no wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,4)\n",
    "    img = ax.imshow(corr_shifts_center, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Pearson-R center shift, no wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,6,5)\n",
    "    # img = ax.imshow(wass_shifts_wrap, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('')\n",
    "    # ax.set_title('EMD corner shift & wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,7)\n",
    "    img = ax.imshow(wass_shifts_center_wrap, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('')\n",
    "    ax.set_title('EMD center shift & wrap')\n",
    "\n",
    "    # ax = fig.add_subplot(2,4,7)\n",
    "    # img = ax.imshow(corr_shifts_wrap, cmap='jet', aspect='auto')\n",
    "    # cbar = fig.colorbar(img, ax=ax)\n",
    "    # # cbar.ax.set_ylabel('Correlation')\n",
    "    # ax.set_title('Pearson-R corner shift & wrap')\n",
    "\n",
    "    ax = fig.add_subplot(2,4,8)\n",
    "    img = ax.imshow(corr_shifts_center_wrap, cmap='jet', aspect='auto')\n",
    "    cbar = fig.colorbar(img, ax=ax)\n",
    "    # cbar.ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Pearson-R center shift & wrap')\n",
    "\n",
    "    # unit_title\n",
    "    ttle = 'LC Tet ' + unit_title.split('tet')[-1].split('_')[0] + ' Unit ' + unit_title.split('_')[-1]\n",
    "    fig.suptitle(ttle + ' | Grid: (' + str(round(prev_gscore, 3)) + ', ' + str(round(curr_gscore, 3)) + \n",
    "                 ') | Border: (' + str(round(max(prev_bscore), 3)) + ', ' + str(round(max(curr_bscore), 3)) + \n",
    "                 ') | SI: (' + str(round(prev_si_score, 3)) + ', ' + str(round(curr_si_score, 3)) + ')',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # fig.suptitle(unit_title + ' | Grid: ' + str(round(gscore, 3)) + ' | Border: ' + str(round(max(bscore), 3)) + ' | SI: ' + str(round(si_score, 3)))\n",
    "    #' | Wass: ' + str(round(wass, 3)) + ' | Corr: ' + str(round(corr, 3)) + \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
